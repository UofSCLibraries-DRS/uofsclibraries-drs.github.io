{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54374787",
   "metadata": {},
   "source": [
    "# Tidy Text Case Studies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf70db64",
   "metadata": {},
   "source": [
    "## Acknowledgements\n",
    "We would like to acknowledge the work of Julia Silge and David Robinson, whose materials were used under the terms of the Creative Commons Attribution-NonCommercial-ShareAlike 3.0 United States License. Their contributions to open data science education, particularly the [Text Mining with R](https://www.tidytextmining.com), provided valuable resources for this project.\n",
    "\n",
    "This notebook was created by Meara Cox using their code and examples as a foundation, with additional explanations and adaptations to support the goals of this project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f19d3c9",
   "metadata": {},
   "source": [
    "## Install Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04473f3d",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "install.packages(\"lubridate\")\n",
    "install.packages(\"ggplot2\")\n",
    "install.packages(\"dplyr\")\n",
    "install.packages(\"readr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b2fc76",
   "metadata": {},
   "source": [
    "## Case study: Comparing Twitter Archives\n",
    "\n",
    "Online text, especially from platforms like Twitter, receives a lot of attention in text analysis. Many sentiment lexicons used in this book were specifically designed for or validated on tweets, reflecting the unique language and style found there. Both authors of this book are active Twitter users, so this case study explores a comparison between the entire Twitter archives of [Julia](https://x.com/juliasilge) and [David](https://x.com/drob).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149bf64a",
   "metadata": {},
   "source": [
    "### Getting the Data and Distribution\n",
    "\n",
    "Individuals can download their own Twitter archives by following [instructions provided on Twitter’s website](https://help.x.com/en/managing-your-account/how-to-download-your-x-archive). After downloading their archives, the next step is to load the data and use the **lubridate** package to convert the string-formatted timestamps into proper date-time objects. This conversion enables easier analysis of tweeting patterns over time. Initially, we can explore overall tweeting behavior by summarizing or visualizing the frequency and timing of tweets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2807208",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "library(lubridate)\n",
    "library(ggplot2)\n",
    "library(dplyr)\n",
    "library(readr)\n",
    "\n",
    "tweets_julia <- read_csv(\"data/tweets_julia.csv\", show_col_types = FALSE)\n",
    "tweets_dave <- read_csv(\"data/tweets_dave.csv\", show_col_types = FALSE)\n",
    "tweets <- bind_rows(tweets_julia %>% \n",
    "                      mutate(person = \"Julia\"),\n",
    "                    tweets_dave %>% \n",
    "                      mutate(person = \"David\")) %>%\n",
    "  mutate(timestamp = ymd_hms(timestamp))\n",
    "\n",
    "ggplot(tweets, aes(x = timestamp, fill = person)) +\n",
    "  geom_histogram(position = \"identity\", bins = 20, show.legend = FALSE) +\n",
    "  facet_wrap(~person, ncol = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b93cc07",
   "metadata": {},
   "source": [
    "David and Julia currently tweet at roughly the same rate and joined Twitter about a year apart, but there was a period of about five years when David was inactive on the platform while Julia remained active. As a result, Julia has accumulated approximately four times as many tweets as David overall."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645486bc",
   "metadata": {},
   "source": [
    "### Word Frequencies\n",
    "\n",
    "We’ll start by using `unnest_tokens()` to convert our tweets into a tidy format with one token per row, but because tweets often include unique structures like hashtags, mentions, links, and special characters, we’ll apply some preprocessing first. We’ll exclude retweets to focus only on original content, then clean the text by removing URLs and unwanted characters such as ampersands.\n",
    "\n",
    "When tokenizing, we’ll use a regular expression within `unnest_tokens()` to preserve mentions (starting with `@`) and hashtags (starting with `#`), which are meaningful in Twitter data. Since hashtags and mentions are retained, we can’t rely solely on `anti_join()` with a standard stop word list. Instead, we’ll use `filter()` with `str_detect()` from the stringr package to exclude common stop words, while still keeping elements like usernames and hashtags intact for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de85f98f",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "library(tidytext)\n",
    "library(stringr)\n",
    "\n",
    "replace_reg <- \"https://t.co/[A-Za-z\\\\d]+|http://[A-Za-z\\\\d]+|&amp;|&lt;|&gt;|RT|https\"\n",
    "unnest_reg <- \"([^A-Za-z_\\\\d#@']|'(?![A-Za-z_\\\\d#@]))\"\n",
    "\n",
    "tidy_tweets <- tweets %>% \n",
    "  filter(!str_detect(text, \"^RT\")) %>%\n",
    "  mutate(text = str_replace_all(text, replace_reg, \"\")) %>%\n",
    "  unnest_tokens(word, text, token = \"regex\", pattern = unnest_reg) %>%\n",
    "  filter(!word %in% stop_words$word,\n",
    "         !word %in% str_remove_all(stop_words$word, \"'\"),\n",
    "         str_detect(word, \"[a-z]\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc02d5f8",
   "metadata": {},
   "source": [
    "We can now compute word frequencies for each individual. To do this, we first group the data by person and word, counting the number of times each word appears per person. Next, we perform a `left\\_join()` to bring in the total word counts for each person — Julia’s total is larger since she has more tweets overall. Finally, we calculate the frequency of each word for each person by dividing the word count by the total words they used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf927ea7",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "frequency <- tidy_tweets %>% \n",
    "  count(person, word, sort = TRUE) %>% \n",
    "  left_join(tidy_tweets %>% \n",
    "              count(person, name = \"total\")) %>%\n",
    "  mutate(freq = n/total)\n",
    "\n",
    "print(frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d741d2",
   "metadata": {},
   "source": [
    "This gives us a clean and tidy data frame, but to visualize those frequencies with one person’s frequency on the x-axis and the other’s on the y-axis, we need to reshape the data. We can use tidyr’s `pivot\\_wider()` function to transform the data frame so that each person’s word frequencies become separate columns, ready for plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da739639",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "library(tidyr)\n",
    "\n",
    "frequency <- frequency %>% \n",
    "  select(person, word, freq) %>% \n",
    "  pivot_wider(names_from = person, values_from = freq) %>%\n",
    "  arrange(Julia, David)\n",
    "\n",
    "print(frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57db2a5b",
   "metadata": {},
   "source": [
    "This reshaped data is now ready for visualization. We can use `geom_jitter()` to help spread out the points, reducing the visual clumping that happens with low-frequency words. Additionally, setting `check_overlap = TRUE` on the text labels will prevent them from overlapping excessively, so only a subset of labels will appear clearly on the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e799a390",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "suppressPackageStartupMessages(library(scales))\n",
    "\n",
    "ggplot(frequency, aes(Julia, David)) +\n",
    "  geom_jitter(alpha = 0.1, size = 2.5, width = 0.25, height = 0.25) +\n",
    "  geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +\n",
    "  scale_x_log10(labels = percent_format()) +\n",
    "  scale_y_log10(labels = percent_format()) +\n",
    "  geom_abline(color = \"red\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220f9c02",
   "metadata": {},
   "source": [
    "Words that fall close to the diagonal line on the plot are used at roughly equal rates by both David and Julia. In contrast, words that sit farther from the line are more heavily favored by one person over the other. The plot only shows words, hashtags, and usernames that both individuals have used at least once in their tweets.\n",
    "\n",
    "It’s clear from this visualization—and will continue to be throughout the chapter—that David and Julia have approached Twitter in very different ways over the years. David's tweets have been almost entirely professional since he became active, whereas Julia's early Twitter use was personal, and even now, her account retains more personal content than David's. This difference in usage patterns is reflected immediately in how they use language on the platform."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e127e7",
   "metadata": {},
   "source": [
    "### Comparing Word Usage\n",
    "\n",
    "We’ve already visualized a comparison of raw word frequencies across the entire Twitter histories of David and Julia. Now, let’s dig deeper by identifying which words are more or less likely to originate from each person’s account using the log odds ratio. To keep the comparison meaningful, we’ll limit the analysis to tweets from 2016—a year when David was consistently active on Twitter and Julia was transitioning into her data science career."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4a7e5d",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "tidy_tweets <- tidy_tweets %>%\n",
    "  filter(timestamp >= as.Date(\"2016-01-01\"),\n",
    "         timestamp < as.Date(\"2017-01-01\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a696dff6",
   "metadata": {},
   "source": [
    "Next, we’ll use `str_detect()` to filter out Twitter usernames from our word column. Otherwise, the analysis would be dominated by names of people David or Julia know, which doesn’t tell us much about broader word usage differences.\n",
    "\n",
    "Once usernames are removed, we count how many times each person uses each word and keep only words that appear more than 10 times to avoid results driven by very rare terms. After reshaping the data with `pivot_wider()`, we calculate the **log odds ratio** for each word as:\n",
    "\n",
    "$$\n",
    "\\text{log odds ratio} = \\ln \\left( \\frac{(n + 1) / (total + 1) \\text{ for David}}{(n + 1) / (total + 1) \\text{ for Julia}} \\right)\n",
    "$$\n",
    "\n",
    "Here:\n",
    "\n",
    "* $n$ is the count of the word for each person,\n",
    "* $total$ is the total word count for each person,\n",
    "* Adding 1 to both numerator and denominator prevents division by zero for rare words.\n",
    "\n",
    "This gives a clearer picture of which words are disproportionately associated with one person versus the other.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc5bf8d",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "word_ratios <- tidy_tweets %>%\n",
    "  filter(!str_detect(word, \"^@\")) %>%\n",
    "  count(word, person) %>%\n",
    "  group_by(word) %>%\n",
    "  filter(sum(n) >= 10) %>%\n",
    "  ungroup() %>%\n",
    "  pivot_wider(names_from = person, values_from = n, values_fill = 0) %>%\n",
    "  mutate_if(is.numeric, list(~(. + 1) / (sum(.) + 1))) %>%\n",
    "  mutate(logratio = log(David / Julia)) %>%\n",
    "  arrange(desc(logratio))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a8257c",
   "metadata": {},
   "source": [
    "Here are some words that have been about equally likely to come from David or Julia’s account during 2016."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13da71fc",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "word_ratios %>% \n",
    "  arrange(abs(logratio)) %>%\n",
    "  print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6ad494",
   "metadata": {},
   "source": [
    "To find the words most strongly associated with each account, we select the top 15 words with the highest positive log odds ratio (most likely from David’s account) and the top 15 words with the lowest (most negative) log odds ratio (most likely from Julia’s account).\n",
    "\n",
    "Plotting these words will clearly show which terms are most distinctive for each person’s tweets, highlighting the differences in their language use during 2016."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c1ea89",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "word_ratios %>%\n",
    "  group_by(logratio < 0) %>%\n",
    "  slice_max(abs(logratio), n = 15) %>% \n",
    "  ungroup() %>%\n",
    "  mutate(word = reorder(word, logratio)) %>%\n",
    "  ggplot(aes(word, logratio, fill = logratio < 0)) +\n",
    "  geom_col(show.legend = FALSE) +\n",
    "  coord_flip() +\n",
    "  ylab(\"log odds ratio (David/Julia)\") +\n",
    "  scale_fill_discrete(name = \"\", labels = c(\"David\", \"Julia\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa059e0",
   "metadata": {},
   "source": [
    "David’s tweets tend to focus on professional topics like conferences and programming, reflecting his career interests, while Julia’s tweets highlight personal themes like family and regional topics such as Utah and Census data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571702cf",
   "metadata": {},
   "source": [
    "### Changes in Word Use\n",
    "\n",
    "The previous section focused on overall word usage, but now we want to explore a different question: which words have changed in frequency the most over time in the Twitter feeds? In other words, which words have they tweeted about more or less as time has gone on?\n",
    "\n",
    "To analyze this, we first create a new time variable in the data frame that groups each tweet into a specific time unit. Using lubridate’s `floor_date()` function, we round timestamps down to the start of each month, which works well for this year of tweets.\n",
    "\n",
    "Once these time bins are set, we count how often each person used each word within each month. Then, we add columns showing the total words tweeted by each person per month and the total uses of each word by each person overall. Finally, we filter the data to keep only words that appear at least 30 times, ensuring we focus on words with sufficient usage for meaningful analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4491caa3",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "words_by_time <- tidy_tweets %>%\n",
    "  filter(!str_detect(word, \"^@\")) %>%\n",
    "  mutate(time_floor = floor_date(timestamp, unit = \"1 month\")) %>%\n",
    "  count(time_floor, person, word) %>%\n",
    "  group_by(person, time_floor) %>%\n",
    "  mutate(time_total = sum(n)) %>%\n",
    "  group_by(person, word) %>%\n",
    "  mutate(word_total = sum(n)) %>%\n",
    "  ungroup() %>%\n",
    "  rename(count = n) %>%\n",
    "  filter(word_total > 30)\n",
    "\n",
    "print(words_by_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951217f2",
   "metadata": {},
   "source": [
    "Each row in this data frame represents a single person’s use of a specific word within a particular time bin. The `count` column shows how many times that person used the word in that time period, `time_total` gives the total number of words the person tweeted during that time bin, and `word_total` indicates the total number of times that person used that word throughout the entire year. This dataset is ready to be used for modeling purposes.\n",
    "\n",
    "Next, we can apply `nest()` from tidyr to create a data frame where each word corresponds to a list-column containing smaller data frames—essentially mini datasets—for each word. Let’s go ahead and do that, then examine the structure of the resulting nested data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a1be20",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "nested_data <- words_by_time %>%\n",
    "  nest(data = c(-word, -person)) \n",
    "\n",
    "print(nested_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6d0985",
   "metadata": {},
   "source": [
    "This data frame contains one row for each unique person-word pair, and the `data` column is a list-column holding smaller data frames for each of those pairs. We’ll use `map()` from the purrr package (Henry and Wickham 2018) to run our modeling function on each of these nested data frames.\n",
    "\n",
    "Since the data involves counts, we’ll fit a generalized linear model using `glm()` with a binomial family. Essentially, this model answers a question like: “Was this particular word mentioned during this time bin (yes or no)? How does the likelihood of mentioning this word change over time?”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cee5560",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "suppressPackageStartupMessages(library(purrr))\n",
    "\n",
    "nested_models <- nested_data %>%\n",
    "  mutate(models = map(data, ~ glm(cbind(count, time_total) ~ time_floor, ., \n",
    "                                  family = \"binomial\")))\n",
    "\n",
    "print(nested_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd31b08c",
   "metadata": {},
   "source": [
    "Now we see a new column holding the modeling results—this is another list-column, each entry containing a `glm` model object. The next step is to use `map()` together with `tidy()` from the broom package to extract the slope coefficients from each model.\n",
    "\n",
    "Since we’re testing many slopes simultaneously and some won’t be statistically significant, we’ll adjust the p-values to account for multiple comparisons, helping to control for false positives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625bee69",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "library(broom)\n",
    "\n",
    "slopes <- nested_models %>%\n",
    "  mutate(models = map(models, tidy)) %>%\n",
    "  unnest(cols = c(models)) %>%\n",
    "  filter(term == \"time_floor\") %>%\n",
    "  mutate(adjusted.p.value = p.adjust(p.value))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79afd908",
   "metadata": {},
   "source": [
    "Next, we’ll identify the most notable slopes—that is, the words whose usage frequencies have changed over time with moderate statistical significance in our tweets. By filtering the results for adjusted p-values below a chosen threshold (e.g., 0.05), we can pinpoint these words and understand which topics or terms have become more or less common during the period analyzed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324d3a1e",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "top_slopes <- slopes %>% \n",
    "  filter(adjusted.p.value < 0.05)\n",
    "\n",
    "print(top_slopes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3119fa30",
   "metadata": {},
   "source": [
    "To visualize these findings, we can create plots showing how the usage of these words has changed over the course of the year for both David and Julia. This will help illustrate trends in word frequency over time, highlighting the differences and similarities in their tweeting patterns throughout the year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabe182d",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "words_by_time %>%\n",
    "  inner_join(top_slopes, by = c(\"word\", \"person\")) %>%\n",
    "  filter(person == \"David\") %>%\n",
    "  ggplot(aes(time_floor, count/time_total, color = word)) +\n",
    "  geom_line(linewidth = 1.3) +\n",
    "  labs(x = NULL, y = \"Word frequency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fcaad86",
   "metadata": {},
   "source": [
    "This shows that David tweeted extensively about the UseR conference during the event, but his mentions dropped off quickly afterward. Additionally, his tweets about Stack Overflow increased toward the end of the year, while references to ggplot2 declined over time.\n",
    "\n",
    "Now let’s plot words that have changed frequency in Julia’s tweets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d881ab5",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "words_by_time %>%\n",
    "  inner_join(top_slopes, by = c(\"word\", \"person\")) %>%\n",
    "  filter(person == \"Julia\") %>%\n",
    "  ggplot(aes(time_floor, count/time_total, color = word)) +\n",
    "  geom_line(linewidth = 1.3) +\n",
    "  labs(x = NULL, y = \"Word frequency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b4b9a1",
   "metadata": {},
   "source": [
    "All of Julia’s significant slopes are negative, indicating she hasn’t increased her use of any particular words over the year. Instead, she’s used a wider variety of words, with the ones shown in this plot appearing more frequently earlier in the year. For example, words related to sharing new blog posts—like the hashtag #rstats and the word “post”—have decreased in usage over time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4678c0",
   "metadata": {},
   "source": [
    "### Favorites and Retweets\n",
    "\n",
    "Another key aspect of tweets is how often they get favorited or retweeted. To analyze which words are associated with higher engagement on Julia’s and David’s tweets, they created a separate dataset that includes favorites and retweets information. Since Twitter archives don’t include this data, they collected roughly 3,200 tweets for each of them directly from the Twitter API, covering about the last 18 months of activity—a period during which both of them increased their tweeting frequency and follower counts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db36fd17",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "tweets_julia <- read_csv(\"data/juliasilge_tweets.csv\", show_col_types = FALSE)\n",
    "tweets_dave <- read_csv(\"data/drob_tweets.csv\", show_col_types = FALSE)\n",
    "tweets <- bind_rows(tweets_julia %>% \n",
    "                      mutate(person = \"Julia\"),\n",
    "                    tweets_dave %>% \n",
    "                      mutate(person = \"David\")) %>%\n",
    "  mutate(created_at = ymd_hms(created_at))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53395ee",
   "metadata": {},
   "source": [
    "Now that we have this smaller, more recent dataset, we’ll again use `unnest_tokens()` to convert the tweets into a tidy format. We’ll filter out all retweets and replies to focus solely on the original tweets posted directly by David and Julia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9c1cbd",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "tidy_tweets <- tweets %>% \n",
    "  filter(!str_detect(text, \"^(RT|@)\")) %>%\n",
    "  mutate(text = str_replace_all(text, replace_reg, \"\")) %>%\n",
    "  unnest_tokens(word, text, token = \"regex\", pattern = unnest_reg) %>%\n",
    "  filter(!word %in% stop_words$word,\n",
    "         !word %in% str_remove_all(stop_words$word, \"'\"))\n",
    "\n",
    "print(tidy_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc065adb",
   "metadata": {},
   "source": [
    "First, let’s examine how many times each person’s tweets were retweeted. We’ll calculate the total retweet count separately for David and Julia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df81bc2",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "totals <- tidy_tweets %>% \n",
    "  group_by(person, id) %>% \n",
    "  summarise(rts = first(retweets)) %>% \n",
    "  group_by(person) %>% \n",
    "  summarise(total_rts = sum(rts))\n",
    "\n",
    "print(totals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db77361b",
   "metadata": {},
   "source": [
    "First, we group by person, tweet, and word to summarize the retweet counts per tweet-word-person combination, counting how many times each word was retweeted in each tweet. Then, we group again by person and word to calculate the median retweet count for each word-person pair and count the total usage of each word by each person, saving that in a column called `uses`. After that, we join this summary with the total retweet counts per person. Finally, we filter the data to include only words that were mentioned at least five times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0760a6d8",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "word_by_rts <- tidy_tweets %>% \n",
    "  group_by(id, word, person) %>% \n",
    "  summarise(rts = first(retweets)) %>% \n",
    "  group_by(person, word) %>% \n",
    "  summarise(retweets = median(rts), uses = n()) %>%\n",
    "  left_join(totals) %>%\n",
    "  filter(retweets != 0) %>%\n",
    "  ungroup()\n",
    "\n",
    "word_by_rts %>% \n",
    "  filter(uses >= 5) %>%\n",
    "  arrange(desc(retweets)) %>%\n",
    "  print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7432db2d",
   "metadata": {},
   "source": [
    "At the top of this sorted data frame, the most retweeted words for both Julia and David relate to packages they contribute to, such as *gganimate* and *tidytext*. Next, we can create a plot showing the words with the highest median retweet counts for each account, highlighting which terms tend to garner more engagement on their tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023addb6",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "word_by_rts %>%\n",
    "  filter(uses >= 5) %>%\n",
    "  group_by(person) %>%\n",
    "  slice_max(retweets, n = 10) %>% \n",
    "  arrange(retweets) %>%\n",
    "  ungroup() %>%\n",
    "  mutate(word = factor(word, unique(word))) %>%\n",
    "  ungroup() %>%\n",
    "  ggplot(aes(word, retweets, fill = person)) +\n",
    "  geom_col(show.legend = FALSE) +\n",
    "  facet_wrap(~ person, scales = \"free\", ncol = 2) +\n",
    "  coord_flip() +\n",
    "  labs(x = NULL, \n",
    "       y = \"Median # of retweets for tweets containing each word\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b6f7b7",
   "metadata": {},
   "source": [
    "We notice many words related to R packages, including *tidytext*—the very package this book focuses on! The “0” values for David come from tweets mentioning package version numbers, like “broom 0.4.0” and similar.\n",
    "\n",
    "Using a similar approach, we can analyze which words correspond to higher numbers of favorites. It will be interesting to see whether those words differ from the ones that lead to more retweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa80cc3",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "totals <- tidy_tweets %>% \n",
    "  group_by(person, id) %>% \n",
    "  summarise(favs = first(favorites)) %>% \n",
    "  group_by(person) %>% \n",
    "  summarise(total_favs = sum(favs))\n",
    "\n",
    "word_by_favs <- tidy_tweets %>% \n",
    "  group_by(id, word, person) %>% \n",
    "  summarise(favs = first(favorites)) %>% \n",
    "  group_by(person, word) %>% \n",
    "  summarise(favorites = median(favs), uses = n()) %>%\n",
    "  left_join(totals) %>%\n",
    "  filter(favorites != 0) %>%\n",
    "  ungroup()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7cb7ba",
   "metadata": {},
   "source": [
    "We have built the data frames we need. Now let’s make our visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4e2bd6",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "word_by_favs %>%\n",
    "  filter(uses >= 5) %>%\n",
    "  group_by(person) %>%\n",
    "  slice_max(favorites, n = 10) %>% \n",
    "  arrange(favorites) %>%\n",
    "  ungroup() %>%\n",
    "  mutate(word = factor(word, unique(word))) %>%\n",
    "  ungroup() %>%\n",
    "  ggplot(aes(word, favorites, fill = person)) +\n",
    "  geom_col(show.legend = FALSE) +\n",
    "  facet_wrap(~ person, scales = \"free\", ncol = 2) +\n",
    "  coord_flip() +\n",
    "  labs(x = NULL, \n",
    "       y = \"Median # of favorites for tweets containing each word\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7fec09",
   "metadata": {},
   "source": [
    "We notice some small differences between David and Julia, especially toward the lower end of the top 10 lists, but overall the words are mostly the same as those linked to retweets. Generally, the words that attract retweets also tend to attract favorites. A standout word for Julia in both charts is the hashtag for the NASA Datanauts program she’s been involved with."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac7a3be",
   "metadata": {},
   "source": [
    "## Case study: Mining NASA Metadata\n",
    "\n",
    "[NASA](https://www.nasa.gov) maintains over 32,000 datasets that span topics ranging from Earth science to aerospace engineering to NASA's internal operations. To better understand the relationships between these datasets, we can analyze their metadata.\n",
    "\n",
    "**What is metadata?**\n",
    "Metadata refers to information that describes other data. In the context of NASA's datasets, this includes details like each dataset's title, description, responsible NASA organization, and human-assigned keywords. The metadata helps users understand the content and purpose of a dataset but does *not* include the dataset's actual measurements or results.\n",
    "\n",
    "NASA strongly emphasizes open science, requiring [publicly accessible research outputs](https://www.nasa.gov/news-release/nasa-unveils-new-public-web-portal-for-research-results/). As part of this commitment, the metadata for all NASA datasets is available [online in JSON format](https://data.nasa.gov/data.json).\n",
    "\n",
    "Below, we’ll treat NASA’s metadata as a text dataset and apply tidy text analysis methods to it. Using tools like word co-occurrence analysis, correlations, tf-idf, and topic modeling, we’ll explore questions such as:\n",
    "\n",
    "* Can we identify relationships between datasets?\n",
    "* Are there clusters of datasets with similar themes?\n",
    "* How do different metadata fields (like titles, descriptions, and keywords) reveal patterns across NASA’s data catalog?\n",
    "\n",
    "This approach demonstrates how text mining techniques can be applied to real-world, domain-specific metadata—whether in the space industry or any other field dealing with large collections of text. Let’s dive into the NASA dataset and begin exploring."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36987025",
   "metadata": {},
   "source": [
    "### How Data is Organized at NASA\n",
    "\n",
    "We’ll start by downloading the JSON file and examining the field names contained in the metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a95e514",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "library(jsonlite)\n",
    "metadata <- fromJSON(\"https://data.nasa.gov/data.json\")\n",
    "names(metadata$dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8eea314",
   "metadata": {},
   "source": [
    "From this, we can pull details ranging from the publishing organization for each dataset to the type of license under which they’re released."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3763fc",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "class(metadata$dataset$title)\n",
    "\n",
    "class(metadata$dataset$description)\n",
    "\n",
    "class(metadata$dataset$keyword)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303298ba",
   "metadata": {},
   "source": [
    "The title and description fields are saved as character vectors, while the keywords are stored as a list of character vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4397331",
   "metadata": {},
   "source": [
    "#### Wrangling and Tidying the Data\n",
    "\n",
    "We’ll create separate tidy data frames for the title, description, and keywords, making sure to keep the dataset IDs with each so we can link them together later if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097b09fe",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "library(dplyr)\n",
    "\n",
    "nasa_title <- tibble(id = metadata$dataset$`_id`$`$oid`, \n",
    "                     title = metadata$dataset$title)\n",
    "print(nasa_title)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e27c0bb",
   "metadata": {},
   "source": [
    "These are just a few sample dataset titles that we’ll be working with. You’ll notice the NASA-assigned IDs are included, and that some datasets share the same title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28bcdc2d",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "nasa_desc <- tibble(id = metadata$dataset$`_id`$`$oid`, \n",
    "                    desc = metadata$dataset$description)\n",
    "\n",
    "nasa_desc %>% \n",
    "  select(desc) %>% \n",
    "  sample_n(5) %>%\n",
    "  print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5c58cc",
   "metadata": {},
   "source": [
    "Here are excerpts from a few selected description fields in the metadata.\n",
    "\n",
    "Next, we’ll create a tidy data frame for the keywords. Since these are stored in a list-column, we’ll use tidyr’s `unnest()` function to expand them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9ca392",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "library(tidyr)\n",
    "\n",
    "nasa_keyword <- tibble(id = metadata$dataset$`_id`$`$oid`, \n",
    "                       keyword = metadata$dataset$keyword) %>%\n",
    "  unnest(keyword)\n",
    "\n",
    "print(nasa_keyword)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}