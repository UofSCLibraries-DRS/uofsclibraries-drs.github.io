{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/CC_BY.png\"><br />\n",
    "\n",
    "Created by [Nathan Kelber](http://nkelber.com) and Ted Lawless for [JSTOR Labs](https://labs.jstor.org/) under [Creative Commons CC BY License](https://creativecommons.org/licenses/by/4.0/)<br />\n",
    "For questions/comments/improvements, email nathan.kelber@ithaka.org.<br />\n",
    "____\n",
    "# Tokenize Text Files with NLTK\n",
    "\n",
    "**Description:**\n",
    "This notebook takes as input:\n",
    "\n",
    "* Plain text files (.txt) in a zipped folder called 'texts' in the data folder\n",
    "* Metadata CSV file called 'metadata.csv' in the data folder (optional)\n",
    "\n",
    "and outputs a single JSON-L file containing the unigrams, bigrams, trigrams, full-text, and metadata. \n",
    "\n",
    "**Use Case:** For Researchers (Mostly code without explanation, not ideal for learners)\n",
    "\n",
    "**Difficulty:** Advanced\n",
    "\n",
    "**Completion time:** 10-15 minutes\n",
    "\n",
    "**Knowledge Required:** \n",
    "* Python Basics ([Start Python Basics 1](../../PythonForDataAnalysis/GettingStarted/basic/python-basics-1.ipynb))\n",
    "\n",
    "**Knowledge Recommended:**\n",
    "\n",
    "* [Python Intermediate 2](../../PythonForDataAnalysis/GettingStarted/intermediate/python-intermediate-2.ipynb)\n",
    "\n",
    "**Data Format:** .txt, .csv, .jsonl\n",
    "\n",
    "**Libraries Used:**\n",
    "* os\n",
    "* json\n",
    "* NLTK\n",
    "* gzip\n",
    "* nltk.corpus\n",
    "* collections\n",
    "* pandas\n",
    "\n",
    "**Research Pipeline:**\n",
    "\n",
    "1. Scan documents\n",
    "2. OCR files\n",
    "3. Clean up texts\n",
    "4. **Tokenize text files** (this notebook)\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Inputs\n",
    "\n",
    "### Texts (.txt)\n",
    "All the texts should be in plaintext format. The filenames may be used for reference, so give them descriptive names that will help you identify them for your analysis. Additional data about each text can be supplied in an optional CSV file described below.\n",
    "\n",
    "Place them in a folder called 'texts' then zip that folder into a single file called 'texts.zip'. The texts \n",
    "\n",
    "### Metadata (.csv) (Optional)\n",
    "A CSV file containing metadata may be included for analysis. For specifications, see\n",
    "* [Dataset Types](https://www.geeksforgeeks.org/data-science/what-is-dataset/#features-of-a-dataset)\n",
    "* [About the Dataset format](https://www.geeksforgeeks.org/data-science/what-is-dataset/#features-of-a-dataset)\n",
    "\n",
    "The fields may include the following:\n",
    "\n",
    "|Column Name|Description|\n",
    "|---|---|\n",
    "|id|a unique item ID (In JSTOR, this is a stable URL)|\n",
    "|title|the title for the document|\n",
    "|isPartOf|the larger work that holds this title (for example, a journal title)|\n",
    "|publicationYear|the year of publication|\n",
    "|doi|the digital object identifier|\n",
    "|docType|the type of document (for example, article or book)|\n",
    "|provider|the source or provider of the dataset|\n",
    "|datePublished|the publication date in yyyy-mm-dd format|\n",
    "|issueNumber|the issue number for a journal publication|\n",
    "|volumeNumber|the volume number for a journal publication|\n",
    "|url|a URL for the item and/or the item's metadata|\n",
    "|creator|the author or authors of the item|\n",
    "|language|the language or languages of the item (eng is the ISO 639 code for English)|\n",
    "|pageStart|the first page number of the print version|\n",
    "|pageEnd|the last page number of the print version|\n",
    "|placeOfPublication|the city of the publisher|\n",
    "|pageCount|the number of print pages in the item|\n",
    "|wordCount|the number of words in the item|\n",
    "|pagination|the page sequence in the print version|\n",
    "|publisher|the publisher for the item|\n",
    "|abstract|the abstract description for the document|\n",
    "|outputFormat|what data is available ([unigrams](https://machinelearningknowledge.ai/generating-unigram-bigram-trigram-and-ngrams-in-nltk/#Unigrams_or_1-grams), [bigrams](https://machinelearningknowledge.ai/generating-unigram-bigram-trigram-and-ngrams-in-nltk/#Bigrams_or_2-grams), [trigrams](https://machinelearningknowledge.ai/generating-unigram-bigram-trigram-and-ngrams-in-nltk/#Trigrams_or_3-grams), and/or full-text)|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../data/texts.zip', <http.client.HTTPMessage at 0x104323590>)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download sample data\n",
    "# Shakespeares Plays from The Folger Shakespeare\n",
    "# https://shakespeare.folger.edu/download-the-folger-shakespeare-complete-set/\n",
    "import urllib.request\n",
    "from pathlib import Path\n",
    "\n",
    "# Check if a data folder exists. If not, create it.\n",
    "data_folder = Path('../data/')\n",
    "data_folder.mkdir(exist_ok=True)\n",
    "\n",
    "zipfile_address = 'https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/data/texts.zip'\n",
    "urllib.request.urlretrieve(zipfile_address, '../data/texts.zip')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile, os, nltk, json, gzip, pandas as pd\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Various functions written for this notebook ###\n",
    "\n",
    "def convert_tuple_bigrams(tuples_to_convert):\n",
    "    \"\"\"Converts NLTK tuples into bigram strings\"\"\"\n",
    "    string_grams = []\n",
    "    for tuple_grams in tuples_to_convert:\n",
    "        first_word = tuple_grams[0]\n",
    "        second_word = tuple_grams[1]\n",
    "        gram_string = f'{first_word} {second_word}'\n",
    "        string_grams.append(gram_string)\n",
    "    return string_grams\n",
    "\n",
    "def convert_tuple_trigrams(tuples_to_convert):\n",
    "    \"\"\"Converts NLTK tuples into trigram strings\"\"\"\n",
    "    string_grams = []\n",
    "    for tuple_grams in tuples_to_convert:\n",
    "        first_word = tuple_grams[0]\n",
    "        second_word = tuple_grams[1]\n",
    "        third_word = tuple_grams[2]\n",
    "        gram_string = f'{first_word} {second_word} {third_word}'\n",
    "        string_grams.append(gram_string)\n",
    "    return string_grams\n",
    "\n",
    "def convert_strings_to_counts(string_grams):\n",
    "    \"\"\"Converts a Counter of n-grams into a dictionary\"\"\"\n",
    "    counter_of_grams = Counter(string_grams)\n",
    "    dict_of_grams = dict(counter_of_grams)\n",
    "    return dict_of_grams\n",
    "\n",
    "def update_metadata_from_csv():\n",
    "    \"\"\"Uses pandas to grab additional metadata fields from a CSV file then adds them to the JSON-L file.\n",
    "    Unused fields can be commented out.\"\"\"\n",
    "    title = df.loc[identifier, 'title']\n",
    "    isPartOf = df.loc[identifier, 'isPartOf']\n",
    "    publicationYear = str(df.loc[identifier, 'publicationYear'])\n",
    "    doi = df.loc[identifier, 'doi']\n",
    "    docType = df.loc[identifier, 'docType']\n",
    "    provider = df.loc[identifier, 'provider']\n",
    "    datePublished = df.loc[identifier, 'datePublished']\n",
    "    issueNumber = str(df.loc[identifier, 'issueNumber'])\n",
    "    volumeNumber = str(df.loc[identifier, 'volumeNumber'])\n",
    "    url = df.loc[identifier, 'url']\n",
    "    creator = df.loc[identifier, 'creator']\n",
    "    publisher = df.loc[identifier, 'publisher']\n",
    "    language = df.loc[identifier, 'language']\n",
    "    pageStart = df.loc[identifier, 'pageStart']\n",
    "    pageEnd = df.loc[identifier, 'pageEnd']\n",
    "    placeOfPublication = df.loc[identifier, 'placeOfPublication']\n",
    "    pageCount = str(df.loc[identifier, 'pageCount'])\n",
    "\n",
    "    data.update([   \n",
    "        ('title', title),\n",
    "        ('isPartOf', isPartOf),\n",
    "        ('publicationYear', publicationYear),\n",
    "        ('doi', doi),\n",
    "        ('docType', docType),\n",
    "        ('provider', provider),\n",
    "        ('datePublished', datePublished),\n",
    "        ('issueNumber', issueNumber),\n",
    "        ('volumeNumber', volumeNumber),\n",
    "        ('url', url),\n",
    "        ('creator', creator),\n",
    "        ('publisher', publisher),\n",
    "        ('language', language),\n",
    "        ('pageStart', pageStart),\n",
    "        ('pageEnd', pageEnd),\n",
    "        ('placeOfPublication', placeOfPublication),\n",
    "        ('pageCount', pageCount),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unzip Texts Folder (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zip file extracted successfully.\n"
     ]
    }
   ],
   "source": [
    "### Extract Zip File of Texts ###\n",
    "# The text file should extract into a folder\n",
    "# called 'texts'\n",
    "\n",
    "# Alternatively, skip this unzipping code cell.\n",
    "# Create a folder called 'texts' in the 'data' folder\n",
    "\n",
    "filename = '../data/texts.zip'\n",
    "\n",
    "try:\n",
    "    corpus_zip = zipfile. ZipFile(filename)\n",
    "    corpus_zip.extractall('../data/texts/')\n",
    "    corpus_zip.close()\n",
    "    print('Zip file extracted successfully.')\n",
    "except:\n",
    "    print('No zip file detected. Upload your zip file to the data folder.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for Metadata CSV (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No metadata CSV found.\n"
     ]
    }
   ],
   "source": [
    "### Check for a metadata CSV file ###\n",
    "\n",
    "csv_filename = 'metadata.csv'\n",
    "\n",
    "if os.path.exists(f'../data/{csv_filename}'):\n",
    "    csv_exists = True\n",
    "    print('Metadata CSV found.')\n",
    "else: \n",
    "    csv_exists = False\n",
    "    print('No metadata CSV found.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the Text Files into NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Establish root folder holding all text files ###\n",
    "# Create corpus using all text files in corpus_root\n",
    "# By default, this uses punkt tokenizer\n",
    "# See https://www.nltk.org/_modules/nltk/corpus/reader/plaintext.html\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "from nltk.tokenize import (TreebankWordTokenizer,\n",
    "                          word_tokenize,\n",
    "                          WordPunctTokenizer,\n",
    "                          TweetTokenizer,\n",
    "                          MWETokenizer)\n",
    "\n",
    "\n",
    "corpus_root = '../data/texts'\n",
    "corpus = PlaintextCorpusReader(corpus_root, '.*txt', word_tokenizer=WordPunctTokenizer())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus created from:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['a-midsummer-nights-dream_TXT_FolgerShakespeare.txt',\n",
       " 'alls-well-that-ends-well_TXT_FolgerShakespeare.txt',\n",
       " 'antony-and-cleopatra_TXT_FolgerShakespeare.txt',\n",
       " 'as-you-like-it_TXT_FolgerShakespeare.txt',\n",
       " 'coriolanus_TXT_FolgerShakespeare.txt',\n",
       " 'cymbeline_TXT_FolgerShakespeare.txt',\n",
       " 'hamlet_TXT_FolgerShakespeare.txt',\n",
       " 'henry-iv-part-1_TXT_FolgerShakespeare.txt',\n",
       " 'henry-iv-part-2_TXT_FolgerShakespeare.txt',\n",
       " 'henry-v_TXT_FolgerShakespeare.txt',\n",
       " 'henry-vi-part-1_TXT_FolgerShakespeare.txt',\n",
       " 'henry-vi-part-2_TXT_FolgerShakespeare.txt',\n",
       " 'henry-vi-part-3_TXT_FolgerShakespeare.txt',\n",
       " 'henry-viii_TXT_FolgerShakespeare.txt',\n",
       " 'julius-caesar_TXT_FolgerShakespeare.txt',\n",
       " 'king-john_TXT_FolgerShakespeare.txt',\n",
       " 'king-lear_TXT_FolgerShakespeare.txt',\n",
       " 'loves-labors-lost_TXT_FolgerShakespeare.txt',\n",
       " 'lucrece_TXT_FolgerShakespeare.txt',\n",
       " 'macbeth_TXT_FolgerShakespeare.txt',\n",
       " 'measure-for-measure_TXT_FolgerShakespeare.txt',\n",
       " 'much-ado-about-nothing_TXT_FolgerShakespeare.txt',\n",
       " 'othello_TXT_FolgerShakespeare.txt',\n",
       " 'pericles_TXT_FolgerShakespeare.txt',\n",
       " 'richard-ii_TXT_FolgerShakespeare.txt',\n",
       " 'richard-iii_TXT_FolgerShakespeare.txt',\n",
       " 'romeo-and-juliet_TXT_FolgerShakespeare.txt',\n",
       " 'shakespeares-sonnets_TXT_FolgerShakespeare.txt',\n",
       " 'the-comedy-of-errors_TXT_FolgerShakespeare.txt',\n",
       " 'the-merchant-of-venice_TXT_FolgerShakespeare.txt',\n",
       " 'the-merry-wives-of-windsor_TXT_FolgerShakespeare.txt',\n",
       " 'the-phoenix-and-turtle_TXT_FolgerShakespeare.txt',\n",
       " 'the-taming-of-the-shrew_TXT_FolgerShakespeare.txt',\n",
       " 'the-tempest_TXT_FolgerShakespeare.txt',\n",
       " 'the-two-gentlemen-of-verona_TXT_FolgerShakespeare.txt',\n",
       " 'the-two-noble-kinsmen_TXT_FolgerShakespeare.txt',\n",
       " 'the-winters-tale_TXT_FolgerShakespeare.txt',\n",
       " 'timon-of-athens_TXT_FolgerShakespeare.txt',\n",
       " 'titus-andronicus_TXT_FolgerShakespeare.txt',\n",
       " 'troilus-and-cressida_TXT_FolgerShakespeare.txt',\n",
       " 'twelfth-night_TXT_FolgerShakespeare.txt',\n",
       " 'venus-and-adonis_TXT_FolgerShakespeare.txt']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Print all File IDs in corpus based on text file names ###\n",
    "text_list = corpus.fileids()\n",
    "print('Corpus created from:')\n",
    "list(text_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate and Output Data to JSON-L File\n",
    "**If an old json-l file already exists, this process will overwrite it**\n",
    "\n",
    "For each text, this code will:\n",
    "1. Gather unigrams, bigrams, trigrams, and full text\n",
    "2. Compute word counts\n",
    "3. Check for additional metadata in a CSV file\n",
    "4. Write any data to JSON-L file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text a-midsummer-nights-dream_TXT_FolgerShakespeare.txt written to json-l file.\n",
      "Text alls-well-that-ends-well_TXT_FolgerShakespeare.txt written to json-l file.\n",
      "Text antony-and-cleopatra_TXT_FolgerShakespeare.txt written to json-l file.\n",
      "Text as-you-like-it_TXT_FolgerShakespeare.txt written to json-l file.\n",
      "Text coriolanus_TXT_FolgerShakespeare.txt written to json-l file.\n",
      "Text cymbeline_TXT_FolgerShakespeare.txt written to json-l file.\n",
      "Text hamlet_TXT_FolgerShakespeare.txt written to json-l file.\n",
      "Text henry-iv-part-1_TXT_FolgerShakespeare.txt written to json-l file.\n",
      "Text henry-iv-part-2_TXT_FolgerShakespeare.txt written to json-l file.\n",
      "Text henry-v_TXT_FolgerShakespeare.txt written to json-l file.\n",
      "Text henry-vi-part-1_TXT_FolgerShakespeare.txt written to json-l file.\n",
      "Text henry-vi-part-2_TXT_FolgerShakespeare.txt written to json-l file.\n",
      "Text henry-vi-part-3_TXT_FolgerShakespeare.txt written to json-l file.\n",
      "Text henry-viii_TXT_FolgerShakespeare.txt written to json-l file.\n",
      "Text julius-caesar_TXT_FolgerShakespeare.txt written to json-l file.\n",
      "Text king-john_TXT_FolgerShakespeare.txt written to json-l file.\n",
      "Text king-lear_TXT_FolgerShakespeare.txt written to json-l file.\n",
      "Text loves-labors-lost_TXT_FolgerShakespeare.txt written to json-l file.\n",
      "Text lucrece_TXT_FolgerShakespeare.txt written to json-l file.\n",
      "Text macbeth_TXT_FolgerShakespeare.txt written to json-l file.\n",
      "Text measure-for-measure_TXT_FolgerShakespeare.txt written to json-l file.\n",
      "Text much-ado-about-nothing_TXT_FolgerShakespeare.txt written to json-l file.\n",
      "Text othello_TXT_FolgerShakespeare.txt written to json-l file.\n",
      "Text pericles_TXT_FolgerShakespeare.txt written to json-l file.\n",
      "Text richard-ii_TXT_FolgerShakespeare.txt written to json-l file.\n",
      "Text richard-iii_TXT_FolgerShakespeare.txt written to json-l file.\n",
      "Text romeo-and-juliet_TXT_FolgerShakespeare.txt written to json-l file.\n",
      "Text shakespeares-sonnets_TXT_FolgerShakespeare.txt written to json-l file.\n",
      "Text the-comedy-of-errors_TXT_FolgerShakespeare.txt written to json-l file.\n",
      "Text the-merchant-of-venice_TXT_FolgerShakespeare.txt written to json-l file.\n",
      "Text the-merry-wives-of-windsor_TXT_FolgerShakespeare.txt written to json-l file.\n",
      "Text the-phoenix-and-turtle_TXT_FolgerShakespeare.txt written to json-l file.\n",
      "Text the-taming-of-the-shrew_TXT_FolgerShakespeare.txt written to json-l file.\n",
      "Text the-tempest_TXT_FolgerShakespeare.txt written to json-l file.\n",
      "Text the-two-gentlemen-of-verona_TXT_FolgerShakespeare.txt written to json-l file.\n",
      "Text the-two-noble-kinsmen_TXT_FolgerShakespeare.txt written to json-l file.\n",
      "Text the-winters-tale_TXT_FolgerShakespeare.txt written to json-l file.\n",
      "Text timon-of-athens_TXT_FolgerShakespeare.txt written to json-l file.\n",
      "Text titus-andronicus_TXT_FolgerShakespeare.txt written to json-l file.\n",
      "Text troilus-and-cressida_TXT_FolgerShakespeare.txt written to json-l file.\n",
      "Text twelfth-night_TXT_FolgerShakespeare.txt written to json-l file.\n",
      "Text venus-and-adonis_TXT_FolgerShakespeare.txt written to json-l file.\n",
      "\n",
      "42 texts written to my_data.jsonl.\n"
     ]
    }
   ],
   "source": [
    "### Create the JSON-L file and gzip it ###\n",
    "\n",
    "# For every text: \n",
    "# 1. Compute unigrams, bigrams, trigrams, and wordCount\n",
    "# 2. Append the data to a JSON-L file\n",
    "# After all data is written, compress the dataset using gzip\n",
    "## **If the JSONL file exists, it will be overwritten**\n",
    "\n",
    "# Define the file output name\n",
    "output_filename = 'my_data.jsonl'\n",
    "\n",
    "# Delete output files if they already exist\n",
    "if os.path.exists(f'../data/{output_filename}'):\n",
    "    os.remove(f'../data/{output_filename}')\n",
    "    print(f'Overwriting old version of {output_filename}')\n",
    "\n",
    "if os.path.exists(f'../data/{output_filename}.gz'):\n",
    "    os.remove(f'../data/{output_filename}.gz')\n",
    "    print(f'Overwriting old version of {output_filename}.gz\\n')\n",
    "                  \n",
    "\n",
    "for text in text_list:\n",
    "    \n",
    "    # Create identifier from filename\n",
    "    identifier = text[:-4]\n",
    "    \n",
    "    # Compute unigrams\n",
    "    unigrams = corpus.words(text)\n",
    "    unigramCount = convert_strings_to_counts(unigrams)\n",
    "    \n",
    "    # Compute bigrams\n",
    "    tuple_bigrams = list(nltk.bigrams(unigrams))\n",
    "    string_bigrams = convert_tuple_bigrams(tuple_bigrams)\n",
    "    bigramCount = convert_strings_to_counts(string_bigrams)\n",
    "    \n",
    "    # Compute trigrams\n",
    "    tuple_trigrams = list(nltk.trigrams(unigrams))\n",
    "    string_trigrams = convert_tuple_trigrams(tuple_trigrams)\n",
    "    trigramCount = convert_strings_to_counts(string_trigrams)\n",
    "    \n",
    "    # Compute fulltext\n",
    "    with open(f'../data/texts/{text}', 'r') as file:\n",
    "        fullText = file.read()\n",
    "    \n",
    "    # Calculate wordCount\n",
    "    wordCount = 0\n",
    "    for counts in unigramCount.values():\n",
    "        wordCount = wordCount + counts\n",
    "  \n",
    "    # Create a dictionary `data` to hold each document's data\n",
    "    # Including id, wordCount, outputFormat, unigramCount,\n",
    "    # bigramCount, trigramCount, fullText, etc.\n",
    "    data = {}\n",
    "    \n",
    "    data.update([\n",
    "        ('id', identifier),\n",
    "        ('title', identifier),\n",
    "        ('outputFormat', ['unigram', 'bigram', 'trigram', 'fullText']),\n",
    "        ('wordCount', wordCount),\n",
    "        ('fullText', fullText),\n",
    "        ('unigramCount', unigramCount), \n",
    "        ('bigramCount', bigramCount), \n",
    "        ('trigramCount', trigramCount)\n",
    "    ])\n",
    "    \n",
    "    # Add additional metadata if there is a metadata.csv available\n",
    "    if csv_exists == True:\n",
    "        # Read in the CSV file and set the index\n",
    "        df = pd.read_csv(f'./data/{csv_filename}')\n",
    "        df.set_index('id', inplace=True)\n",
    "        # Update Metadata\n",
    "        update_metadata_from_csv()\n",
    "        \n",
    "    \n",
    "    # Write the document to the json file  \n",
    "    with open(f'../data/{output_filename}', 'a') as outfile:\n",
    "        json.dump(data, outfile)\n",
    "        outfile.write('\\n')\n",
    "        print(f'Text {text} written to json-l file.')\n",
    "\n",
    "print('\\n' + str(len(text_list)) + f' texts written to {output_filename}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gzip the JSON-L file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compression complete. \n",
      "my_data.jsonl.gz has been created.\n"
     ]
    }
   ],
   "source": [
    "# GZip dataset\n",
    "\n",
    "f_in = open(f'../data/{output_filename}', 'rb')\n",
    "f_out = gzip.open(f'../data/{output_filename}.gz', 'wb')\n",
    "f_out.writelines(f_in)\n",
    "f_out.close()\n",
    "f_in.close()\n",
    "print(f'Compression complete. \\n{output_filename}.gz has been created.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** The Constellate Lab saves Jupyter Notebooks but not dataset files. Be sure to save your dataset to your local machine or cloud storage."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "300.014px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}