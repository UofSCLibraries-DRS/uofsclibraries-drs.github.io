
<!DOCTYPE html>


<html lang="en" data-content_root="../../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Performing Analysis on Tidy Text &#8212; USC Libraries - DRS Resources for Data Literacy</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../../../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../../../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../../../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'src/TextMining/With_R/Tidy_Text_Analysis';</script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
        
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../../../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../../_static/USC_Libraries_logo_horizontal_RGB_2C.png" class="logo__image only-light" alt="USC Libraries - DRS Resources for Data Literacy - Home"/>
    <script>document.write(`<img src="../../../_static/USC_Libraries_logo_horizontal_RGB_2C.png" class="logo__image only-dark" alt="USC Libraries - DRS Resources for Data Literacy - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../../intro.html">
                    Introduction
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Data Wrangling with Python</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../Python/BasicPython.html">Basic Python Tutorials</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Python/IntermediatePython.html">Intermediate Python Tutorials</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Python/PandasPython.html">Pandas Python Tutorials</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../DataProcessingBasics/DataCleaning_Basics.html">Data Cleaning Basics</a></li>


</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Data Wrangling with R</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../R_Code/Data_Wrangling.html">Data Wrangling with R</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Data Extraction</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../API/API.html">Using APIs</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Text Mining</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../IntroductionTextAnalysis.html">Introduction to Text Analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="../AdvancedTextAnalysis.html">Advanced Text Analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="../MultiLingualNer.html">Named Entity Recognition (NER)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../overview/Tidy_Text.html">Tidy Text</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Data Visualization</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../overview/Charts.html">Bar, Line, Scatter Plot</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Wolfe-notebooks/Maps.html">Creating Maps with Python</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Machine Learning</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../overview/MachineLearning.html">Brief Overview</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">App Development</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../overview/AppDevelopment.html">App Development</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/UofSCLibraries-DRS/uofsclibraries-drs.github.io" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/UofSCLibraries-DRS/uofsclibraries-drs.github.io/issues/new?title=Issue%20on%20page%20%2Fsrc/TextMining/With_R/Tidy_Text_Analysis.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../../_sources/src/TextMining/With_R/Tidy_Text_Analysis.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Performing Analysis on Tidy Text</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#acknowledgements">Acknowledgements</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#packages-to-install">Packages to Install</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#sentiment-analysis">Sentiment Analysis</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-sentiments-dataset">The <code class="docutils literal notranslate"><span class="pre">sentiments</span></code> Dataset</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sentiment-analysis-with-inner-join">Sentiment Analysis with Inner Join</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#comparing-sentiment-dictionaries">Comparing Sentiment Dictionaries</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#most-common-positive-and-negative-words">Most Common Positive and Negative Words</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#wordclouds">Wordclouds</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#looking-beyond-just-words">Looking Beyond just Words</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#analyzing-word-and-document-frequency">Analyzing Word and Document Frequency</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#term-frequency-in-jane-austen-s-novels">Term Frequency in Jane Austen’s Novels</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#zipfs-law">Zipf’s Law</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-bind-tf-idf-function">The <code class="docutils literal notranslate"><span class="pre">bind_tf_idf()</span></code> Function</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#a-corpus-of-physics-texts">A Corpus of Physics Texts</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#relationships-between-words">Relationships Between Words</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tokenizing-by-n-gram">Tokenizing by n-gram</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#counting-and-filtering-n-grams">Counting and Filtering n-grams</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#analyzing-bigrams">Analyzing bigrams</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#using-bigrams-to-provide-context">Using Bigrams to Provide Context</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#visualizing-a-network-of-bigrams-with-ggraph">Visualizing a Network of Bigrams with ggraph</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#visualizing-bigrams-in-other-texts">Visualizing Bigrams in Other Texts</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#counting-and-correlating-pairs-of-words-with-the-widyr-package">Counting and Correlating Pairs of Words with the widyr Package</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#counting-and-correlating-among-sections">Counting and Correlating Among Sections</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#pairwise-correlation">Pairwise Correlation</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#converting-to-and-from-non-tidy-formats">Converting To and From Non-tidy Formats</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tidying-a-document-term-matrix">Tidying a Document-term Matrix</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#tidying-documenttermmatrix-objects">Tidying DocumentTermMatrix Objects</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#tidying-dfm-objects">Tidying dfm Objects</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#casting-tidy-text-data-into-a-matrix">Casting Tidy Text Data Into a Matrix</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tidying-corpus-objects-with-metadata">Tidying Corpus Objects with Metadata</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#topic-modeling">Topic Modeling</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#latent-dirichlet-allocation">Latent Dirichlet Allocation</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#word-topic-probabilities">Word-topic Probabilities</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#document-topic-probabilities">Document-topic Probabilities</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-the-great-library-heist">Example: The Great Library Heist</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#lda-on-chapters">LDA on Chapters</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#per-document-classification">Per-document classification</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#by-word-assignments-augment">By Word Assignments: <code class="docutils literal notranslate"><span class="pre">augment</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#alternative-lda-implementations">Alternative LDA Implementations</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="performing-analysis-on-tidy-text">
<h1>Performing Analysis on Tidy Text<a class="headerlink" href="#performing-analysis-on-tidy-text" title="Link to this heading">#</a></h1>
<section id="acknowledgements">
<h2>Acknowledgements<a class="headerlink" href="#acknowledgements" title="Link to this heading">#</a></h2>
<p>We would like to acknowledge the work of Julia Silge and David Robinson, whose materials were used under the terms of the Creative Commons Attribution-NonCommercial-ShareAlike 3.0 United States License. Their contributions to open data science education, particularly the <a class="reference external" href="https://www.tidytextmining.com">Text Mining with R</a>, provided valuable resources for this project.</p>
<p>This notebook was created by Meara Cox using their code and examples as a foundation, with additional explanations and adaptations to support the goals of this project.</p>
</section>
<section id="packages-to-install">
<h2>Packages to Install<a class="headerlink" href="#packages-to-install" title="Link to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">install.packages</span><span class="p">(</span><span class="s">&quot;tidytext&quot;</span><span class="p">)</span>
<span class="nf">install.packages</span><span class="p">(</span><span class="s">&quot;textdata&quot;</span><span class="p">)</span>
<span class="nf">install.packages</span><span class="p">(</span><span class="s">&quot;dplyr&quot;</span><span class="p">)</span>
<span class="nf">install.packages</span><span class="p">(</span><span class="s">&quot;stringr&quot;</span><span class="p">)</span>
<span class="nf">install.packages</span><span class="p">(</span><span class="s">&quot;janeaustenr&quot;</span><span class="p">)</span>
<span class="nf">install.packages</span><span class="p">(</span><span class="s">&quot;wordcloud&quot;</span><span class="p">)</span>
<span class="nf">install.packages</span><span class="p">(</span><span class="s">&quot;reshape2&quot;</span><span class="p">)</span>
<span class="nf">install.packages</span><span class="p">(</span><span class="s">&quot;igraph&quot;</span><span class="p">)</span>
<span class="nf">install.packages</span><span class="p">(</span><span class="s">&quot;ggraph&quot;</span><span class="p">)</span>
<span class="nf">install.packages</span><span class="p">(</span><span class="s">&quot;widyr&quot;</span><span class="p">)</span>
<span class="nf">install.packages</span><span class="p">(</span><span class="s">&quot;topicmodels&quot;</span><span class="p">)</span>
<span class="nf">install.packages</span><span class="p">(</span><span class="s">&quot;quanteda&quot;</span><span class="p">)</span>
<span class="nf">install.packages</span><span class="p">(</span><span class="s">&quot;rvest&quot;</span><span class="p">)</span>
<span class="nf">install.packages</span><span class="p">(</span><span class="s">&quot;tibble&quot;</span><span class="p">)</span>
<span class="nf">install.packages</span><span class="p">(</span><span class="s">&quot;purrr&quot;</span><span class="p">)</span>
<span class="nf">install.packages</span><span class="p">(</span><span class="s">&quot;mallet&quot;</span><span class="p">)</span>
<span class="nf">install.packages</span><span class="p">(</span><span class="s">&quot;httr&quot;</span><span class="p">)</span>
<span class="nf">install.packages</span><span class="p">(</span><span class="s">&quot;jsonlite&quot;</span><span class="p">)</span>
<span class="nf">install.packages</span><span class="p">(</span><span class="s">&quot;tidyverse&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>The downloaded binary packages are in
	/var/folders/2h/84wxzls579b1yv00g4jj02fh0000gn/T//RtmptdIsPl/downloaded_packages

The downloaded binary packages are in
	/var/folders/2h/84wxzls579b1yv00g4jj02fh0000gn/T//RtmptdIsPl/downloaded_packages

The downloaded binary packages are in
	/var/folders/2h/84wxzls579b1yv00g4jj02fh0000gn/T//RtmptdIsPl/downloaded_packages

The downloaded binary packages are in
	/var/folders/2h/84wxzls579b1yv00g4jj02fh0000gn/T//RtmptdIsPl/downloaded_packages

The downloaded binary packages are in
	/var/folders/2h/84wxzls579b1yv00g4jj02fh0000gn/T//RtmptdIsPl/downloaded_packages

The downloaded binary packages are in
	/var/folders/2h/84wxzls579b1yv00g4jj02fh0000gn/T//RtmptdIsPl/downloaded_packages

The downloaded binary packages are in
	/var/folders/2h/84wxzls579b1yv00g4jj02fh0000gn/T//RtmptdIsPl/downloaded_packages

The downloaded binary packages are in
	/var/folders/2h/84wxzls579b1yv00g4jj02fh0000gn/T//RtmptdIsPl/downloaded_packages

The downloaded binary packages are in
	/var/folders/2h/84wxzls579b1yv00g4jj02fh0000gn/T//RtmptdIsPl/downloaded_packages

The downloaded binary packages are in
	/var/folders/2h/84wxzls579b1yv00g4jj02fh0000gn/T//RtmptdIsPl/downloaded_packages

The downloaded binary packages are in
	/var/folders/2h/84wxzls579b1yv00g4jj02fh0000gn/T//RtmptdIsPl/downloaded_packages

The downloaded binary packages are in
	/var/folders/2h/84wxzls579b1yv00g4jj02fh0000gn/T//RtmptdIsPl/downloaded_packages

The downloaded binary packages are in
	/var/folders/2h/84wxzls579b1yv00g4jj02fh0000gn/T//RtmptdIsPl/downloaded_packages

The downloaded binary packages are in
	/var/folders/2h/84wxzls579b1yv00g4jj02fh0000gn/T//RtmptdIsPl/downloaded_packages

The downloaded binary packages are in
	/var/folders/2h/84wxzls579b1yv00g4jj02fh0000gn/T//RtmptdIsPl/downloaded_packages

The downloaded binary packages are in
	/var/folders/2h/84wxzls579b1yv00g4jj02fh0000gn/T//RtmptdIsPl/downloaded_packages

The downloaded binary packages are in
	/var/folders/2h/84wxzls579b1yv00g4jj02fh0000gn/T//RtmptdIsPl/downloaded_packages

The downloaded binary packages are in
	/var/folders/2h/84wxzls579b1yv00g4jj02fh0000gn/T//RtmptdIsPl/downloaded_packages
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>also installing the dependencies &#39;ps&#39;, &#39;sass&#39;, &#39;processx&#39;, &#39;highr&#39;, &#39;xfun&#39;, &#39;bslib&#39;, &#39;fontawesome&#39;, &#39;jquerylib&#39;, &#39;tinytex&#39;, &#39;blob&#39;, &#39;DBI&#39;, &#39;gargle&#39;, &#39;ids&#39;, &#39;rematch2&#39;, &#39;textshaping&#39;, &#39;callr&#39;, &#39;knitr&#39;, &#39;rmarkdown&#39;, &#39;conflicted&#39;, &#39;dbplyr&#39;, &#39;dtplyr&#39;, &#39;googledrive&#39;, &#39;googlesheets4&#39;, &#39;modelr&#39;, &#39;ragg&#39;, &#39;reprex&#39;
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>The downloaded binary packages are in
	/var/folders/2h/84wxzls579b1yv00g4jj02fh0000gn/T//RtmptdIsPl/downloaded_packages
</pre></div>
</div>
</div>
</div>
</section>
<section id="sentiment-analysis">
<h2>Sentiment Analysis<a class="headerlink" href="#sentiment-analysis" title="Link to this heading">#</a></h2>
<p>In the previous chapter, we explored the tidy text format and how it helps analyze word frequency and compare documents. Now, we shift focus to opinion mining or sentiment analysis, where we assess the emotional tone of a text. Just as human readers infer emotions like positivity, negativity, surprise, or disgust from words, we can use text mining tools to programmatically analyze a text’s emotional content. A common approach to sentiment analysis is to break the text into individual words, assign sentiment scores to those words, and sum them to understand the overall sentiment of the text. While this isn’t the only method for sentiment analysis, it is widely used and fits naturally within the tidy data framework and toolset.</p>
<section id="the-sentiments-dataset">
<h3>The <code class="docutils literal notranslate"><span class="pre">sentiments</span></code> Dataset<a class="headerlink" href="#the-sentiments-dataset" title="Link to this heading">#</a></h3>
<p>As discussed earlier, there are several methods and dictionaries available for evaluating opinion or emotion in text. The <strong>tidytext</strong> package provides access to three commonly used general-purpose sentiment lexicons: <strong>AFINN</strong>, developed by <a class="reference external" href="https://www2.imm.dtu.dk/pubdb/pubs/6010-full.html">Finn Årup Nielsen</a>; <strong>bing</strong>, from <a class="reference external" href="https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html">Bing Liu and collaborators</a>; and <strong>nrc</strong>, created by <a class="reference external" href="https://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm">Saif Mohammad and Peter Turney</a>. All three lexicons are based on unigrams, meaning they assign sentiment scores to individual words. The <em>nrc</em> lexicon categorizes words as positive, negative, or into specific emotions like joy, anger, sadness, and trust, using a simple “yes”/”no” system. The <em>bing</em> lexicon also applies a binary positive or negative label to each word, while the <code class="docutils literal notranslate"><span class="pre">AFINN</span></code> lexicon assigns numerical sentiment scores ranging from -5 (most negative) to 5 (most positive). These lexicons are distributed under different licenses, so it’s important to review the terms and ensure they align with your project requirements before use.</p>
<p>The <a class="reference external" href="https://juliasilge.github.io/tidytext/reference/get_sentiments.html"><code class="docutils literal notranslate"><span class="pre">get_sentiments()</span></code></a> function allows us to easily access specific sentiment lexicons along with their corresponding sentiment measures. By specifying the lexicon name (such as <code class="docutils literal notranslate"><span class="pre">&quot;afinn&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;bing&quot;</span></code>, or <code class="docutils literal notranslate"><span class="pre">&quot;nrc&quot;</span></code>), we can retrieve the appropriate set of words and their sentiment scores or categories, making it straightforward to incorporate sentiment analysis into a tidy text workflow.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">library</span><span class="p">(</span><span class="n">tidytext</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="nf">get_sentiments</span><span class="p">(</span><span class="s">&quot;afinn&quot;</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span># A tibble: 2,477 x 2
   word       value
   &lt;chr&gt;      &lt;dbl&gt;
 1 abandon       -<span class=" -Color -Color-Red">2</span>
 2 abandoned     -<span class=" -Color -Color-Red">2</span>
 3 abandons      -<span class=" -Color -Color-Red">2</span>
 4 abducted      -<span class=" -Color -Color-Red">2</span>
 5 abduction     -<span class=" -Color -Color-Red">2</span>
 6 abductions    -<span class=" -Color -Color-Red">2</span>
 7 abhor         -<span class=" -Color -Color-Red">3</span>
 8 abhorred      -<span class=" -Color -Color-Red">3</span>
 9 abhorrent     -<span class=" -Color -Color-Red">3</span>
10 abhors        -<span class=" -Color -Color-Red">3</span>
# i 2,467 more rows
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">print</span><span class="p">(</span><span class="nf">get_sentiments</span><span class="p">(</span><span class="s">&quot;bing&quot;</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span># A tibble: 6,786 x 2
   word        sentiment
   &lt;chr&gt;       &lt;chr&gt;    
 1 2-faces     negative 
 2 abnormal    negative 
 3 abolish     negative 
 4 abominable  negative 
 5 abominably  negative 
 6 abominate   negative 
 7 abomination negative 
 8 abort       negative 
 9 aborted     negative 
10 aborts      negative 
# i 6,776 more rows
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">print</span><span class="p">(</span><span class="nf">get_sentiments</span><span class="p">(</span><span class="s">&quot;nrc&quot;</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span># A tibble: 13,872 x 2
   word        sentiment
   &lt;chr&gt;       &lt;chr&gt;    
 1 abacus      trust    
 2 abandon     fear     
 3 abandon     negative 
 4 abandon     sadness  
 5 abandoned   anger    
 6 abandoned   fear     
 7 abandoned   negative 
 8 abandoned   sadness  
 9 abandonment anger    
10 abandonment fear     
# i 13,862 more rows
</pre></div>
</div>
</div>
</div>
<p>The sentiment lexicons used in text mining were developed through either crowdsourcing platforms like Amazon Mechanical Turk or the manual effort of individual researchers, and they were validated using datasets such as crowdsourced opinions, movie or restaurant reviews, or Twitter posts. Because of this, applying these lexicons to texts that differ greatly in style or era—like 200-year-old narrative fiction—may produce less precise results, although shared vocabulary still allows meaningful sentiment analysis. In addition to these general-purpose lexicons, domain-specific options exist, such as those designed for financial texts. These dictionary-based methods work by summing the sentiment scores of individual words but have limitations, as they don’t account for context, qualifiers, or negation (e.g., “not good”). For many types of narrative text where sarcasm or constant negation is minimal, this limitation is less significant, though later chapters explore strategies for handling negation. It’s also important to choose an appropriate text chunk size for analysis—large text sections can have sentiment scores that cancel each other out, whereas sentence- or paragraph-level analysis often yields clearer results.</p>
</section>
<section id="sentiment-analysis-with-inner-join">
<h3>Sentiment Analysis with Inner Join<a class="headerlink" href="#sentiment-analysis-with-inner-join" title="Link to this heading">#</a></h3>
<p>When text data is in a tidy format, performing sentiment analysis becomes as simple as using an inner join. This highlights one of the key advantages of approaching text mining as a tidy data task—the tools and workflows remain consistent and intuitive. Just as removing stop words involves an <code class="docutils literal notranslate"><span class="pre">anti_join</span></code>, sentiment analysis works by applying an <code class="docutils literal notranslate"><span class="pre">inner_join</span></code> to combine the tokenized text with a sentiment lexicon, matching words in the text to their corresponding sentiment values. This tidy approach simplifies the process and integrates seamlessly with other tidyverse functions.</p>
<p>To find the most common joy-related words in <em>Emma</em> using the NRC lexicon, we first need to convert the novel’s text into a tidy format with one word per row, using <code class="docutils literal notranslate"><span class="pre">unnest_tokens()</span></code>, just like we did earlier. To preserve important context, we can also create additional columns that track which line and chapter each word appears in. This is done using <code class="docutils literal notranslate"><span class="pre">group_by()</span></code> to organize the text by book and <code class="docutils literal notranslate"><span class="pre">mutate()</span></code> to assign line numbers and detect chapters, often using a regular expression. Once the text is structured this way, we can easily filter for words associated with joy in the NRC lexicon and analyze their frequency within the novel.</p>
<p>Notice that we named the output column from <a class="reference external" href="https://juliasilge.github.io/tidytext/reference/unnest_tokens.html"><code class="docutils literal notranslate"><span class="pre">unnest_tokens()</span></code></a> as <strong>word</strong>, which is a practical and consistent choice. The sentiment lexicons and stop word datasets provided by <strong>tidytext</strong> also use <strong>word</strong> as the column name for individual tokens. By keeping this naming consistent, performing operations like <code class="docutils literal notranslate"><span class="pre">inner_join()</span></code> for sentiment analysis or <code class="docutils literal notranslate"><span class="pre">anti_join()</span></code> for removing stop words becomes straightforward and seamless, avoiding unnecessary renaming or data wrangling.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">library</span><span class="p">(</span><span class="n">janeaustenr</span><span class="p">)</span>
<span class="nf">library</span><span class="p">(</span><span class="n">dplyr</span><span class="p">)</span>
<span class="nf">library</span><span class="p">(</span><span class="n">stringr</span><span class="p">)</span>

<span class="n">tidy_books</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">austen_books</span><span class="p">()</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">group_by</span><span class="p">(</span><span class="n">book</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">mutate</span><span class="p">(</span>
<span class="w">    </span><span class="n">linenumber</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">row_number</span><span class="p">(),</span>
<span class="w">    </span><span class="n">chapter</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">cumsum</span><span class="p">(</span><span class="nf">str_detect</span><span class="p">(</span><span class="n">text</span><span class="p">,</span><span class="w"> </span>
<span class="w">                                </span><span class="nf">regex</span><span class="p">(</span><span class="s">&quot;^chapter [\\divxlc]&quot;</span><span class="p">,</span><span class="w"> </span>
<span class="w">                                      </span><span class="n">ignore_case</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">))))</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">ungroup</span><span class="p">()</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">unnest_tokens</span><span class="p">(</span><span class="n">word</span><span class="p">,</span><span class="w"> </span><span class="n">text</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Notice that we named the output column from <a class="reference external" href="https://juliasilge.github.io/tidytext/reference/unnest_tokens.html"><code class="docutils literal notranslate"><span class="pre">unnest_tokens()</span></code></a> as <code class="docutils literal notranslate"><span class="pre">word</span></code>. This is a convenient choice, as both the sentiment lexicons and stop word lists use <code class="docutils literal notranslate"><span class="pre">word</span></code> as their column name, making joins and filtering operations much simpler.</p>
<p>With the text now in a tidy format—one word per row—we’re ready to run sentiment analysis. We’ll start by using the NRC lexicon and applying <a class="reference external" href="https://dplyr.tidyverse.org/reference/filter.html"><code class="docutils literal notranslate"><span class="pre">filter()</span></code></a> to select only the words associated with joy. Then, we’ll filter the book text for words from <em>Emma</em> and use <a class="reference external" href="https://dplyr.tidyverse.org/reference/mutate-joins.html"><code class="docutils literal notranslate"><span class="pre">inner_join()</span></code></a> to connect it to the sentiment lexicon. To see the most common joy-related words in <em>Emma</em>, we can apply <a class="reference external" href="https://dplyr.tidyverse.org/reference/count.html"><code class="docutils literal notranslate"><span class="pre">count()</span></code></a> from <strong>dplyr</strong>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">nrc_joy</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">get_sentiments</span><span class="p">(</span><span class="s">&quot;nrc&quot;</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span>
<span class="w">  </span><span class="nf">filter</span><span class="p">(</span><span class="n">sentiment</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="s">&quot;joy&quot;</span><span class="p">)</span>

<span class="n">tidy_books</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">filter</span><span class="p">(</span><span class="n">book</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="s">&quot;Emma&quot;</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">inner_join</span><span class="p">(</span><span class="n">nrc_joy</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">count</span><span class="p">(</span><span class="n">word</span><span class="p">,</span><span class="w"> </span><span class="n">sort</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">print</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Joining with `by = join_by(word)`
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span># A tibble: 301 x 2
   word          n
   &lt;chr&gt;     &lt;int&gt;
 1 good        359
 2 friend      166
 3 hope        143
 4 happy       125
 5 love        117
 6 deal         92
 7 found        92
 8 present      89
 9 kind         82
10 happiness    76
# i 291 more rows
</pre></div>
</div>
</div>
</div>
<p>We mostly see positive, uplifting words about hope, friendship, and love here. There are also some words that Austen may not have intended to sound joyful, like “found” or “present”; we’ll look at this issue more closely in Section 2.4.</p>
<p>We can also explore how sentiment shifts throughout each novel. This requires only a few lines of code, mostly using <strong>dplyr</strong> functions. First, we assign a sentiment score to each word by joining the Bing lexicon with our text using <a class="reference external" href="https://dplyr.tidyverse.org/reference/mutate-joins.html"><code class="docutils literal notranslate"><span class="pre">inner_join()</span></code></a>.</p>
<p>Next, we count the number of positive and negative words within set sections of the text. To do this, we create an index to track our position in the narrative; this index uses integer division to group the text into sections of 80 lines each.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">%/%</span></code> operator performs integer division (<code class="docutils literal notranslate"><span class="pre">x</span> <span class="pre">%/%</span> <span class="pre">y</span></code> is the same as <code class="docutils literal notranslate"><span class="pre">floor(x/y)</span></code>), allowing us to track which 80-line section we are analyzing for positive and negative sentiment.</p>
<p>If sections are too small, we might not have enough words to reliably estimate sentiment, while sections that are too large can blur the narrative structure. For these novels, 80-line sections work well, though the ideal size depends on factors like the length of the text and how the lines were structured originally. Finally, we use <a class="reference external" href="https://tidyr.tidyverse.org/reference/pivot_wider.html"><code class="docutils literal notranslate"><span class="pre">pivot_wider()</span></code></a> to separate positive and negative sentiment into different columns and calculate the net sentiment by subtracting negative from positive.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">library</span><span class="p">(</span><span class="n">tidyr</span><span class="p">)</span>

<span class="n">jane_austen_sentiment</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">tidy_books</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">inner_join</span><span class="p">(</span><span class="nf">get_sentiments</span><span class="p">(</span><span class="s">&quot;bing&quot;</span><span class="p">))</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">count</span><span class="p">(</span><span class="n">book</span><span class="p">,</span><span class="w"> </span><span class="n">index</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">linenumber</span><span class="w"> </span><span class="o">%/%</span><span class="w"> </span><span class="m">80</span><span class="p">,</span><span class="w"> </span><span class="n">sentiment</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">pivot_wider</span><span class="p">(</span><span class="n">names_from</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">sentiment</span><span class="p">,</span><span class="w"> </span><span class="n">values_from</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">n</span><span class="p">,</span><span class="w"> </span><span class="n">values_fill</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span>
<span class="w">  </span><span class="nf">mutate</span><span class="p">(</span><span class="n">sentiment</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">positive</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">negative</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Joining with `by = join_by(word)`
Warning message in inner_join(., get_sentiments(&quot;bing&quot;)):
&quot;Detected an unexpected many-to-many relationship between `x` and `y`.
<span class=" -Color -Color-Cyan">i</span> Row 435443 of `x` matches multiple rows in `y`.
<span class=" -Color -Color-Cyan">i</span> Row 5051 of `y` matches multiple rows in `x`.
<span class=" -Color -Color-Cyan">i</span> If a many-to-many relationship is expected, set `relationship =
  &quot;many-to-many&quot;` to silence this warning.&quot;
</pre></div>
</div>
</div>
</div>
<p>Now that we’ve calculated sentiment scores for sections of each novel, we can visualize how sentiment changes over the course of the story. To do this, we plot the sentiment scores against the <strong>index</strong> on the x-axis, which represents the narrative timeline divided into sections of text.</p>
<p>This approach lets us clearly see how the emotional tone of each novel rises or falls, revealing points where the story shifts toward more positive or negative sentiment as the plot progresses. This provides a useful, high-level overview of the emotional structure of the narrative.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">library</span><span class="p">(</span><span class="n">ggplot2</span><span class="p">)</span>

<span class="nf">ggplot</span><span class="p">(</span><span class="n">jane_austen_sentiment</span><span class="p">,</span><span class="w"> </span><span class="nf">aes</span><span class="p">(</span><span class="n">index</span><span class="p">,</span><span class="w"> </span><span class="n">sentiment</span><span class="p">,</span><span class="w"> </span><span class="n">fill</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">book</span><span class="p">))</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="nf">geom_col</span><span class="p">(</span><span class="n">show.legend</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">FALSE</span><span class="p">)</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="nf">facet_wrap</span><span class="p">(</span><span class="o">~</span><span class="n">book</span><span class="p">,</span><span class="w"> </span><span class="n">ncol</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">2</span><span class="p">,</span><span class="w"> </span><span class="n">scales</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;free_x&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<a class="reference internal image-reference" href="../../../_images/24f4932b162eb9b84270703c6549835ead338d0dd3186163a8371f66644724ad.png"><img alt="../../../_images/24f4932b162eb9b84270703c6549835ead338d0dd3186163a8371f66644724ad.png" src="../../../_images/24f4932b162eb9b84270703c6549835ead338d0dd3186163a8371f66644724ad.png" style="width: 420px; height: 420px;" />
</a>
</div>
</div>
</section>
<section id="comparing-sentiment-dictionaries">
<h3>Comparing Sentiment Dictionaries<a class="headerlink" href="#comparing-sentiment-dictionaries" title="Link to this heading">#</a></h3>
<p>With multiple sentiment lexicons available, it’s helpful to compare them to determine which best suits your analysis. To explore this, we can apply all three sentiment lexicons—<strong>AFINN</strong>, <strong>bing</strong>, and <strong>nrc</strong>—and examine how each captures sentiment changes across the narrative arc of <em>Pride and Prejudice</em>.</p>
<p>We start by using <a class="reference external" href="https://dplyr.tidyverse.org/reference/filter.html"><code class="docutils literal notranslate"><span class="pre">filter()</span></code></a> to isolate only the words from <em>Pride and Prejudice</em> so we can focus the analysis on that specific text. From there, we can apply each lexicon to compare how sentiment patterns emerge using different approaches.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">pride_prejudice</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">tidy_books</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span>
<span class="w">  </span><span class="nf">filter</span><span class="p">(</span><span class="n">book</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="s">&quot;Pride &amp; Prejudice&quot;</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="n">pride_prejudice</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span># A tibble: 122,204 x 4
   book              linenumber chapter word     
   &lt;fct&gt;                  &lt;int&gt;   &lt;int&gt; &lt;chr&gt;    
 1 Pride &amp; Prejudice          1       0 pride    
 2 Pride &amp; Prejudice          1       0 and      
 3 Pride &amp; Prejudice          1       0 prejudice
 4 Pride &amp; Prejudice          3       0 by       
 5 Pride &amp; Prejudice          3       0 jane     
 6 Pride &amp; Prejudice          3       0 austen   
 7 Pride &amp; Prejudice          7       1 chapter  
 8 Pride &amp; Prejudice          7       1 1        
 9 Pride &amp; Prejudice         10       1 it       
10 Pride &amp; Prejudice         10       1 is       
# i 122,194 more rows
</pre></div>
</div>
</div>
</div>
<p>We can now use <a class="reference external" href="https://dplyr.tidyverse.org/reference/mutate-joins.html"><code class="docutils literal notranslate"><span class="pre">inner_join()</span></code></a> to apply the different sentiment lexicons and calculate sentiment scores throughout the novel. It’s important to remember that the <strong>AFINN</strong> lexicon assigns words a numeric score ranging from -5 (strongly negative) to 5 (strongly positive), while the <strong>bing</strong> and <strong>nrc</strong> lexicons categorize words simply as positive or negative.</p>
<p>Because of these differences, we use slightly different approaches when calculating sentiment for each lexicon. For all three, we divide the text into larger sections using integer division (<code class="docutils literal notranslate"><span class="pre">%/%</span></code>), grouping lines together to capture sentiment trends over meaningful chunks of the narrative. We then apply <a class="reference external" href="https://dplyr.tidyverse.org/reference/count.html"><code class="docutils literal notranslate"><span class="pre">count()</span></code></a>, <a class="reference external" href="https://tidyr.tidyverse.org/reference/pivot_wider.html"><code class="docutils literal notranslate"><span class="pre">pivot_wider()</span></code></a>, and <a class="reference external" href="https://dplyr.tidyverse.org/reference/mutate.html"><code class="docutils literal notranslate"><span class="pre">mutate()</span></code></a> to compute net sentiment in each section, allowing for a clear comparison of how sentiment evolves across the novel using each lexicon’s unique scoring method.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">afinn</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">pride_prejudice</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span>
<span class="w">  </span><span class="nf">inner_join</span><span class="p">(</span><span class="nf">get_sentiments</span><span class="p">(</span><span class="s">&quot;afinn&quot;</span><span class="p">))</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span>
<span class="w">  </span><span class="nf">group_by</span><span class="p">(</span><span class="n">index</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">linenumber</span><span class="w"> </span><span class="o">%/%</span><span class="w"> </span><span class="m">80</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span>
<span class="w">  </span><span class="nf">summarise</span><span class="p">(</span><span class="n">sentiment</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">sum</span><span class="p">(</span><span class="n">value</span><span class="p">))</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span>
<span class="w">  </span><span class="nf">mutate</span><span class="p">(</span><span class="n">method</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;AFINN&quot;</span><span class="p">)</span>

<span class="n">bing_and_nrc</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">bind_rows</span><span class="p">(</span>
<span class="w">  </span><span class="n">pride_prejudice</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span>
<span class="w">    </span><span class="nf">inner_join</span><span class="p">(</span><span class="nf">get_sentiments</span><span class="p">(</span><span class="s">&quot;bing&quot;</span><span class="p">))</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">    </span><span class="nf">mutate</span><span class="p">(</span><span class="n">method</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;Bing et al.&quot;</span><span class="p">),</span>
<span class="w">  </span><span class="n">pride_prejudice</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span>
<span class="w">    </span><span class="nf">inner_join</span><span class="p">(</span><span class="nf">get_sentiments</span><span class="p">(</span><span class="s">&quot;nrc&quot;</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span>
<span class="w">                 </span><span class="nf">filter</span><span class="p">(</span><span class="n">sentiment</span><span class="w"> </span><span class="o">%in%</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="s">&quot;positive&quot;</span><span class="p">,</span><span class="w"> </span>
<span class="w">                                         </span><span class="s">&quot;negative&quot;</span><span class="p">))</span>
<span class="w">    </span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">    </span><span class="nf">mutate</span><span class="p">(</span><span class="n">method</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;NRC&quot;</span><span class="p">))</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">count</span><span class="p">(</span><span class="n">method</span><span class="p">,</span><span class="w"> </span><span class="n">index</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">linenumber</span><span class="w"> </span><span class="o">%/%</span><span class="w"> </span><span class="m">80</span><span class="p">,</span><span class="w"> </span><span class="n">sentiment</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">pivot_wider</span><span class="p">(</span><span class="n">names_from</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">sentiment</span><span class="p">,</span>
<span class="w">              </span><span class="n">values_from</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">n</span><span class="p">,</span>
<span class="w">              </span><span class="n">values_fill</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span>
<span class="w">  </span><span class="nf">mutate</span><span class="p">(</span><span class="n">sentiment</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">positive</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">negative</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Joining with `by = join_by(word)`
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Joining with `by = join_by(word)`
Joining with `by = join_by(word)`
Warning message in inner_join(., get_sentiments(&quot;nrc&quot;) %&gt;% filter(sentiment %in% :
&quot;Detected an unexpected many-to-many relationship between `x` and `y`.
<span class=" -Color -Color-Cyan">i</span> Row 215 of `x` matches multiple rows in `y`.
<span class=" -Color -Color-Cyan">i</span> Row 5178 of `y` matches multiple rows in `x`.
<span class=" -Color -Color-Cyan">i</span> If a many-to-many relationship is expected, set `relationship =
  &quot;many-to-many&quot;` to silence this warning.&quot;
</pre></div>
</div>
</div>
</div>
<p>We now have calculated the net sentiment—positive minus negative—for each chunk of the novel text using all three sentiment lexicons: <strong>AFINN</strong>, <strong>bing</strong>, and <strong>nrc</strong>. The next step is to bind these results together into a single data frame so we can easily compare them side by side. Once combined, we can visualize the sentiment trends across the narrative using a plot. This allows us to directly observe how each lexicon captures emotional shifts in the story and highlights both the similarities and differences in their sentiment patterns.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">bind_rows</span><span class="p">(</span><span class="n">afinn</span><span class="p">,</span><span class="w"> </span>
<span class="w">          </span><span class="n">bing_and_nrc</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">ggplot</span><span class="p">(</span><span class="nf">aes</span><span class="p">(</span><span class="n">index</span><span class="p">,</span><span class="w"> </span><span class="n">sentiment</span><span class="p">,</span><span class="w"> </span><span class="n">fill</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">method</span><span class="p">))</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="nf">geom_col</span><span class="p">(</span><span class="n">show.legend</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">FALSE</span><span class="p">)</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="nf">facet_wrap</span><span class="p">(</span><span class="o">~</span><span class="n">method</span><span class="p">,</span><span class="w"> </span><span class="n">ncol</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1</span><span class="p">,</span><span class="w"> </span><span class="n">scales</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;free_y&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<a class="reference internal image-reference" href="../../../_images/2e274ba7733d530c392f41daf808438234851658b8c95ce7ed786b34a115a0a5.png"><img alt="../../../_images/2e274ba7733d530c392f41daf808438234851658b8c95ce7ed786b34a115a0a5.png" src="../../../_images/2e274ba7733d530c392f41daf808438234851658b8c95ce7ed786b34a115a0a5.png" style="width: 420px; height: 420px;" />
</a>
</div>
</div>
<p>The three sentiment lexicons produce different absolute results, but they reveal similar overall sentiment patterns across the novel. All three show comparable peaks and dips in sentiment at roughly the same points in the story, reflecting consistent emotional shifts. However, the <strong>AFINN</strong> lexicon tends to produce the largest absolute values, with higher positive and negative extremes. The <strong>bing</strong> lexicon shows smaller absolute values and often identifies longer stretches of consistently positive or negative text. Meanwhile, the <strong>NRC</strong> lexicon shifts the sentiment scores higher overall, labeling more of the text as positive but still capturing similar relative sentiment changes. These patterns are consistent across other novels as well: NRC sentiment tends to be higher, AFINN shows more variability, and Bing highlights longer contiguous blocks of sentiment, but all three lexicons generally agree on the broader narrative sentiment trends.</p>
<p>The NRC lexicon often produces higher overall sentiment scores compared to the Bing et al. lexicon because of differences in how the two are constructed. One key reason is that the NRC lexicon contains a larger proportion of positive words relative to negative words, which biases its overall sentiment scores upward. To better understand this, we can look at the actual counts of positive and negative words within each lexicon. By comparing how many words are labeled positive versus negative in both the NRC and Bing lexicons, we can see why the NRC results tend to skew more positive, even when both detect similar relative sentiment shifts across a text.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">get_sentiments</span><span class="p">(</span><span class="s">&quot;nrc&quot;</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span>
<span class="w">  </span><span class="nf">filter</span><span class="p">(</span><span class="n">sentiment</span><span class="w"> </span><span class="o">%in%</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="s">&quot;positive&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;negative&quot;</span><span class="p">))</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span>
<span class="w">  </span><span class="nf">count</span><span class="p">(</span><span class="n">sentiment</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">print</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span># A tibble: 2 x 2
  sentiment     n
  &lt;chr&gt;     &lt;int&gt;
1 negative   3316
2 positive   2308
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">get_sentiments</span><span class="p">(</span><span class="s">&quot;bing&quot;</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span>
<span class="w">  </span><span class="nf">count</span><span class="p">(</span><span class="n">sentiment</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">print</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span># A tibble: 2 x 2
  sentiment     n
  &lt;chr&gt;     &lt;int&gt;
1 negative   4781
2 positive   2005
</pre></div>
</div>
</div>
</div>
<p>Both the NRC and Bing lexicons actually contain more negative words than positive ones, but the Bing lexicon has a higher ratio of negative to positive words compared to NRC. This difference helps explain why the Bing lexicon tends to produce lower overall sentiment scores, while the NRC lexicon often skews more positive. Additionally, systematic differences in how well the lexicons’ negative words match the vocabulary of a particular author—like Jane Austen—can further influence the results. Despite these absolute differences, the lexicons generally show similar relative sentiment trajectories and changes in narrative slope across the text. This context is important to keep in mind when selecting a sentiment lexicon, as the choice can impact both the magnitude and interpretation of sentiment analysis results.</p>
</section>
<section id="most-common-positive-and-negative-words">
<h3>Most Common Positive and Negative Words<a class="headerlink" href="#most-common-positive-and-negative-words" title="Link to this heading">#</a></h3>
<p>One advantage of having a tidy data frame that includes both sentiment labels and individual words is that it allows us to directly analyze which words contribute to positive or negative sentiment. By using <a class="reference external" href="https://dplyr.tidyverse.org/reference/count.html">`count()</a> with both the <strong>word</strong> and <strong>sentiment</strong> columns as arguments, we can quickly see how often each word appears in the text and how it contributes to either positive or negative sentiment. This approach provides a clear breakdown of which specific words are driving the sentiment patterns we observe, offering deeper insight into the language used in the text.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">bing_word_counts</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">tidy_books</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">inner_join</span><span class="p">(</span><span class="nf">get_sentiments</span><span class="p">(</span><span class="s">&quot;bing&quot;</span><span class="p">))</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">count</span><span class="p">(</span><span class="n">word</span><span class="p">,</span><span class="w"> </span><span class="n">sentiment</span><span class="p">,</span><span class="w"> </span><span class="n">sort</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">ungroup</span><span class="p">()</span>

<span class="nf">print</span><span class="p">(</span><span class="n">bing_word_counts</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Joining with `by = join_by(word)`
Warning message in inner_join(., get_sentiments(&quot;bing&quot;)):
&quot;Detected an unexpected many-to-many relationship between `x` and `y`.
<span class=" -Color -Color-Cyan">i</span> Row 435443 of `x` matches multiple rows in `y`.
<span class=" -Color -Color-Cyan">i</span> Row 5051 of `y` matches multiple rows in `x`.
<span class=" -Color -Color-Cyan">i</span> If a many-to-many relationship is expected, set `relationship =
  &quot;many-to-many&quot;` to silence this warning.&quot;
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span># A tibble: 2,585 x 3
   word     sentiment     n
   &lt;chr&gt;    &lt;chr&gt;     &lt;int&gt;
 1 miss     negative   1855
 2 well     positive   1523
 3 good     positive   1380
 4 great    positive    981
 5 like     positive    725
 6 better   positive    639
 7 enough   positive    613
 8 happy    positive    534
 9 love     positive    495
10 pleasure positive    462
# i 2,575 more rows
</pre></div>
</div>
</div>
</div>
<p>This can also be visualized, and we can pipe the results directly into <strong>ggplot2</strong> thanks to the consistent use of tidy data tools.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">bing_word_counts</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">group_by</span><span class="p">(</span><span class="n">sentiment</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">slice_max</span><span class="p">(</span><span class="n">n</span><span class="p">,</span><span class="w"> </span><span class="n">n</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">10</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span>
<span class="w">  </span><span class="nf">ungroup</span><span class="p">()</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">mutate</span><span class="p">(</span><span class="n">word</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">reorder</span><span class="p">(</span><span class="n">word</span><span class="p">,</span><span class="w"> </span><span class="n">n</span><span class="p">))</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">ggplot</span><span class="p">(</span><span class="nf">aes</span><span class="p">(</span><span class="n">n</span><span class="p">,</span><span class="w"> </span><span class="n">word</span><span class="p">,</span><span class="w"> </span><span class="n">fill</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">sentiment</span><span class="p">))</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="nf">geom_col</span><span class="p">(</span><span class="n">show.legend</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">FALSE</span><span class="p">)</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="nf">facet_wrap</span><span class="p">(</span><span class="o">~</span><span class="n">sentiment</span><span class="p">,</span><span class="w"> </span><span class="n">scales</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;free_y&quot;</span><span class="p">)</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="nf">labs</span><span class="p">(</span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;Contribution to sentiment&quot;</span><span class="p">,</span>
<span class="w">       </span><span class="n">y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">NULL</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<a class="reference internal image-reference" href="../../../_images/5987821c5be311aa2597339c7986746fb1de1c58a911a1be13faa00f5628fb91.png"><img alt="../../../_images/5987821c5be311aa2597339c7986746fb1de1c58a911a1be13faa00f5628fb91.png" src="../../../_images/5987821c5be311aa2597339c7986746fb1de1c58a911a1be13faa00f5628fb91.png" style="width: 420px; height: 420px;" />
</a>
</div>
</div>
<p>This reveals an anomaly in the sentiment analysis—the word “miss” is marked as negative, even though in Jane Austen’s works it is commonly used as a title for young, unmarried women. If needed for our analysis, we could easily add “miss” to a custom stop words list using <a class="reference external" href="https://dplyr.tidyverse.org/reference/bind_rows.html"><code class="docutils literal notranslate"><span class="pre">bind_rows()</span></code></a>. This could be done with an approach like the one shown here.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">custom_stop_words</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">bind_rows</span><span class="p">(</span><span class="nf">tibble</span><span class="p">(</span><span class="n">word</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="s">&quot;miss&quot;</span><span class="p">),</span><span class="w">  </span>
<span class="w">                                      </span><span class="n">lexicon</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="s">&quot;custom&quot;</span><span class="p">)),</span><span class="w"> </span>
<span class="w">                               </span><span class="n">stop_words</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="n">custom_stop_words</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span># A tibble: 1,150 x 2
   word        lexicon
   &lt;chr&gt;       &lt;chr&gt;  
 1 miss        custom 
 2 a           SMART  
 3 a&#39;s         SMART  
 4 able        SMART  
 5 about       SMART  
 6 above       SMART  
 7 according   SMART  
 8 accordingly SMART  
 9 across      SMART  
10 actually    SMART  
# i 1,140 more rows
</pre></div>
</div>
</div>
</div>
</section>
<section id="wordclouds">
<h3>Wordclouds<a class="headerlink" href="#wordclouds" title="Link to this heading">#</a></h3>
<p>We’ve seen that the tidy text mining approach integrates well with <strong>ggplot2</strong>, but having our data in a tidy format is also helpful for creating other types of visualizations.</p>
<p>For instance, we can use the <strong>wordcloud</strong> package, which relies on base R graphics. Let’s revisit the most common words across Jane Austen’s works, but display them as a word cloud.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">library</span><span class="p">(</span><span class="n">wordcloud</span><span class="p">)</span>

<span class="n">tidy_books</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">anti_join</span><span class="p">(</span><span class="n">stop_words</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">count</span><span class="p">(</span><span class="n">word</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">with</span><span class="p">(</span><span class="nf">wordcloud</span><span class="p">(</span><span class="n">word</span><span class="p">,</span><span class="w"> </span><span class="n">n</span><span class="p">,</span><span class="w"> </span><span class="n">max.words</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">100</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Joining with `by = join_by(word)`
</pre></div>
</div>
<a class="reference internal image-reference" href="../../../_images/c4d39e37a346ade38ef357cb26baf2a31f3828174da1098dee92eb29109a7d1f.png"><img alt="../../../_images/c4d39e37a346ade38ef357cb26baf2a31f3828174da1098dee92eb29109a7d1f.png" src="../../../_images/c4d39e37a346ade38ef357cb26baf2a31f3828174da1098dee92eb29109a7d1f.png" style="width: 420px; height: 420px;" />
</a>
</div>
</div>
<p>For functions like <a class="reference external" href="https://rdrr.io/cran/wordcloud/man/comparison.cloud.html"><code class="docutils literal notranslate"><span class="pre">comparison.cloud()</span></code></a>, the data frame often needs to be converted into a matrix using reshape2’s <a class="reference external" href="https://rdrr.io/cran/reshape2/man/cast.html"><code class="docutils literal notranslate"><span class="pre">acast()</span></code></a> function. To prepare for this, we first perform sentiment analysis by tagging positive and negative words with an inner_join, followed by finding the most common positive and negative words. Up to the point where we pass the data to <a class="reference external" href="https://rdrr.io/cran/wordcloud/man/comparison.cloud.html"><code class="docutils literal notranslate"><span class="pre">comparison.cloud()</span></code></a>, all of this can be done seamlessly using joins, piping, and <strong>dplyr</strong>, thanks to having the data structured in a tidy format.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">library</span><span class="p">(</span><span class="n">reshape2</span><span class="p">)</span>

<span class="n">tidy_books</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">inner_join</span><span class="p">(</span><span class="nf">get_sentiments</span><span class="p">(</span><span class="s">&quot;bing&quot;</span><span class="p">))</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">count</span><span class="p">(</span><span class="n">word</span><span class="p">,</span><span class="w"> </span><span class="n">sentiment</span><span class="p">,</span><span class="w"> </span><span class="n">sort</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">acast</span><span class="p">(</span><span class="n">word</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">sentiment</span><span class="p">,</span><span class="w"> </span><span class="n">value.var</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;n&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">fill</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">comparison.cloud</span><span class="p">(</span><span class="n">colors</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="s">&quot;gray20&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;gray80&quot;</span><span class="p">),</span>
<span class="w">                   </span><span class="n">max.words</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">100</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Joining with `by = join_by(word)`
Warning message in inner_join(., get_sentiments(&quot;bing&quot;)):
&quot;Detected an unexpected many-to-many relationship between `x` and `y`.
<span class=" -Color -Color-Cyan">i</span> Row 435443 of `x` matches multiple rows in `y`.
<span class=" -Color -Color-Cyan">i</span> Row 5051 of `y` matches multiple rows in `x`.
<span class=" -Color -Color-Cyan">i</span> If a many-to-many relationship is expected, set `relationship =
  &quot;many-to-many&quot;` to silence this warning.&quot;
</pre></div>
</div>
<a class="reference internal image-reference" href="../../../_images/93e8f4913dc936e0b2c8b654b6a5f37c83a98393b1127b2b0024541675524991.png"><img alt="../../../_images/93e8f4913dc936e0b2c8b654b6a5f37c83a98393b1127b2b0024541675524991.png" src="../../../_images/93e8f4913dc936e0b2c8b654b6a5f37c83a98393b1127b2b0024541675524991.png" style="width: 420px; height: 420px;" />
</a>
</div>
</div>
<p>The size of each word corresponds to its frequency within its assigned sentiment group. This allows us to easily identify the most prominent positive and negative words. However, keep in mind that word sizes are scaled separately within each sentiment, so the sizes are not directly comparable across positive and negative categories.</p>
</section>
<section id="looking-beyond-just-words">
<h3>Looking Beyond just Words<a class="headerlink" href="#looking-beyond-just-words" title="Link to this heading">#</a></h3>
<p>While much useful analysis can be done by tokenizing text at the word level, there are situations where other units of text are more appropriate. For example, some sentiment analysis algorithms go beyond unigrams (single words) to assess the sentiment of an entire sentence. These methods can account for context, such as negation, recognizing that:</p>
<p><em>I am not having a good day.</em></p>
<p>is a negative sentence, despite containing words like <em>good</em>. R packages like <strong>coreNLP</strong> <a class="reference external" href="https://www.tidytextmining.com/references#ref-R-coreNLP">(T. Arnold and Tilton 2016)</a>, <strong>cleanNLP</strong> <a class="reference external" href="https://www.tidytextmining.com/references#ref-R-cleanNLP">(T. B. Arnold 2016)</a>, and <strong>sentimentr</strong> <a class="reference external" href="https://www.tidytextmining.com/references#ref-R-sentimentr">(Rinker 2017)</a> handle this type of sentiment analysis. In these cases, it’s common to tokenize by sentence and use a different output column name to reflect the change in text unit.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">p_and_p_sentences</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">tibble</span><span class="p">(</span><span class="n">text</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">prideprejudice</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span>
<span class="w">  </span><span class="nf">unnest_tokens</span><span class="p">(</span><span class="n">sentence</span><span class="p">,</span><span class="w"> </span><span class="n">text</span><span class="p">,</span><span class="w"> </span><span class="n">token</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;sentences&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Let’s look at just one.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">p_and_p_sentences</span><span class="o">$</span><span class="n">sentence</span><span class="p">[</span><span class="m">2</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html">'by jane austen'</div></div>
</div>
<p>Sentence tokenizing can sometimes struggle with UTF-8 encoded text, especially in sections of dialogue, but tends to perform better with ASCII punctuation. If this causes issues, one solution is to use <a class="reference external" href="https://rdrr.io/r/base/iconv.html"><code class="docutils literal notranslate"><span class="pre">iconv()</span></code></a> to convert the text encoding, for example with <code class="docutils literal notranslate"><span class="pre">iconv(text,</span> <span class="pre">to</span> <span class="pre">=</span> <span class="pre">'latin1')</span></code> inside a mutate statement before unnesting.</p>
<p>Another option with <a class="reference external" href="https://juliasilge.github.io/tidytext/reference/unnest_tokens.html"><code class="docutils literal notranslate"><span class="pre">unnest_tokens()</span></code></a> is to split the text into tokens using a regular expression. For instance, we could use this approach to break Jane Austen’s novels into a data frame by chapter based on a chapter header pattern.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">austen_chapters</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">austen_books</span><span class="p">()</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">group_by</span><span class="p">(</span><span class="n">book</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">unnest_tokens</span><span class="p">(</span><span class="n">chapter</span><span class="p">,</span><span class="w"> </span><span class="n">text</span><span class="p">,</span><span class="w"> </span><span class="n">token</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;regex&quot;</span><span class="p">,</span><span class="w"> </span>
<span class="w">                </span><span class="n">pattern</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;Chapter|CHAPTER [\\dIVXLC]&quot;</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">ungroup</span><span class="p">()</span>

<span class="n">austen_chapters</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span>
<span class="w">  </span><span class="nf">group_by</span><span class="p">(</span><span class="n">book</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span>
<span class="w">  </span><span class="nf">summarise</span><span class="p">(</span><span class="n">chapters</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">n</span><span class="p">())</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">print</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span># A tibble: 6 x 2
  book                chapters
  &lt;fct&gt;                  &lt;int&gt;
1 Sense &amp; Sensibility       51
2 Pride &amp; Prejudice         62
3 Mansfield Park            49
4 Emma                      56
5 Northanger Abbey          32
6 Persuasion                25
</pre></div>
</div>
</div>
</div>
<p>We’ve successfully recovered the correct number of chapters for each novel (with one extra row for each title), and in the <code class="docutils literal notranslate"><span class="pre">austen_chapters</span></code> data frame, each row now represents a chapter.</p>
<p>Earlier, we used a similar regex pattern to locate chapter breaks when tokenizing text into one-word-per-row format. Now, using this tidy structure, we can explore questions like: <em>Which chapters in Jane Austen’s novels are the most negative?</em></p>
<p>To answer this, we start by retrieving the list of negative words from the <strong>bing</strong> lexicon. Then, we create a data frame that counts the total number of words per chapter—this helps us normalize for chapter length. Next, we count how many negative words appear in each chapter and divide that by the total number of words in that chapter. Finally, we identify which chapter in each book has the highest <strong>proportion</strong> of negative words.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">bingnegative</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">get_sentiments</span><span class="p">(</span><span class="s">&quot;bing&quot;</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span>
<span class="w">  </span><span class="nf">filter</span><span class="p">(</span><span class="n">sentiment</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="s">&quot;negative&quot;</span><span class="p">)</span>

<span class="n">wordcounts</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">tidy_books</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">group_by</span><span class="p">(</span><span class="n">book</span><span class="p">,</span><span class="w"> </span><span class="n">chapter</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">summarize</span><span class="p">(</span><span class="n">words</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">n</span><span class="p">())</span>

<span class="n">tidy_books</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">semi_join</span><span class="p">(</span><span class="n">bingnegative</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">group_by</span><span class="p">(</span><span class="n">book</span><span class="p">,</span><span class="w"> </span><span class="n">chapter</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">summarize</span><span class="p">(</span><span class="n">negativewords</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">n</span><span class="p">())</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">left_join</span><span class="p">(</span><span class="n">wordcounts</span><span class="p">,</span><span class="w"> </span><span class="n">by</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="s">&quot;book&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;chapter&quot;</span><span class="p">))</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">mutate</span><span class="p">(</span><span class="n">ratio</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">negativewords</span><span class="o">/</span><span class="n">words</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">filter</span><span class="p">(</span><span class="n">chapter</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="m">0</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">slice_max</span><span class="p">(</span><span class="n">ratio</span><span class="p">,</span><span class="w"> </span><span class="n">n</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span>
<span class="w">  </span><span class="nf">ungroup</span><span class="p">()</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">print</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>`summarise()` has grouped output by &#39;book&#39;. You can override using the
`.groups` argument.
Joining with `by = join_by(word)`
`summarise()` has grouped output by &#39;book&#39;. You can override using the
`.groups` argument.
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span># A tibble: 6 x 5
  book                chapter negativewords words  ratio
  &lt;fct&gt;                 &lt;int&gt;         &lt;int&gt; &lt;int&gt;  &lt;dbl&gt;
1 Sense &amp; Sensibility      43           161  3405 0.0473
2 Pride &amp; Prejudice        34           111  2104 0.0528
3 Mansfield Park           46           173  3685 0.0469
4 Emma                     15           151  3340 0.0452
5 Northanger Abbey         21           149  2982 0.0500
6 Persuasion                4            62  1807 0.0343
</pre></div>
</div>
</div>
</div>
<p>The chapters with the highest proportion of negative words correspond to some of the most emotionally intense moments in each novel. For example, in Chapter 43 of <em>Sense and Sensibility</em>, Marianne is gravely ill and near death, while Chapter 34 of <em>Pride and Prejudice</em> features Mr. Darcy’s awkward first marriage proposal. Chapter 46 of <em>Mansfield Park</em> reveals Henry’s scandalous adultery near the story’s end, and in Chapter 15 of <em>Emma</em>, the unsettling Mr. Elton makes a proposal. In Chapter 21 of <em>Northanger Abbey</em>, Catherine is deeply caught up in her Gothic-inspired fantasies of murder, and Chapter 4 of <em>Persuasion</em> presents Anne’s painful flashback of refusing Captain Wentworth and her regret over that decision. These peaks in negative sentiment align closely with pivotal and often sorrowful moments in Austen’s narratives.</p>
</section>
</section>
<section id="analyzing-word-and-document-frequency">
<h2>Analyzing Word and Document Frequency<a class="headerlink" href="#analyzing-word-and-document-frequency" title="Link to this heading">#</a></h2>
<p>A key question in text mining and natural language processing is how to quantify what a document is about by examining its words. One simple measure is <em>term frequency (tf)</em>, which counts how often a word appears in a document. However, some frequently occurring words like “the,” “is,” and “of” may not carry much meaningful information. While removing such stop words is common, this approach can be overly simplistic, since these words might be important in certain contexts.</p>
<p>A more nuanced method involves <em>inverse document frequency (idf)</em>, which downweights words that appear in many documents and upweights rarer terms. Combining tf and idf gives us <em>tf-idf</em> a measure of how important a word is to a specific document within a larger collection. Although tf-idf is a heuristic rather than a theoretically rigorous measure, it has proven very useful in text mining and search engines.</p>
<p>Mathematically, idf for a term is defined as the natural logarithm of the total number of documents divided by the number of documents containing that term. Using tidy data principles, as introduced in Chapter 1, we can efficiently calculate and analyze tf-idf scores to identify key terms that define individual documents within a corpus.</p>
<section id="term-frequency-in-jane-austen-s-novels">
<h3>Term Frequency in Jane Austen’s Novels<a class="headerlink" href="#term-frequency-in-jane-austen-s-novels" title="Link to this heading">#</a></h3>
<p>Let’s begin by exploring Jane Austen’s published novels to analyze term frequency and then tf-idf. Using <strong>dplyr</strong> functions like <a class="reference external" href="https://dplyr.tidyverse.org/reference/group_by.html"><code class="docutils literal notranslate"><span class="pre">group_by()</span></code></a> and <code class="docutils literal notranslate"><span class="pre">join()</span></code>, we can identify the most commonly used words across her works. At the same time, we’ll calculate the total number of words in each novel, which will be useful later when we compute tf-idf scores. This approach helps us understand word importance both within individual novels and across the entire collection.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">library</span><span class="p">(</span><span class="n">dplyr</span><span class="p">)</span>
<span class="nf">library</span><span class="p">(</span><span class="n">janeaustenr</span><span class="p">)</span>
<span class="nf">library</span><span class="p">(</span><span class="n">tidytext</span><span class="p">)</span>

<span class="n">book_words</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">austen_books</span><span class="p">()</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">unnest_tokens</span><span class="p">(</span><span class="n">word</span><span class="p">,</span><span class="w"> </span><span class="n">text</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">count</span><span class="p">(</span><span class="n">book</span><span class="p">,</span><span class="w"> </span><span class="n">word</span><span class="p">,</span><span class="w"> </span><span class="n">sort</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">)</span>

<span class="n">total_words</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">book_words</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span>
<span class="w">  </span><span class="nf">group_by</span><span class="p">(</span><span class="n">book</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span>
<span class="w">  </span><span class="nf">summarize</span><span class="p">(</span><span class="n">total</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">sum</span><span class="p">(</span><span class="n">n</span><span class="p">))</span>

<span class="n">book_words</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">left_join</span><span class="p">(</span><span class="n">book_words</span><span class="p">,</span><span class="w"> </span><span class="n">total_words</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="n">book_words</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Joining with `by = join_by(book)`
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span># A tibble: 40,378 x 4
   book              word      n  total
   &lt;fct&gt;             &lt;chr&gt; &lt;int&gt;  &lt;int&gt;
 1 Mansfield Park    the    6206 160465
 2 Mansfield Park    to     5475 160465
 3 Mansfield Park    and    5438 160465
 4 Emma              to     5239 160996
 5 Emma              the    5201 160996
 6 Emma              and    4896 160996
 7 Mansfield Park    of     4778 160465
 8 Pride &amp; Prejudice the    4331 122204
 9 Emma              of     4291 160996
10 Pride &amp; Prejudice to     4162 122204
# i 40,368 more rows
</pre></div>
</div>
</div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">book_words</span></code> data frame has one row for each word–book pair; the column <code class="docutils literal notranslate"><span class="pre">n</span></code> shows how many times that word appears in the book, and <code class="docutils literal notranslate"><span class="pre">total</span></code> is the total number of words in that book. Unsurprisingly, the most frequent words include common ones like “the,” “and,” and “to.”</p>
<p>We can visualize the distribution of <code class="docutils literal notranslate"><span class="pre">n/total</span></code> for each novel—this ratio represents the term frequency, showing how often a word occurs relative to the total words in that novel.</p>
<p>These plots have long right tails—representing those very common words—which aren’t fully displayed here. Overall, the distributions are similar across all the novels, showing many words that appear infrequently and only a few that occur very frequently.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">library</span><span class="p">(</span><span class="n">ggplot2</span><span class="p">)</span>

<span class="nf">ggplot</span><span class="p">(</span><span class="n">book_words</span><span class="p">,</span><span class="w"> </span><span class="nf">aes</span><span class="p">(</span><span class="n">n</span><span class="o">/</span><span class="n">total</span><span class="p">,</span><span class="w"> </span><span class="n">fill</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">book</span><span class="p">))</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="nf">geom_histogram</span><span class="p">(</span><span class="n">show.legend</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">FALSE</span><span class="p">)</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="nf">xlim</span><span class="p">(</span><span class="kc">NA</span><span class="p">,</span><span class="w"> </span><span class="m">0.0009</span><span class="p">)</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="nf">facet_wrap</span><span class="p">(</span><span class="o">~</span><span class="n">book</span><span class="p">,</span><span class="w"> </span><span class="n">ncol</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">2</span><span class="p">,</span><span class="w"> </span><span class="n">scales</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;free_y&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
Warning message:
&quot;Removed 896 rows containing non-finite outside the scale range (`stat_bin()`).&quot;
Warning message:
&quot;Removed 6 rows containing missing values or values outside the scale range
(`geom_bar()`).&quot;
</pre></div>
</div>
<a class="reference internal image-reference" href="../../../_images/2a232851e02d214933b8c1ef5dfb74238feeb1f74f38ea4b695b2bb33ec21ab4.png"><img alt="../../../_images/2a232851e02d214933b8c1ef5dfb74238feeb1f74f38ea4b695b2bb33ec21ab4.png" src="../../../_images/2a232851e02d214933b8c1ef5dfb74238feeb1f74f38ea4b695b2bb33ec21ab4.png" style="width: 420px; height: 420px;" />
</a>
</div>
</div>
</section>
<section id="zipfs-law">
<h3>Zipf’s Law<a class="headerlink" href="#zipfs-law" title="Link to this heading">#</a></h3>
<p>Distributions like those above are typical in natural language. In fact, these long-tailed patterns are so common in any text corpus—whether a book, website content, or spoken language—that the relationship between a word’s frequency and its rank has been extensively studied. This relationship is famously described by Zipf’s law, named after George Zipf, a 20th-century American linguist.</p>
<p>Zipf’s law states that a word’s frequency is roughly <strong>inversely proportional</strong> to its rank in the frequency list. Since we already have the data frame used to plot term frequencies, we can easily explore Zipf’s law for Jane Austen’s novels using just a few lines of <strong>dplyr</strong> code.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">freq_by_rank</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">book_words</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span>
<span class="w">  </span><span class="nf">group_by</span><span class="p">(</span><span class="n">book</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span>
<span class="w">  </span><span class="nf">mutate</span><span class="p">(</span><span class="n">rank</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">row_number</span><span class="p">(),</span><span class="w"> </span>
<span class="w">         </span><span class="n">term_frequency</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">n</span><span class="o">/</span><span class="n">total</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">ungroup</span><span class="p">()</span>

<span class="nf">print</span><span class="p">(</span><span class="n">freq_by_rank</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span># A tibble: 40,378 x 6
   book              word      n  total  rank term_frequency
   &lt;fct&gt;             &lt;chr&gt; &lt;int&gt;  &lt;int&gt; &lt;int&gt;          &lt;dbl&gt;
 1 Mansfield Park    the    6206 160465     1         0.0387
 2 Mansfield Park    to     5475 160465     2         0.0341
 3 Mansfield Park    and    5438 160465     3         0.0339
 4 Emma              to     5239 160996     1         0.0325
 5 Emma              the    5201 160996     2         0.0323
 6 Emma              and    4896 160996     3         0.0304
 7 Mansfield Park    of     4778 160465     4         0.0298
 8 Pride &amp; Prejudice the    4331 122204     1         0.0354
 9 Emma              of     4291 160996     4         0.0267
10 Pride &amp; Prejudice to     4162 122204     2         0.0341
# i 40,368 more rows
</pre></div>
</div>
</div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">rank</span></code> column assigns each word its position in the frequency table, which is ordered by <code class="docutils literal notranslate"><span class="pre">n</span></code> (the count), and we can use <a class="reference external" href="https://dplyr.tidyverse.org/reference/row_number.html"><code class="docutils literal notranslate"><span class="pre">row_number()</span></code></a> to generate these ranks. After that, we calculate the term frequency just as before. Zipf’s law is typically visualized by plotting rank on the x-axis and term frequency on the y-axis, both using logarithmic scales. When plotted this way, the inverse relationship predicted by Zipf’s law appears as a straight line with a constant negative slope.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">freq_by_rank</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span>
<span class="w">  </span><span class="nf">ggplot</span><span class="p">(</span><span class="nf">aes</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span><span class="w"> </span><span class="n">term_frequency</span><span class="p">,</span><span class="w"> </span><span class="n">color</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">book</span><span class="p">))</span><span class="w"> </span><span class="o">+</span><span class="w"> </span>
<span class="w">  </span><span class="nf">geom_line</span><span class="p">(</span><span class="n">linewidth</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1.1</span><span class="p">,</span><span class="w"> </span><span class="n">alpha</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0.8</span><span class="p">,</span><span class="w"> </span><span class="n">show.legend</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">FALSE</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w"> </span>
<span class="w">  </span><span class="nf">scale_x_log10</span><span class="p">()</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="nf">scale_y_log10</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<a class="reference internal image-reference" href="../../../_images/45cb9a932423c6f777b5e135e33183d654c0941cbbe49d1631e297e3997b38ce.png"><img alt="../../../_images/45cb9a932423c6f777b5e135e33183d654c0941cbbe49d1631e297e3997b38ce.png" src="../../../_images/45cb9a932423c6f777b5e135e33183d654c0941cbbe49d1631e297e3997b38ce.png" style="width: 420px; height: 420px;" />
</a>
</div>
</div>
<p>The graph is plotted using log-log coordinates, showing that all six of Jane Austen’s novels follow similar patterns where the relationship between rank and frequency has a clear negative slope. However, this slope isn’t perfectly constant—suggesting the distribution might be better described as a broken <a class="reference external" href="https://en.wikipedia.org/wiki/Power_law">power law</a> divided into, say, three sections. To explore this further, let’s focus on the middle section of the rank range and calculate the exponent of the power law there.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">rank_subset</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">freq_by_rank</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span>
<span class="w">  </span><span class="nf">filter</span><span class="p">(</span><span class="n">rank</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="m">500</span><span class="p">,</span>
<span class="w">         </span><span class="n">rank</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="m">10</span><span class="p">)</span>

<span class="nf">lm</span><span class="p">(</span><span class="nf">log10</span><span class="p">(</span><span class="n">term_frequency</span><span class="p">)</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="nf">log10</span><span class="p">(</span><span class="n">rank</span><span class="p">),</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">rank_subset</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Call:
lm(formula = log10(term_frequency) ~ log10(rank), data = rank_subset)

Coefficients:
(Intercept)  log10(rank)  
    -0.6226      -1.1125  
</pre></div>
</div>
</div>
</div>
<p>Classic Zipf’s law states that frequency is proportional to 1 divided by rank, which corresponds to a slope of about -1 on a log-log plot. Since our calculated slope is close to -1, let’s plot this fitted power law alongside the data from above to visually compare how well it matches.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">freq_by_rank</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span>
<span class="w">  </span><span class="nf">ggplot</span><span class="p">(</span><span class="nf">aes</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span><span class="w"> </span><span class="n">term_frequency</span><span class="p">,</span><span class="w"> </span><span class="n">color</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">book</span><span class="p">))</span><span class="w"> </span><span class="o">+</span><span class="w"> </span>
<span class="w">  </span><span class="nf">geom_abline</span><span class="p">(</span><span class="n">intercept</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">-0.62</span><span class="p">,</span><span class="w"> </span><span class="n">slope</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">-1.1</span><span class="p">,</span><span class="w"> </span>
<span class="w">              </span><span class="n">color</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;gray50&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">linetype</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">2</span><span class="p">)</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="nf">geom_line</span><span class="p">(</span><span class="n">linewidth</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1.1</span><span class="p">,</span><span class="w"> </span><span class="n">alpha</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0.8</span><span class="p">,</span><span class="w"> </span><span class="n">show.legend</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">FALSE</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w"> </span>
<span class="w">  </span><span class="nf">scale_x_log10</span><span class="p">()</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="nf">scale_y_log10</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<a class="reference internal image-reference" href="../../../_images/4e7f4fe06547962b8f7c3c5ddbfb18103ba2e1751d77701105eabeff809742d4.png"><img alt="../../../_images/4e7f4fe06547962b8f7c3c5ddbfb18103ba2e1751d77701105eabeff809742d4.png" src="../../../_images/4e7f4fe06547962b8f7c3c5ddbfb18103ba2e1751d77701105eabeff809742d4.png" style="width: 420px; height: 420px;" />
</a>
</div>
</div>
<p>Our analysis of Jane Austen’s novels reveals a result close to the classic Zipf’s law. The deviations at high ranks—where fewer rare words appear than a single power law would predict—are typical in many language corpora. However, the deviations at low ranks are more unusual: Austen uses a smaller proportion of the very most common words compared to many other text collections. This type of analysis can be easily extended to compare different authors or other text collections, all while leveraging the simplicity and power of tidy data principles.</p>
</section>
<section id="the-bind-tf-idf-function">
<h3>The <code class="docutils literal notranslate"><span class="pre">bind_tf_idf()</span></code> Function<a class="headerlink" href="#the-bind-tf-idf-function" title="Link to this heading">#</a></h3>
<p>The idea behind <em>tf-idf</em> is to identify the most important words in each document by reducing the weight of very common words and increasing the weight of words that are relatively rare across the whole collection—in this case, Jane Austen’s novels as a group. Calculating <em>tf-idf</em> helps highlight words that are frequent within a specific text but not overused across the entire corpus.</p>
<p>To do this, we use the <a class="reference external" href="https://juliasilge.github.io/tidytext/reference/bind_tf_idf.html"><code class="docutils literal notranslate"><span class="pre">bind_tf_idf()</span></code></a> function from the <strong>tidytext</strong> package, which takes a tidy text dataset as input. The dataset should have one row per token (word) per document. It needs a column with the terms (like <code class="docutils literal notranslate"><span class="pre">word</span></code>), a column identifying the documents (like <code class="docutils literal notranslate"><span class="pre">book</span></code>), and a column with the counts of each term in each document (<code class="docutils literal notranslate"><span class="pre">n</span></code>). Although we previously calculated totals for each book, <a class="reference external" href="https://juliasilge.github.io/tidytext/reference/bind_tf_idf.html"><code class="docutils literal notranslate"><span class="pre">bind_tf_idf()</span></code></a> doesn’t require those—just the complete list of words and their counts for each document.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">book_tf_idf</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">book_words</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">bind_tf_idf</span><span class="p">(</span><span class="n">word</span><span class="p">,</span><span class="w"> </span><span class="n">book</span><span class="p">,</span><span class="w"> </span><span class="n">n</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="n">book_tf_idf</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span># A tibble: 40,378 x 7
   book              word      n  total     tf   idf tf_idf
   &lt;fct&gt;             &lt;chr&gt; &lt;int&gt;  &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;
 1 Mansfield Park    the    6206 160465 0.0387     0      0
 2 Mansfield Park    to     5475 160465 0.0341     0      0
 3 Mansfield Park    and    5438 160465 0.0339     0      0
 4 Emma              to     5239 160996 0.0325     0      0
 5 Emma              the    5201 160996 0.0323     0      0
 6 Emma              and    4896 160996 0.0304     0      0
 7 Mansfield Park    of     4778 160465 0.0298     0      0
 8 Pride &amp; Prejudice the    4331 122204 0.0354     0      0
 9 Emma              of     4291 160996 0.0267     0      0
10 Pride &amp; Prejudice to     4162 122204 0.0341     0      0
# i 40,368 more rows
</pre></div>
</div>
</div>
</div>
<p>Notice that the <em>inverse document frequency (idf)</em>, and therefore <em>tf-idf</em>, is zero for extremely common words that appear in all six of Jane Austen’s novels. This happens because the <em>idf</em> is calculated as the natural logarithm of the total number of documents divided by the number of documents containing the word—so when a word appears in every document, that ratio is 1, and log(1) equals zero. As a result, common words get very low or zero tf-idf scores, which reduces their importance in the analysis.</p>
<p>In contrast, words that appear in fewer documents have higher idf values, boosting their <em>tf-idf</em> scores and highlighting their uniqueness to particular texts. Let’s now explore which terms have the highest tf-idf values in Jane Austen’s works.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">book_tf_idf</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">select</span><span class="p">(</span><span class="o">-</span><span class="n">total</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">arrange</span><span class="p">(</span><span class="nf">desc</span><span class="p">(</span><span class="n">tf_idf</span><span class="p">))</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">print</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span># A tibble: 40,378 x 6
   book                word          n      tf   idf  tf_idf
   &lt;fct&gt;               &lt;chr&gt;     &lt;int&gt;   &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;
 1 Sense &amp; Sensibility elinor      623 0.00519  1.79 0.00931
 2 Sense &amp; Sensibility marianne    492 0.00410  1.79 0.00735
 3 Mansfield Park      crawford    493 0.00307  1.79 0.00550
 4 Pride &amp; Prejudice   darcy       373 0.00305  1.79 0.00547
 5 Persuasion          elliot      254 0.00304  1.79 0.00544
 6 Emma                emma        786 0.00488  1.10 0.00536
 7 Northanger Abbey    tilney      196 0.00252  1.79 0.00452
 8 Emma                weston      389 0.00242  1.79 0.00433
 9 Pride &amp; Prejudice   bennet      294 0.00241  1.79 0.00431
10 Persuasion          wentworth   191 0.00228  1.79 0.00409
# i 40,368 more rows
</pre></div>
</div>
</div>
</div>
<p>Here, we observe mostly proper nouns—names that are indeed significant in these novels. Since none of these names appear in all six novels, they receive higher <em>tf-idf scores</em>, making them distinctive and characteristic terms for each individual book within Jane Austen’s corpus.</p>
<p>Some <em>idf</em> values repeat because the corpus has six documents, so the values correspond to the natural logarithms of ratios like ln(6/1), ln(6/2), and so on.</p>
<p>Next, let’s visualize these high tf-idf words to better understand their importance across the novels.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">library</span><span class="p">(</span><span class="n">forcats</span><span class="p">)</span>

<span class="n">book_tf_idf</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">group_by</span><span class="p">(</span><span class="n">book</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">slice_max</span><span class="p">(</span><span class="n">tf_idf</span><span class="p">,</span><span class="w"> </span><span class="n">n</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">15</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">ungroup</span><span class="p">()</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">ggplot</span><span class="p">(</span><span class="nf">aes</span><span class="p">(</span><span class="n">tf_idf</span><span class="p">,</span><span class="w"> </span><span class="nf">fct_reorder</span><span class="p">(</span><span class="n">word</span><span class="p">,</span><span class="w"> </span><span class="n">tf_idf</span><span class="p">),</span><span class="w"> </span><span class="n">fill</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">book</span><span class="p">))</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="nf">geom_col</span><span class="p">(</span><span class="n">show.legend</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">FALSE</span><span class="p">)</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="nf">facet_wrap</span><span class="p">(</span><span class="o">~</span><span class="n">book</span><span class="p">,</span><span class="w"> </span><span class="n">ncol</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">2</span><span class="p">,</span><span class="w"> </span><span class="n">scales</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;free&quot;</span><span class="p">)</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="nf">labs</span><span class="p">(</span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;tf-idf&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">NULL</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<a class="reference internal image-reference" href="../../../_images/0740807dc1ada6768a390deb0c8edd34cba7daeeefa94ba7beee53800f1509c6.png"><img alt="../../../_images/0740807dc1ada6768a390deb0c8edd34cba7daeeefa94ba7beee53800f1509c6.png" src="../../../_images/0740807dc1ada6768a390deb0c8edd34cba7daeeefa94ba7beee53800f1509c6.png" style="width: 420px; height: 420px;" />
</a>
</div>
</div>
<p>These still highlight mainly proper nouns! These words, identified by their <em>high tf-idf scores</em>, are the most important and distinctive terms for each novel—something most readers would agree with. What <em>tf-idf</em> reveals here is that Jane Austen’s language is quite consistent across her six novels, and the key differences that set each novel apart are the proper nouns—the names of characters and places. This perfectly illustrates the purpose of <em>tf-idf</em>: to pinpoint words that are especially significant to individual documents within a larger collection.</p>
</section>
<section id="a-corpus-of-physics-texts">
<h3>A Corpus of Physics Texts<a class="headerlink" href="#a-corpus-of-physics-texts" title="Link to this heading">#</a></h3>
<p>Let’s explore a different set of documents to see which terms stand out as important. Stepping away from fiction and narrative, we’ll download several classic physics texts from Project Gutenberg: <a class="reference external" href="https://www.gutenberg.org/ebooks/37729"><em>Discourse on Floating Bodies</em> by Galileo Galilei</a>, <a class="reference external" href="https://www.gutenberg.org/ebooks/14725"><em>Treatise on Light</em> by Christiaan Huygens</a>, <a class="reference external" href="https://www.gutenberg.org/ebooks/13476"><em>Experiments with Alternate Currents of High Potential and High Frequency</em> by Nikola Tesla</a>, and <a class="reference external" href="https://www.gutenberg.org/ebooks/30155"><em>Relativity: The Special and General Theory</em> by Albert Einstein</a>.</p>
<p>This collection is quite diverse—these physics classics span over 300 years, and some were originally written in other languages before being translated into English. While not perfectly uniform, this variety makes for a fascinating analysis of important terms using <em>tf-idf</em>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">library</span><span class="p">(</span><span class="n">gutenbergr</span><span class="p">)</span>

<span class="n">physics</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">gutenberg_download</span><span class="p">(</span><span class="nf">c</span><span class="p">(</span><span class="m">37729</span><span class="p">,</span><span class="w"> </span><span class="m">14725</span><span class="p">,</span><span class="w"> </span><span class="m">13476</span><span class="p">,</span><span class="w"> </span><span class="m">30155</span><span class="p">),</span><span class="w"> </span>
<span class="w">                              </span><span class="n">meta_fields</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;author&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Now that we have the texts, let’s use <a class="reference external" href="https://juliasilge.github.io/tidytext/reference/unnest_tokens.html"><code class="docutils literal notranslate"><span class="pre">unnest_tokens()</span></code></a> and <a class="reference external" href="https://dplyr.tidyverse.org/reference/count.html"><code class="docutils literal notranslate"><span class="pre">count()</span></code></a> to find out how many times each word was used in each text.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">physics_words</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">physics</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">unnest_tokens</span><span class="p">(</span><span class="n">word</span><span class="p">,</span><span class="w"> </span><span class="n">text</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">count</span><span class="p">(</span><span class="n">author</span><span class="p">,</span><span class="w"> </span><span class="n">word</span><span class="p">,</span><span class="w"> </span><span class="n">sort</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="n">physics_words</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span># A tibble: 12,664 x 3
   author              word      n
   &lt;chr&gt;               &lt;chr&gt; &lt;int&gt;
 1 Galilei, Galileo    the    3760
 2 Tesla, Nikola       the    3604
 3 Huygens, Christiaan the    3553
 4 Einstein, Albert    the    2993
 5 Galilei, Galileo    of     2049
 6 Einstein, Albert    of     2029
 7 Tesla, Nikola       of     1737
 8 Huygens, Christiaan of     1708
 9 Huygens, Christiaan to     1207
10 Tesla, Nikola       a      1177
# i 12,654 more rows
</pre></div>
</div>
</div>
</div>
<p>Here we are looking at just the raw word counts, but it’s important to keep in mind that these documents vary greatly in length. To better understand which terms are truly important relative to the size of each text, we calculate the <em>tf-idf scores</em>. This adjusts for document length and highlights distinctive words in each text. After calculating <em>tf-idf</em>, we can visualize the words with the highest tf-idf values to see which terms stand out most in each physics classic.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">plot_physics</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">physics_words</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">bind_tf_idf</span><span class="p">(</span><span class="n">word</span><span class="p">,</span><span class="w"> </span><span class="n">author</span><span class="p">,</span><span class="w"> </span><span class="n">n</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">mutate</span><span class="p">(</span><span class="n">author</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">factor</span><span class="p">(</span><span class="n">author</span><span class="p">,</span><span class="w"> </span><span class="n">levels</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="s">&quot;Galilei, Galileo&quot;</span><span class="p">,</span>
<span class="w">                                            </span><span class="s">&quot;Huygens, Christiaan&quot;</span><span class="p">,</span><span class="w"> </span>
<span class="w">                                            </span><span class="s">&quot;Tesla, Nikola&quot;</span><span class="p">,</span>
<span class="w">                                            </span><span class="s">&quot;Einstein, Albert&quot;</span><span class="p">)))</span>

<span class="n">plot_physics</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span>
<span class="w">  </span><span class="nf">group_by</span><span class="p">(</span><span class="n">author</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span>
<span class="w">  </span><span class="nf">slice_max</span><span class="p">(</span><span class="n">tf_idf</span><span class="p">,</span><span class="w"> </span><span class="n">n</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">15</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span>
<span class="w">  </span><span class="nf">ungroup</span><span class="p">()</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">mutate</span><span class="p">(</span><span class="n">word</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">reorder</span><span class="p">(</span><span class="n">word</span><span class="p">,</span><span class="w"> </span><span class="n">tf_idf</span><span class="p">))</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">ggplot</span><span class="p">(</span><span class="nf">aes</span><span class="p">(</span><span class="n">tf_idf</span><span class="p">,</span><span class="w"> </span><span class="n">word</span><span class="p">,</span><span class="w"> </span><span class="n">fill</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">author</span><span class="p">))</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="nf">geom_col</span><span class="p">(</span><span class="n">show.legend</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">FALSE</span><span class="p">)</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="nf">labs</span><span class="p">(</span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;tf-idf&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">NULL</span><span class="p">)</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="nf">facet_wrap</span><span class="p">(</span><span class="o">~</span><span class="n">author</span><span class="p">,</span><span class="w"> </span><span class="n">ncol</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">2</span><span class="p">,</span><span class="w"> </span><span class="n">scales</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;free&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<a class="reference internal image-reference" href="../../../_images/2e5970d43d6ca9dc64b22d09f6d7817eaa0abee3a427b5dd71ced9e40f3e4962.png"><img alt="../../../_images/2e5970d43d6ca9dc64b22d09f6d7817eaa0abee3a427b5dd71ced9e40f3e4962.png" src="../../../_images/2e5970d43d6ca9dc64b22d09f6d7817eaa0abee3a427b5dd71ced9e40f3e4962.png" style="width: 420px; height: 420px;" />
</a>
</div>
</div>
<p>Yeah, that definitely catches the eye! The letter “k” showing up with a high tf-idf score in Einstein’s text probably means it’s a key technical term or variable used frequently in <em>Relativity: The Special and General Theory</em> but not much elsewhere. In physics and math, single letters like “k” often represent constants, coefficients, or variables, so it makes sense it would stand out compared to other documents. It’s a neat example of how tf-idf highlights terms that are uniquely important to a specific document! Want to dig into what “k” represents in Einstein’s work?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">library</span><span class="p">(</span><span class="n">stringr</span><span class="p">)</span>

<span class="n">physics</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span>
<span class="w">  </span><span class="nf">filter</span><span class="p">(</span><span class="nf">str_detect</span><span class="p">(</span><span class="n">text</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;_k_&quot;</span><span class="p">))</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span>
<span class="w">  </span><span class="nf">select</span><span class="p">(</span><span class="n">text</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">print</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span># A tibble: 7 x 1
  text                                                                         
  &lt;chr&gt;                                                                        
1 &quot;surface AB at the points AK_k_B. Then instead of the hemispherical&quot;         
2 &quot;would needs be that from all the other points K_k_B there should&quot;           
3 &quot;necessarily be equal to CD, because C_k_ is equal to CK, and C_g_ to&quot;       
4 &quot;the crystal at K_k_, all the points of the wave CO_oc_ will have&quot;           
5 &quot;O_o_ has reached K_k_. Which is easy to comprehend, since, of these&quot;        
6 &quot;CO_oc_ in the crystal, when O_o_ has arrived at K_k_, because it forms&quot;     
7 &quot;\u03c1 is the average density of the matter and _k_ is a constant connected&quot;
</pre></div>
</div>
</div>
</div>
<p>Some text cleaning may be necessary. Also, notice that “co” and “ordinate” appear separately among the high tf-idf words for Einstein’s text; this happens because <a class="reference external" href="https://juliasilge.github.io/tidytext/reference/unnest_tokens.html">`unnest_tokens()</a> by default splits on punctuation such as hyphens. The tf-idf values for “co” and “ordinate” are nearly identical!</p>
<p>Additionally, abbreviations like “AB” and “RC” refer to names of rays, circles, angles, and similar concepts in Huygens’ work.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">physics</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span>
<span class="w">  </span><span class="nf">filter</span><span class="p">(</span><span class="nf">str_detect</span><span class="p">(</span><span class="n">text</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;RC&quot;</span><span class="p">))</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span>
<span class="w">  </span><span class="nf">select</span><span class="p">(</span><span class="n">text</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">print</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span># A tibble: 44 x 1
   text                                                                  
   &lt;chr&gt;                                                                 
 1 line RC, parallel and equal to AB, to be a portion of a wave of light,
 2 represents the partial wave coming from the point A, after the wave RC
 3 be the propagation of the wave RC which fell on AB, and would be the  
 4 transparent body; seeing that the wave RC, having come to the aperture
 5 incident rays. Let there be such a ray RC falling upon the surface    
 6 CK. Make CO perpendicular to RC, and across the angle KCO adjust OK,  
 7 the required refraction of the ray RC. The demonstration of this is,  
 8 explaining ordinary refraction. For the refraction of the ray RC is   
 9 29. Now as we have found CI the refraction of the ray RC, similarly   
10 the ray _r_C is inclined equally with RC, the line C_d_ will          
# i 34 more rows
</pre></div>
</div>
</div>
</div>
<p>To create a clearer and more meaningful plot, let’s remove some of these less relevant words. We do this by creating a custom list of stop words and applying <a class="reference external" href="https://dplyr.tidyverse.org/reference/filter-joins.html"><code class="docutils literal notranslate"><span class="pre">anti_join()</span></code></a> to filter them out. This method is flexible and can be adapted to various situations. Since we’re removing words from the tidy data frame, we’ll need to revisit some earlier steps in the process.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">mystopwords</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">tibble</span><span class="p">(</span><span class="n">word</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="s">&quot;eq&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;co&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;rc&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;ac&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;ak&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;bn&quot;</span><span class="p">,</span><span class="w"> </span>
<span class="w">                                   </span><span class="s">&quot;fig&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;file&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;cg&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;cb&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;cm&quot;</span><span class="p">,</span>
<span class="w">                               </span><span class="s">&quot;ab&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;_k&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;_k_&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;_x&quot;</span><span class="p">))</span>

<span class="n">physics_words</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">anti_join</span><span class="p">(</span><span class="n">physics_words</span><span class="p">,</span><span class="w"> </span><span class="n">mystopwords</span><span class="p">,</span><span class="w"> </span>
<span class="w">                           </span><span class="n">by</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;word&quot;</span><span class="p">)</span>

<span class="n">plot_physics</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">physics_words</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">bind_tf_idf</span><span class="p">(</span><span class="n">word</span><span class="p">,</span><span class="w"> </span><span class="n">author</span><span class="p">,</span><span class="w"> </span><span class="n">n</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">mutate</span><span class="p">(</span><span class="n">word</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">str_remove_all</span><span class="p">(</span><span class="n">word</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;_&quot;</span><span class="p">))</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">group_by</span><span class="p">(</span><span class="n">author</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span>
<span class="w">  </span><span class="nf">slice_max</span><span class="p">(</span><span class="n">tf_idf</span><span class="p">,</span><span class="w"> </span><span class="n">n</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">15</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">ungroup</span><span class="p">()</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">mutate</span><span class="p">(</span><span class="n">word</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">fct_reorder</span><span class="p">(</span><span class="n">word</span><span class="p">,</span><span class="w"> </span><span class="n">tf_idf</span><span class="p">))</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">mutate</span><span class="p">(</span><span class="n">author</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">factor</span><span class="p">(</span><span class="n">author</span><span class="p">,</span><span class="w"> </span><span class="n">levels</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="s">&quot;Galilei, Galileo&quot;</span><span class="p">,</span>
<span class="w">                                            </span><span class="s">&quot;Huygens, Christiaan&quot;</span><span class="p">,</span>
<span class="w">                                            </span><span class="s">&quot;Tesla, Nikola&quot;</span><span class="p">,</span>
<span class="w">                                            </span><span class="s">&quot;Einstein, Albert&quot;</span><span class="p">)))</span>

<span class="nf">ggplot</span><span class="p">(</span><span class="n">plot_physics</span><span class="p">,</span><span class="w"> </span><span class="nf">aes</span><span class="p">(</span><span class="n">tf_idf</span><span class="p">,</span><span class="w"> </span><span class="n">word</span><span class="p">,</span><span class="w"> </span><span class="n">fill</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">author</span><span class="p">))</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="nf">geom_col</span><span class="p">(</span><span class="n">show.legend</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">FALSE</span><span class="p">)</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="nf">facet_wrap</span><span class="p">(</span><span class="o">~</span><span class="n">author</span><span class="p">,</span><span class="w"> </span><span class="n">ncol</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">2</span><span class="p">,</span><span class="w"> </span><span class="n">scales</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;free&quot;</span><span class="p">)</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="nf">labs</span><span class="p">(</span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;tf-idf&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">NULL</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<a class="reference internal image-reference" href="../../../_images/5ec2e1a3a1112a86d1cde522e2cb0bc35a2fa237231f5b8f1a191840d3bf2d82.png"><img alt="../../../_images/5ec2e1a3a1112a86d1cde522e2cb0bc35a2fa237231f5b8f1a191840d3bf2d82.png" src="../../../_images/5ec2e1a3a1112a86d1cde522e2cb0bc35a2fa237231f5b8f1a191840d3bf2d82.png" style="width: 420px; height: 420px;" />
</a>
</div>
</div>
<p>One takeaway from this is that modern physics discussions rarely mention ramparts or describe things as ethereal.</p>
<p>The examples from Jane Austen and physics in this chapter showed little overlap in high tf-idf words across different categories like books or authors. However, if you do encounter shared high tf-idf words across categories, you might consider using <a class="reference external" href="https://juliasilge.github.io/tidytext/reference/reorder_within.html"><code class="docutils literal notranslate"><span class="pre">reorder_within()</span></code></a> and <code class="docutils literal notranslate"><span class="pre">scale_*_reordered()</span></code> to create clearer visualizations.</p>
</section>
</section>
<section id="relationships-between-words">
<h2>Relationships Between Words<a class="headerlink" href="#relationships-between-words" title="Link to this heading">#</a></h2>
<p>Up to this point, we’ve treated words as individual units and explored how they relate to sentiments or documents. However, much of text analysis focuses on the relationships between words — such as which words tend to appear next to one another or co-occur within the same text.</p>
<p>In this chapter, we’ll look at tidytext methods for calculating and visualizing word relationships within the dataset. This includes using <code class="docutils literal notranslate"><span class="pre">token</span> <span class="pre">=</span> <span class="pre">&quot;ngrams&quot;</span></code> to break text into pairs of adjacent words instead of single words. We’ll also introduce two helpful packages: <a class="reference external" href="https://github.com/thomasp85/ggraph">ggraph</a>, which builds network plots based on ggplot2, and <a class="reference external" href="https://github.com/juliasilge/widyr">widyr</a>, which calculates pairwise correlations and distances in tidy data frames. These tools expand how we can explore text data within the tidy framework.</p>
<section id="tokenizing-by-n-gram">
<h3>Tokenizing by n-gram<a class="headerlink" href="#tokenizing-by-n-gram" title="Link to this heading">#</a></h3>
<p>So far, we’ve used <code class="docutils literal notranslate"><span class="pre">unnest_tokens()</span></code> to break text into individual words or sentences, which works well for sentiment and frequency analysis. However, we can also use this function to create sequences of consecutive words, known as <em>n-grams</em>. By analyzing how often one word follows another, we can better understand the relationships between words.</p>
<p>To do this, we simply add <code class="docutils literal notranslate"><span class="pre">token</span> <span class="pre">=</span> <span class="pre">&quot;ngrams&quot;</span></code> to <a class="reference external" href="https://juliasilge.github.io/tidytext/reference/unnest_tokens.html"><code class="docutils literal notranslate"><span class="pre">unnest_tokens()</span></code></a> and set <code class="docutils literal notranslate"><span class="pre">n</span></code> to the number of consecutive words we want to group. When <code class="docutils literal notranslate"><span class="pre">n</span> <span class="pre">=</span> <span class="pre">2</span></code>, we generate pairs of consecutive words, commonly referred to as <strong>bigrams</strong>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">library</span><span class="p">(</span><span class="n">dplyr</span><span class="p">)</span>
<span class="nf">library</span><span class="p">(</span><span class="n">tidytext</span><span class="p">)</span>
<span class="nf">library</span><span class="p">(</span><span class="n">janeaustenr</span><span class="p">)</span>

<span class="n">austen_bigrams</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">austen_books</span><span class="p">()</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">unnest_tokens</span><span class="p">(</span><span class="n">bigram</span><span class="p">,</span><span class="w"> </span><span class="n">text</span><span class="p">,</span><span class="w"> </span><span class="n">token</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;ngrams&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">n</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">2</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">filter</span><span class="p">(</span><span class="o">!</span><span class="nf">is.na</span><span class="p">(</span><span class="n">bigram</span><span class="p">))</span>

<span class="nf">print</span><span class="p">(</span><span class="n">austen_bigrams</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span># A tibble: 662,792 x 2
   book                bigram         
   &lt;fct&gt;               &lt;chr&gt;          
 1 Sense &amp; Sensibility sense and      
 2 Sense &amp; Sensibility and sensibility
 3 Sense &amp; Sensibility by jane        
 4 Sense &amp; Sensibility jane austen    
 5 Sense &amp; Sensibility chapter 1      
 6 Sense &amp; Sensibility the family     
 7 Sense &amp; Sensibility family of      
 8 Sense &amp; Sensibility of dashwood    
 9 Sense &amp; Sensibility dashwood had   
10 Sense &amp; Sensibility had long       
# i 662,782 more rows
</pre></div>
</div>
</div>
</div>
<p>This structure remains consistent with the tidy text format, where each row represents one token, but in this case, the tokens are bigrams rather than individual words. Additional information, like the book name, is still included.</p>
<p>It’s important to note that these bigrams overlap — for example, “sense and” is one token, and “and sensibility” is the next.</p>
<section id="counting-and-filtering-n-grams">
<h4>Counting and Filtering n-grams<a class="headerlink" href="#counting-and-filtering-n-grams" title="Link to this heading">#</a></h4>
<p>The same tidy tools we’ve been using for single words work just as well for n-gram analysis. We can use functions like <a class="reference external" href="https://dplyr.tidyverse.org/reference/count.html"><code class="docutils literal notranslate"><span class="pre">count()</span></code></a> from dplyr to easily find the most common bigrams in the text.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">austen_bigrams</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">count</span><span class="p">(</span><span class="n">bigram</span><span class="p">,</span><span class="w"> </span><span class="n">sort</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">print</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span># A tibble: 193,212 x 2
   bigram       n
   &lt;chr&gt;    &lt;int&gt;
 1 of the    2853
 2 to be     2670
 3 in the    2221
 4 it was    1691
 5 i am      1485
 6 she had   1405
 7 of her    1363
 8 to the    1315
 9 she was   1309
10 had been  1206
# i 193,202 more rows
</pre></div>
</div>
</div>
</div>
<p>As expected, many of the most frequent bigrams consist of common, uninformative words like “of the” or “to be,” which are considered stop words. To clean this up, we can use <a class="reference external" href="https://tidyr.tidyverse.org/reference/separate.html"><code class="docutils literal notranslate"><span class="pre">separate()</span></code></a> from tidyr to split the bigrams into two columns—<code class="docutils literal notranslate"><span class="pre">word1</span></code> and <code class="docutils literal notranslate"><span class="pre">word2</span></code>—allowing us to filter out any bigrams where either word is a stop word.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">library</span><span class="p">(</span><span class="n">tidyr</span><span class="p">)</span>

<span class="n">bigrams_separated</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">austen_bigrams</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">separate</span><span class="p">(</span><span class="n">bigram</span><span class="p">,</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="s">&quot;word1&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;word2&quot;</span><span class="p">),</span><span class="w"> </span><span class="n">sep</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot; &quot;</span><span class="p">)</span>

<span class="n">bigrams_filtered</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">bigrams_separated</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">filter</span><span class="p">(</span><span class="o">!</span><span class="n">word1</span><span class="w"> </span><span class="o">%in%</span><span class="w"> </span><span class="n">stop_words</span><span class="o">$</span><span class="n">word</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">filter</span><span class="p">(</span><span class="o">!</span><span class="n">word2</span><span class="w"> </span><span class="o">%in%</span><span class="w"> </span><span class="n">stop_words</span><span class="o">$</span><span class="n">word</span><span class="p">)</span>

<span class="c1"># new bigram counts:</span>
<span class="n">bigram_counts</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">bigrams_filtered</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span>
<span class="w">  </span><span class="nf">count</span><span class="p">(</span><span class="n">word1</span><span class="p">,</span><span class="w"> </span><span class="n">word2</span><span class="p">,</span><span class="w"> </span><span class="n">sort</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="n">bigram_counts</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span># A tibble: 28,971 x 3
   word1   word2         n
   &lt;chr&gt;   &lt;chr&gt;     &lt;int&gt;
 1 sir     thomas      266
 2 miss    crawford    196
 3 captain wentworth   143
 4 miss    woodhouse   143
 5 frank   churchill   114
 6 lady    russell     110
 7 sir     walter      108
 8 lady    bertram     101
 9 miss    fairfax      98
10 colonel brandon      96
# i 28,961 more rows
</pre></div>
</div>
</div>
</div>
<p>We notice that many of the most frequent bigrams in Jane Austen’s novels are names, either full names or names paired with titles like “Mr.” or “Miss.”</p>
<p>In some cases, though, we might want to work with the bigrams as single, recombined units again. For that, we can use <a class="reference external" href="https://tidyr.tidyverse.org/reference/unite.html"><code class="docutils literal notranslate"><span class="pre">unite()</span></code></a> from tidyr, which reverses the effect of <a class="reference external" href="https://tidyr.tidyverse.org/reference/separate.html"><code class="docutils literal notranslate"><span class="pre">separate()</span></code></a> and merges multiple columns back into one. Using a combination of <code class="docutils literal notranslate"><span class="pre">separate()</span></code>, filtering out stop words, <code class="docutils literal notranslate"><span class="pre">count()</span></code>, and then <code class="docutils literal notranslate"><span class="pre">unite()</span></code>, we can easily identify the most frequent meaningful bigrams that exclude common stop words.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">bigrams_united</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">bigrams_filtered</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">unite</span><span class="p">(</span><span class="n">bigram</span><span class="p">,</span><span class="w"> </span><span class="n">word1</span><span class="p">,</span><span class="w"> </span><span class="n">word2</span><span class="p">,</span><span class="w"> </span><span class="n">sep</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot; &quot;</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="n">bigrams_united</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span># A tibble: 38,910 x 2
   book                bigram                  
   &lt;fct&gt;               &lt;chr&gt;                   
 1 Sense &amp; Sensibility jane austen             
 2 Sense &amp; Sensibility chapter 1               
 3 Sense &amp; Sensibility norland park            
 4 Sense &amp; Sensibility surrounding acquaintance
 5 Sense &amp; Sensibility late owner              
 6 Sense &amp; Sensibility advanced age            
 7 Sense &amp; Sensibility constant companion      
 8 Sense &amp; Sensibility happened ten            
 9 Sense &amp; Sensibility henry dashwood          
10 Sense &amp; Sensibility norland estate          
# i 38,900 more rows
</pre></div>
</div>
</div>
</div>
<p>For other types of analysis, you might want to focus on trigrams—sequences of three consecutive words. To extract those, you simply set <code class="docutils literal notranslate"><span class="pre">n</span> <span class="pre">=</span> <span class="pre">3</span></code> when using <code class="docutils literal notranslate"><span class="pre">unnest_tokens()</span></code> with <code class="docutils literal notranslate"><span class="pre">token</span> <span class="pre">=</span> <span class="pre">&quot;ngrams&quot;</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">austen_books</span><span class="p">()</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">unnest_tokens</span><span class="p">(</span><span class="n">trigram</span><span class="p">,</span><span class="w"> </span><span class="n">text</span><span class="p">,</span><span class="w"> </span><span class="n">token</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;ngrams&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">n</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">3</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">filter</span><span class="p">(</span><span class="o">!</span><span class="nf">is.na</span><span class="p">(</span><span class="n">trigram</span><span class="p">))</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">separate</span><span class="p">(</span><span class="n">trigram</span><span class="p">,</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="s">&quot;word1&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;word2&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;word3&quot;</span><span class="p">),</span><span class="w"> </span><span class="n">sep</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot; &quot;</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">filter</span><span class="p">(</span><span class="o">!</span><span class="n">word1</span><span class="w"> </span><span class="o">%in%</span><span class="w"> </span><span class="n">stop_words</span><span class="o">$</span><span class="n">word</span><span class="p">,</span>
<span class="w">         </span><span class="o">!</span><span class="n">word2</span><span class="w"> </span><span class="o">%in%</span><span class="w"> </span><span class="n">stop_words</span><span class="o">$</span><span class="n">word</span><span class="p">,</span>
<span class="w">         </span><span class="o">!</span><span class="n">word3</span><span class="w"> </span><span class="o">%in%</span><span class="w"> </span><span class="n">stop_words</span><span class="o">$</span><span class="n">word</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">count</span><span class="p">(</span><span class="n">word1</span><span class="p">,</span><span class="w"> </span><span class="n">word2</span><span class="p">,</span><span class="w"> </span><span class="n">word3</span><span class="p">,</span><span class="w"> </span><span class="n">sort</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">print</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span># A tibble: 6,139 x 4
   word1     word2     word3         n
   &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;     &lt;int&gt;
 1 dear      miss      woodhouse    20
 2 miss      de        bourgh       17
 3 lady      catherine de           11
 4 poor      miss      taylor       11
 5 sir       walter    elliot       10
 6 catherine de        bourgh        9
 7 dear      sir       thomas        8
 8 replied   miss      crawford      7
 9 sir       william   lucas         7
10 ten       thousand  pounds        7
# i 6,129 more rows
</pre></div>
</div>
</div>
</div>
</section>
<section id="analyzing-bigrams">
<h4>Analyzing bigrams<a class="headerlink" href="#analyzing-bigrams" title="Link to this heading">#</a></h4>
<p>Having one bigram per row makes exploratory text analysis straightforward. For instance, we can easily investigate the most frequently mentioned “streets” in each book by filtering bigrams for those where the second word is “street” and summarizing the results.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">bigrams_filtered</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">filter</span><span class="p">(</span><span class="n">word2</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="s">&quot;street&quot;</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">count</span><span class="p">(</span><span class="n">book</span><span class="p">,</span><span class="w"> </span><span class="n">word1</span><span class="p">,</span><span class="w"> </span><span class="n">sort</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">print</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span># A tibble: 33 x 3
   book                word1           n
   &lt;fct&gt;               &lt;chr&gt;       &lt;int&gt;
 1 Sense &amp; Sensibility harley         16
 2 Sense &amp; Sensibility berkeley       15
 3 Northanger Abbey    milsom         10
 4 Northanger Abbey    pulteney       10
 5 Mansfield Park      wimpole         9
 6 Pride &amp; Prejudice   gracechurch     8
 7 Persuasion          milsom          5
 8 Sense &amp; Sensibility bond            4
 9 Sense &amp; Sensibility conduit         4
10 Persuasion          rivers          4
# i 23 more rows
</pre></div>
</div>
</div>
</div>
<p>We can also treat bigrams as terms within documents, similar to how we handled single words. For instance, we can calculate the <em>tf-idf</em> values for bigrams across Jane Austen’s novels. These <em>tf-idf scores</em> highlight which word pairs are most characteristic of each book, and we can visualize them in the same way we previously visualized single-word <em>tf-idf</em> values.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">bigram_tf_idf</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">bigrams_united</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">count</span><span class="p">(</span><span class="n">book</span><span class="p">,</span><span class="w"> </span><span class="n">bigram</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">bind_tf_idf</span><span class="p">(</span><span class="n">bigram</span><span class="p">,</span><span class="w"> </span><span class="n">book</span><span class="p">,</span><span class="w"> </span><span class="n">n</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">arrange</span><span class="p">(</span><span class="nf">desc</span><span class="p">(</span><span class="n">tf_idf</span><span class="p">))</span>

<span class="nf">print</span><span class="p">(</span><span class="n">bigram_tf_idf</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span># A tibble: 31,388 x 6
   book                bigram                n     tf   idf tf_idf
   &lt;fct&gt;               &lt;chr&gt;             &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;
 1 Mansfield Park      sir thomas          266 0.0304  1.79 0.0546
 2 Persuasion          captain wentworth   143 0.0290  1.79 0.0519
 3 Mansfield Park      miss crawford       196 0.0224  1.79 0.0402
 4 Persuasion          lady russell        110 0.0223  1.79 0.0399
 5 Persuasion          sir walter          108 0.0219  1.79 0.0392
 6 Emma                miss woodhouse      143 0.0173  1.79 0.0309
 7 Northanger Abbey    miss tilney          74 0.0165  1.79 0.0295
 8 Sense &amp; Sensibility colonel brandon      96 0.0155  1.79 0.0278
 9 Sense &amp; Sensibility sir john             94 0.0152  1.79 0.0273
10 Pride &amp; Prejudice   lady catherine       87 0.0139  1.79 0.0248
# i 31,378 more rows
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">bigram_tf_idf</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">group_by</span><span class="p">(</span><span class="n">book</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">slice_max</span><span class="p">(</span><span class="n">tf_idf</span><span class="p">,</span><span class="w"> </span><span class="n">n</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">10</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">ungroup</span><span class="p">()</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">ggplot</span><span class="p">(</span><span class="nf">aes</span><span class="p">(</span><span class="n">tf_idf</span><span class="p">,</span><span class="w"> </span><span class="nf">fct_reorder</span><span class="p">(</span><span class="n">bigram</span><span class="p">,</span><span class="w"> </span><span class="n">tf_idf</span><span class="p">),</span><span class="w"> </span><span class="n">fill</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">book</span><span class="p">))</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="nf">geom_col</span><span class="p">(</span><span class="n">show.legend</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">FALSE</span><span class="p">)</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="nf">facet_wrap</span><span class="p">(</span><span class="o">~</span><span class="n">book</span><span class="p">,</span><span class="w"> </span><span class="n">ncol</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">2</span><span class="p">,</span><span class="w"> </span><span class="n">scales</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;free&quot;</span><span class="p">)</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="nf">labs</span><span class="p">(</span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;tf-idf&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">NULL</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<a class="reference internal image-reference" href="../../../_images/38e5ede7d5902ea2a2cb72b4e3e2f8f4051bed5afb4e25a6d775bef87df22869.png"><img alt="../../../_images/38e5ede7d5902ea2a2cb72b4e3e2f8f4051bed5afb4e25a6d775bef87df22869.png" src="../../../_images/38e5ede7d5902ea2a2cb72b4e3e2f8f4051bed5afb4e25a6d775bef87df22869.png" style="width: 420px; height: 420px;" />
</a>
</div>
</div>
<p>Just like we saw in Chapter 3, the bigrams that distinguish each Austen novel are mostly names. We also notice some verb-name pairs, like “replied elinor” in <em>Sense &amp; Sensibility</em> or “cried catherine” in <em>Northanger Abbey</em>.</p>
<p>Analyzing tf-idf for bigrams has its pros and cons. On the plus side, pairs of consecutive words can capture context and structure that single words miss—“maple grove” in <em>Emma</em>, for example, is more meaningful than just “maple.” On the downside, bigrams tend to be much rarer than individual words, so their counts are sparser. Because of this, bigrams are especially helpful when working with very large text datasets.</p>
</section>
<section id="using-bigrams-to-provide-context">
<h4>Using Bigrams to Provide Context<a class="headerlink" href="#using-bigrams-to-provide-context" title="Link to this heading">#</a></h4>
<p>Our sentiment analysis method from above counted positive or negative words based on a reference lexicon. However, one issue with this approach is that context can be just as important as the word itself. For instance, words like “happy” and “like” would register as positive even in sentences like “I’m not happy and I don’t like it!”</p>
<p>With our data now structured as bigrams, it becomes straightforward to see how often words are preceded by negations like “not”:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">bigrams_separated</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">filter</span><span class="p">(</span><span class="n">word1</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="s">&quot;not&quot;</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">count</span><span class="p">(</span><span class="n">word1</span><span class="p">,</span><span class="w"> </span><span class="n">word2</span><span class="p">,</span><span class="w"> </span><span class="n">sort</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">print</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span># A tibble: 1,178 x 3
   word1 word2     n
   &lt;chr&gt; &lt;chr&gt; &lt;int&gt;
 1 not   be      580
 2 not   to      335
 3 not   have    307
 4 not   know    237
 5 not   a       184
 6 not   think   162
 7 not   been    151
 8 not   the     135
 9 not   at      126
10 not   in      110
# i 1,168 more rows
</pre></div>
</div>
</div>
</div>
<p>By conducting sentiment analysis on the bigram data, we can identify how frequently sentiment-bearing words are preceded by negations like “not.” This allows us to either exclude or invert their sentiment contribution accordingly.</p>
<p>Let’s apply the AFINN lexicon for this analysis, which assigns numeric sentiment scores to words—positive values for positive sentiment and negative values for negative sentiment.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">AFINN</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">get_sentiments</span><span class="p">(</span><span class="s">&quot;afinn&quot;</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="n">AFINN</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span># A tibble: 2,477 x 2
   word       value
   &lt;chr&gt;      &lt;dbl&gt;
 1 abandon       -<span class=" -Color -Color-Red">2</span>
 2 abandoned     -<span class=" -Color -Color-Red">2</span>
 3 abandons      -<span class=" -Color -Color-Red">2</span>
 4 abducted      -<span class=" -Color -Color-Red">2</span>
 5 abduction     -<span class=" -Color -Color-Red">2</span>
 6 abductions    -<span class=" -Color -Color-Red">2</span>
 7 abhor         -<span class=" -Color -Color-Red">3</span>
 8 abhorred      -<span class=" -Color -Color-Red">3</span>
 9 abhorrent     -<span class=" -Color -Color-Red">3</span>
10 abhors        -<span class=" -Color -Color-Red">3</span>
# i 2,467 more rows
</pre></div>
</div>
</div>
</div>
<p>Next, we can look at which sentiment-associated words most often appear right after “not.” This helps us understand how negation affects the sentiment of those words by identifying the most frequent such bigrams.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">not_words</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">bigrams_separated</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">filter</span><span class="p">(</span><span class="n">word1</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="s">&quot;not&quot;</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">inner_join</span><span class="p">(</span><span class="n">AFINN</span><span class="p">,</span><span class="w"> </span><span class="n">by</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="n">word2</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;word&quot;</span><span class="p">))</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">count</span><span class="p">(</span><span class="n">word2</span><span class="p">,</span><span class="w"> </span><span class="n">value</span><span class="p">,</span><span class="w"> </span><span class="n">sort</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="n">not_words</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span># A tibble: 229 x 3
   word2   value     n
   &lt;chr&gt;   &lt;dbl&gt; &lt;int&gt;
 1 like        2    95
 2 help        2    77
 3 want        1    41
 4 wish        1    39
 5 allow       1    30
 6 care        2    21
 7 sorry      -<span class=" -Color -Color-Red">1</span>    20
 8 leave      -<span class=" -Color -Color-Red">1</span>    17
 9 pretend    -<span class=" -Color -Color-Red">1</span>    17
10 worth       2    17
# i 219 more rows
</pre></div>
</div>
</div>
</div>
<p>For instance, the word most frequently following “not” with an associated sentiment was “like,” which typically carries a positive score of 2.</p>
<p>It’s also insightful to identify which words contributed most to sentiment in the “wrong” direction due to negation. We calculate this by multiplying each word’s sentiment value by its frequency — so a word with a sentiment score of +3 appearing 10 times has the same impact as a word with a score of +1 appearing 30 times. We then visualize these results in a bar plot.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">library</span><span class="p">(</span><span class="n">ggplot2</span><span class="p">)</span>

<span class="w">  </span><span class="nf">mutate</span><span class="p">(</span><span class="n">contribution</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">n</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">value</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">arrange</span><span class="p">(</span><span class="nf">desc</span><span class="p">(</span><span class="nf">abs</span><span class="p">(</span><span class="n">contribution</span><span class="p">)))</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">head</span><span class="p">(</span><span class="m">20</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">mutate</span><span class="p">(</span><span class="n">word2</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">reorder</span><span class="p">(</span><span class="n">word2</span><span class="p">,</span><span class="w"> </span><span class="n">contribution</span><span class="p">))</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">ggplot</span><span class="p">(</span><span class="nf">aes</span><span class="p">(</span><span class="n">n</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">value</span><span class="p">,</span><span class="w"> </span><span class="n">word2</span><span class="p">,</span><span class="w"> </span><span class="n">fill</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">n</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">value</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="m">0</span><span class="p">))</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="nf">geom_col</span><span class="p">(</span><span class="n">show.legend</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">FALSE</span><span class="p">)</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="nf">labs</span><span class="p">(</span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;Sentiment value * number of occurrences&quot;</span><span class="p">,</span>
<span class="w">       </span><span class="n">y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;Words preceded by \&quot;not\&quot;&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<a class="reference internal image-reference" href="../../../_images/7071d0ac99dede0ebcee73f8526d2dd4870b8465dfa1f61367ba7533caa70d1c.png"><img alt="../../../_images/7071d0ac99dede0ebcee73f8526d2dd4870b8465dfa1f61367ba7533caa70d1c.png" src="../../../_images/7071d0ac99dede0ebcee73f8526d2dd4870b8465dfa1f61367ba7533caa70d1c.png" style="width: 420px; height: 420px;" />
</a>
</div>
</div>
<p>The bigrams “not like” and “not help” were by far the biggest contributors to misclassification, causing the text to appear more positive than it actually is. However, phrases such as “not afraid” and “not fail” sometimes make the text seem more negative than intended.</p>
<p>“Not” isn’t the only word that modifies the sentiment of the following term. We could select four or more common negation words and apply the same join-and-count method to analyze all of them together.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">negation_words</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="s">&quot;not&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;no&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;never&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;without&quot;</span><span class="p">)</span>

<span class="n">negated_words</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">bigrams_separated</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">filter</span><span class="p">(</span><span class="n">word1</span><span class="w"> </span><span class="o">%in%</span><span class="w"> </span><span class="n">negation_words</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">inner_join</span><span class="p">(</span><span class="n">AFINN</span><span class="p">,</span><span class="w"> </span><span class="n">by</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="n">word2</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;word&quot;</span><span class="p">))</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">count</span><span class="p">(</span><span class="n">word1</span><span class="p">,</span><span class="w"> </span><span class="n">word2</span><span class="p">,</span><span class="w"> </span><span class="n">value</span><span class="p">,</span><span class="w"> </span><span class="n">sort</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We can then create a visualization showing the most frequent words that follow each specific negation word. While “not like” and “not help” remain the top examples, we also observe pairs like “no great” and “never loved.” Combining this with the methods from before, we could invert the AFINN sentiment scores for words that come after negations. This demonstrates just a few ways that analyzing consecutive words adds valuable context to text mining techniques.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">negated_words</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">mutate</span><span class="p">(</span><span class="n">contribution</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">n</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">value</span><span class="p">,</span>
<span class="w">         </span><span class="n">word2</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">reorder</span><span class="p">(</span><span class="nf">paste</span><span class="p">(</span><span class="n">word2</span><span class="p">,</span><span class="w"> </span><span class="n">word1</span><span class="p">,</span><span class="w"> </span><span class="n">sep</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;__&quot;</span><span class="p">),</span><span class="w"> </span><span class="n">contribution</span><span class="p">))</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">group_by</span><span class="p">(</span><span class="n">word1</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">slice_max</span><span class="p">(</span><span class="nf">abs</span><span class="p">(</span><span class="n">contribution</span><span class="p">),</span><span class="w"> </span><span class="n">n</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">12</span><span class="p">,</span><span class="w"> </span><span class="n">with_ties</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">FALSE</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">ggplot</span><span class="p">(</span><span class="nf">aes</span><span class="p">(</span><span class="n">word2</span><span class="p">,</span><span class="w"> </span><span class="n">contribution</span><span class="p">,</span><span class="w"> </span><span class="n">fill</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">n</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">value</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="m">0</span><span class="p">))</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="nf">geom_col</span><span class="p">(</span><span class="n">show.legend</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">FALSE</span><span class="p">)</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="nf">facet_wrap</span><span class="p">(</span><span class="o">~</span><span class="w"> </span><span class="n">word1</span><span class="p">,</span><span class="w"> </span><span class="n">scales</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;free&quot;</span><span class="p">)</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="nf">scale_x_discrete</span><span class="p">(</span><span class="n">labels</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kr">function</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="w"> </span><span class="nf">gsub</span><span class="p">(</span><span class="s">&quot;__.+$&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">x</span><span class="p">))</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="nf">xlab</span><span class="p">(</span><span class="s">&quot;Words preceded by negation term&quot;</span><span class="p">)</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="nf">ylab</span><span class="p">(</span><span class="s">&quot;Sentiment value * # of occurrences&quot;</span><span class="p">)</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="nf">coord_flip</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<a class="reference internal image-reference" href="../../../_images/e59458c1353f9a5e1bf3ae00e69e336d4a13da7e1ea611b7fc672ba61b88a904.png"><img alt="../../../_images/e59458c1353f9a5e1bf3ae00e69e336d4a13da7e1ea611b7fc672ba61b88a904.png" src="../../../_images/e59458c1353f9a5e1bf3ae00e69e336d4a13da7e1ea611b7fc672ba61b88a904.png" style="width: 420px; height: 420px;" />
</a>
</div>
</div>
</section>
<section id="visualizing-a-network-of-bigrams-with-ggraph">
<h4>Visualizing a Network of Bigrams with ggraph<a class="headerlink" href="#visualizing-a-network-of-bigrams-with-ggraph" title="Link to this heading">#</a></h4>
<p>We might want to visualize all the relationships between words at once, not just a few top pairs. A common way to do this is by creating a network—or “graph”—where words are nodes connected by edges.</p>
<p>Here, “graph” means a structure made of connected nodes, not just a plot. A graph can be built from tidy data that contains three key variables:</p>
<ul class="simple">
<li><p><strong>from:</strong> the starting node of an edge</p></li>
<li><p><strong>to:</strong> the ending node of an edge</p></li>
<li><p><strong>weight:</strong> a numeric value associated with the edge (like frequency)</p></li>
</ul>
<p>The <a class="reference external" href="https://igraph.org">igraph</a> package provides many tools for handling and analyzing such networks. To create an igraph object from tidy data, we use the <a class="reference external" href="https://r.igraph.org/reference/graph_from_data_frame.html"><code class="docutils literal notranslate"><span class="pre">graph_from_data_frame()</span></code></a> function, which takes a data frame of edges with columns for “from”, “to”, and any edge attributes (such as counts <code class="docutils literal notranslate"><span class="pre">n</span></code> in this case).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">library</span><span class="p">(</span><span class="n">igraph</span><span class="p">)</span>

<span class="c1"># original counts</span>
<span class="nf">print</span><span class="p">(</span><span class="n">bigram_counts</span><span class="p">)</span>


<span class="c1"># filter for only relatively common combinations</span>
<span class="n">bigram_graph</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">bigram_counts</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">filter</span><span class="p">(</span><span class="n">n</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="m">20</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">graph_from_data_frame</span><span class="p">()</span>

<span class="nf">print</span><span class="p">(</span><span class="n">bigram_graph</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span># A tibble: 28,971 x 3
   word1   word2         n
   &lt;chr&gt;   &lt;chr&gt;     &lt;int&gt;
 1 sir     thomas      266
 2 miss    crawford    196
 3 captain wentworth   143
 4 miss    woodhouse   143
 5 frank   churchill   114
 6 lady    russell     110
 7 sir     walter      108
 8 lady    bertram     101
 9 miss    fairfax      98
10 colonel brandon      96
# i 28,961 more rows
IGRAPH b3d14c8 DN-- 85 70 -- 
+ attr: name (v/c), n (e/n)
+ edges from b3d14c8 (vertex names):
 [1] sir     -&gt;thomas     miss    -&gt;crawford   captain -&gt;wentworth 
 [4] miss    -&gt;woodhouse  frank   -&gt;churchill  lady    -&gt;russell   
 [7] sir     -&gt;walter     lady    -&gt;bertram    miss    -&gt;fairfax   
[10] colonel -&gt;brandon    sir     -&gt;john       miss    -&gt;bates     
[13] jane    -&gt;fairfax    lady    -&gt;catherine  lady    -&gt;middleton 
[16] miss    -&gt;tilney     miss    -&gt;bingley    thousand-&gt;pounds    
[19] miss    -&gt;dashwood   dear    -&gt;miss       miss    -&gt;bennet    
[22] miss    -&gt;morland    captain -&gt;benwick    miss    -&gt;smith     
+ ... omitted several edges
</pre></div>
</div>
</div>
</div>
<p>While igraph includes some plotting functions, they aren’t its main focus, so many other packages have created better visualization tools for graph objects. We recommend the <strong>ggraph</strong> package <a class="reference external" href="https://www.tidytextmining.com/references#ref-R-ggraph">(Pedersen 2017)</a>, which builds graph visualizations using the grammar of graphics—just like <strong>ggplot2</strong>, which you’re already familiar with.</p>
<p>With <strong>ggraph</strong>, you start by converting your igraph object using the <code class="docutils literal notranslate"><span class="pre">ggraph()</span></code> function. Then you add layers just like in ggplot2. For a basic network plot, you typically add three layers:</p>
<ul class="simple">
<li><p><strong>nodes</strong> (the points or vertices),</p></li>
<li><p><strong>edges</strong> (the connections between nodes), and</p></li>
<li><p><strong>text</strong> (labels on the nodes).</p></li>
</ul>
<p>This approach makes graph visualization intuitive and flexible within the tidy data framework.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">library</span><span class="p">(</span><span class="n">ggraph</span><span class="p">)</span>

<span class="nf">set.seed</span><span class="p">(</span><span class="m">2017</span><span class="p">)</span>

<span class="nf">ggraph</span><span class="p">(</span><span class="n">bigram_graph</span><span class="p">,</span><span class="w"> </span><span class="n">layout</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;fr&quot;</span><span class="p">)</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="nf">geom_edge_link</span><span class="p">()</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="nf">geom_node_point</span><span class="p">()</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="nf">geom_node_text</span><span class="p">(</span><span class="nf">aes</span><span class="p">(</span><span class="n">label</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">name</span><span class="p">),</span><span class="w"> </span><span class="n">vjust</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1</span><span class="p">,</span><span class="w"> </span><span class="n">hjust</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<a class="reference internal image-reference" href="../../../_images/f918cbe4d5f5f8f29d47b6e1964ec2f3ed78a0653a552f5bfc82a3afcd880389.png"><img alt="../../../_images/f918cbe4d5f5f8f29d47b6e1964ec2f3ed78a0653a552f5bfc82a3afcd880389.png" src="../../../_images/f918cbe4d5f5f8f29d47b6e1964ec2f3ed78a0653a552f5bfc82a3afcd880389.png" style="width: 420px; height: 420px;" />
</a>
</div>
</div>
<p>The above visualization reveals key aspects of the text’s structure. Salutations like “miss,” “lady,” “sir,” and “colonel” appear as central nodes, frequently connected to names that follow them. Around the edges, we notice common short phrases made up of pairs or triplets of words, such as “half hour,” “thousand pounds,” and “short time/pause.” This network view helps highlight how certain words cluster together in meaningful patterns within the text.</p>
<p>We can also refine the graph’s appearance to enhance readability:</p>
<ul class="simple">
<li><p>The <code class="docutils literal notranslate"><span class="pre">edge\_alpha</span></code> aesthetic is added to the links, making connections more transparent when bigrams are less frequent and more opaque when they’re common.</p></li>
<li><p>Directionality is shown with arrows, created using <a class="reference external" href="https://rdrr.io/r/grid/arrow.html"><code class="docutils literal notranslate"><span class="pre">grid::arrow()</span></code></a>, with an <code class="docutils literal notranslate"><span class="pre">end\_cap</span></code> option to ensure arrows stop just before the nodes for clarity.</p></li>
<li><p>The nodes are styled to be more visually appealing—made larger and colored blue.</p></li>
<li><p>Finally, we apply <a class="reference external" href="https://ggplot2.tidyverse.org/reference/ggtheme.html"><code class="docutils literal notranslate"><span class="pre">theme\_void()</span></code></a> to remove background elements and axes, which is ideal for network visualizations.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">set.seed</span><span class="p">(</span><span class="m">2020</span><span class="p">)</span>

<span class="n">a</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">grid</span><span class="o">::</span><span class="nf">arrow</span><span class="p">(</span><span class="n">type</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;closed&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">length</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">unit</span><span class="p">(</span><span class="n">.</span><span class="m">15</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;inches&quot;</span><span class="p">))</span>

<span class="nf">ggraph</span><span class="p">(</span><span class="n">bigram_graph</span><span class="p">,</span><span class="w"> </span><span class="n">layout</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;fr&quot;</span><span class="p">)</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="nf">geom_edge_link</span><span class="p">(</span><span class="nf">aes</span><span class="p">(</span><span class="n">edge_alpha</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">n</span><span class="p">),</span><span class="w"> </span><span class="n">show.legend</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">FALSE</span><span class="p">,</span>
<span class="w">                 </span><span class="n">arrow</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">a</span><span class="p">,</span><span class="w"> </span><span class="n">end_cap</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">circle</span><span class="p">(</span><span class="n">.</span><span class="m">07</span><span class="p">,</span><span class="w"> </span><span class="s">&#39;inches&#39;</span><span class="p">))</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="nf">geom_node_point</span><span class="p">(</span><span class="n">color</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;lightblue&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">5</span><span class="p">)</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="nf">geom_node_text</span><span class="p">(</span><span class="nf">aes</span><span class="p">(</span><span class="n">label</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">name</span><span class="p">),</span><span class="w"> </span><span class="n">vjust</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1</span><span class="p">,</span><span class="w"> </span><span class="n">hjust</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1</span><span class="p">)</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="nf">theme_void</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<a class="reference internal image-reference" href="../../../_images/fb425ac2b3c5f8dbc4e9717b09d348a202fe127325556d6124a04f6e6abbb7e8.png"><img alt="../../../_images/fb425ac2b3c5f8dbc4e9717b09d348a202fe127325556d6124a04f6e6abbb7e8.png" src="../../../_images/fb425ac2b3c5f8dbc4e9717b09d348a202fe127325556d6124a04f6e6abbb7e8.png" style="width: 420px; height: 420px;" />
</a>
</div>
</div>
<p>Getting your networks to look polished with <strong>ggraph</strong> might take some trial and error, but this network structure offers a powerful and flexible way to visualize relationships in tidy data.</p>
<p>Keep in mind, this visualization represents a <strong>Markov chain</strong>, a common model in text analysis where each word depends only on the one before it. For example, a Markov chain-based text generator might start with “dear,” then move to “sir,” followed by “william,” “walter,” or “thomas,” by selecting the most frequent next word at each step.</p>
<p>To keep the graph readable, we only display the most common word-to-word links, but theoretically, you could build a massive graph showing every connection present in the text.</p>
</section>
<section id="visualizing-bigrams-in-other-texts">
<h4>Visualizing Bigrams in Other Texts<a class="headerlink" href="#visualizing-bigrams-in-other-texts" title="Link to this heading">#</a></h4>
<p>We put significant effort into cleaning and visualizing bigrams from a text dataset, so now let’s wrap that process into a function. This will allow us to easily apply the same bigram counting and visualization steps to other text datasets.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">library</span><span class="p">(</span><span class="n">dplyr</span><span class="p">)</span>
<span class="nf">library</span><span class="p">(</span><span class="n">tidyr</span><span class="p">)</span>
<span class="nf">library</span><span class="p">(</span><span class="n">tidytext</span><span class="p">)</span>
<span class="nf">library</span><span class="p">(</span><span class="n">ggplot2</span><span class="p">)</span>
<span class="nf">library</span><span class="p">(</span><span class="n">igraph</span><span class="p">)</span>
<span class="nf">library</span><span class="p">(</span><span class="n">ggraph</span><span class="p">)</span>

<span class="n">count_bigrams</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="kr">function</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">dataset</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">    </span><span class="nf">unnest_tokens</span><span class="p">(</span><span class="n">bigram</span><span class="p">,</span><span class="w"> </span><span class="n">text</span><span class="p">,</span><span class="w"> </span><span class="n">token</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;ngrams&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">n</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">2</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">    </span><span class="nf">separate</span><span class="p">(</span><span class="n">bigram</span><span class="p">,</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="s">&quot;word1&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;word2&quot;</span><span class="p">),</span><span class="w"> </span><span class="n">sep</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot; &quot;</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">    </span><span class="nf">filter</span><span class="p">(</span><span class="o">!</span><span class="n">word1</span><span class="w"> </span><span class="o">%in%</span><span class="w"> </span><span class="n">stop_words</span><span class="o">$</span><span class="n">word</span><span class="p">,</span>
<span class="w">           </span><span class="o">!</span><span class="n">word2</span><span class="w"> </span><span class="o">%in%</span><span class="w"> </span><span class="n">stop_words</span><span class="o">$</span><span class="n">word</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">    </span><span class="nf">count</span><span class="p">(</span><span class="n">word1</span><span class="p">,</span><span class="w"> </span><span class="n">word2</span><span class="p">,</span><span class="w"> </span><span class="n">sort</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">)</span>
<span class="p">}</span>

<span class="n">visualize_bigrams</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="kr">function</span><span class="p">(</span><span class="n">bigrams</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="nf">set.seed</span><span class="p">(</span><span class="m">2016</span><span class="p">)</span>
<span class="w">  </span><span class="n">a</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">grid</span><span class="o">::</span><span class="nf">arrow</span><span class="p">(</span><span class="n">type</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;closed&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">length</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">unit</span><span class="p">(</span><span class="n">.</span><span class="m">15</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;inches&quot;</span><span class="p">))</span>
<span class="w">  </span>
<span class="w">  </span><span class="n">bigrams</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">    </span><span class="nf">graph_from_data_frame</span><span class="p">()</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">    </span><span class="nf">ggraph</span><span class="p">(</span><span class="n">layout</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;fr&quot;</span><span class="p">)</span><span class="w"> </span><span class="o">+</span>
<span class="w">    </span><span class="nf">geom_edge_link</span><span class="p">(</span><span class="nf">aes</span><span class="p">(</span><span class="n">edge_alpha</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">n</span><span class="p">),</span><span class="w"> </span><span class="n">show.legend</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">FALSE</span><span class="p">,</span><span class="w"> </span><span class="n">arrow</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">a</span><span class="p">)</span><span class="w"> </span><span class="o">+</span>
<span class="w">    </span><span class="nf">geom_node_point</span><span class="p">(</span><span class="n">color</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;lightblue&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">5</span><span class="p">)</span><span class="w"> </span><span class="o">+</span>
<span class="w">    </span><span class="nf">geom_node_text</span><span class="p">(</span><span class="nf">aes</span><span class="p">(</span><span class="n">label</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">name</span><span class="p">),</span><span class="w"> </span><span class="n">vjust</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1</span><span class="p">,</span><span class="w"> </span><span class="n">hjust</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1</span><span class="p">)</span><span class="w"> </span><span class="o">+</span>
<span class="w">    </span><span class="nf">theme_void</span><span class="p">()</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
</div>
<p>Now, we can apply our bigram visualization to other texts, like the King James Version of the Bible:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">library</span><span class="p">(</span><span class="n">gutenbergr</span><span class="p">)</span>

<span class="c1"># the King James version is book 10 on Project Gutenberg:</span>
<span class="n">kjv</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">gutenberg_download</span><span class="p">(</span><span class="m">10</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">library</span><span class="p">(</span><span class="n">stringr</span><span class="p">)</span>

<span class="n">kjv_bigrams</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">kjv</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">count_bigrams</span><span class="p">()</span>

<span class="c1"># filter out rare combinations, as well as digits</span>
<span class="n">kjv_bigrams</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">filter</span><span class="p">(</span><span class="n">n</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="m">40</span><span class="p">,</span>
<span class="w">         </span><span class="o">!</span><span class="nf">str_detect</span><span class="p">(</span><span class="n">word1</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;\\d&quot;</span><span class="p">),</span>
<span class="w">         </span><span class="o">!</span><span class="nf">str_detect</span><span class="p">(</span><span class="n">word2</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;\\d&quot;</span><span class="p">))</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">visualize_bigrams</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<a class="reference internal image-reference" href="../../../_images/bc5843823bf42de5528948aa71288fdd3b6bffb8c95dd2303586af69e1182aa2.png"><img alt="../../../_images/bc5843823bf42de5528948aa71288fdd3b6bffb8c95dd2303586af69e1182aa2.png" src="../../../_images/bc5843823bf42de5528948aa71288fdd3b6bffb8c95dd2303586af69e1182aa2.png" style="width: 420px; height: 420px;" />
</a>
</div>
</div>
<p>This reveals a typical language pattern in the Bible, especially centered on “thy” and “thou” — words that might well be considered stopwords! Using the gutenbergr package alongside the <code class="docutils literal notranslate"><span class="pre">count\_bigrams</span></code> and <code class="docutils literal notranslate"><span class="pre">visualize\_bigrams</span></code> functions, you can explore and visualize bigrams in other classic texts that interest you.</p>
</section>
</section>
<section id="counting-and-correlating-pairs-of-words-with-the-widyr-package">
<h3>Counting and Correlating Pairs of Words with the widyr Package<a class="headerlink" href="#counting-and-correlating-pairs-of-words-with-the-widyr-package" title="Link to this heading">#</a></h3>
<p>Tokenizing by n-grams is a helpful approach for examining pairs of adjacent words, but we might also be curious about words that frequently appear together within a document or chapter, even if they aren’t positioned side by side.</p>
<p>While tidy data provides a convenient structure for grouping by rows or comparing variables, comparing between rows — such as counting how often two words appear within the same document or measuring how strongly they are correlated — can be more complex. Typically, to perform pairwise counts or calculate correlations, the data needs to be reshaped into a wide matrix format first.</p>
<p>We’ll explore specific ways to convert tidy text data into wide matrices later, but for this task, that step isn’t required. The <strong>widyr</strong> package simplifies these kinds of operations, allowing us to skip the manual process of widening, analyzing, and then returning to tidy format.</p>
<p>We’ll focus on widyr functions designed for pairwise comparisons — for example, comparing words across different documents or sections of text — making it easy to compute pairwise counts or correlations while keeping the workflow consistent with tidy principles.</p>
<section id="counting-and-correlating-among-sections">
<h4>Counting and Correlating Among Sections<a class="headerlink" href="#counting-and-correlating-among-sections" title="Link to this heading">#</a></h4>
<p>Let’s take the book <em>Pride and Prejudice</em> and divide it into sections of 10 lines, similar to how we grouped larger sections for sentiment analysis. This smaller chunk size allows us to analyze word patterns in more detail. Specifically, we can investigate which words frequently appear together within the same 10-line section, providing insight into word relationships beyond immediate adjacency.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">austen_section_words</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">austen_books</span><span class="p">()</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">filter</span><span class="p">(</span><span class="n">book</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="s">&quot;Pride &amp; Prejudice&quot;</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">mutate</span><span class="p">(</span><span class="n">section</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">row_number</span><span class="p">()</span><span class="w"> </span><span class="o">%/%</span><span class="w"> </span><span class="m">10</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">filter</span><span class="p">(</span><span class="n">section</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="m">0</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">unnest_tokens</span><span class="p">(</span><span class="n">word</span><span class="p">,</span><span class="w"> </span><span class="n">text</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">filter</span><span class="p">(</span><span class="o">!</span><span class="n">word</span><span class="w"> </span><span class="o">%in%</span><span class="w"> </span><span class="n">stop_words</span><span class="o">$</span><span class="n">word</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="n">austen_section_words</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span># A tibble: 37,240 x 3
   book              section word        
   &lt;fct&gt;               &lt;dbl&gt; &lt;chr&gt;       
 1 Pride &amp; Prejudice       1 truth       
 2 Pride &amp; Prejudice       1 universally 
 3 Pride &amp; Prejudice       1 acknowledged
 4 Pride &amp; Prejudice       1 single      
 5 Pride &amp; Prejudice       1 possession  
 6 Pride &amp; Prejudice       1 fortune     
 7 Pride &amp; Prejudice       1 wife        
 8 Pride &amp; Prejudice       1 feelings    
 9 Pride &amp; Prejudice       1 views       
10 Pride &amp; Prejudice       1 entering    
# i 37,230 more rows
</pre></div>
</div>
</div>
</div>
<p>The <a class="reference external" href="https://juliasilge.github.io/widyr/reference/pairwise_count.html"><code class="docutils literal notranslate"><span class="pre">pairwise_count()</span></code></a> function from the <strong>widyr</strong> package is particularly helpful for this kind of analysis. As the prefix <code class="docutils literal notranslate"><span class="pre">pairwise\_</span></code> suggests, the result will include one row for each unique pair of words from the <code class="docutils literal notranslate"><span class="pre">word</span></code> variable. Using this, we can count how often pairs of words appear together within the same section of text — in this case, within the same 10-line section of <em>Pride and Prejudice</em>. This approach gives us insight into word co-occurrence patterns beyond adjacent word pairs like bigrams.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">library</span><span class="p">(</span><span class="n">widyr</span><span class="p">)</span>

<span class="c1"># count words co-occuring within sections</span>
<span class="n">word_pairs</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">austen_section_words</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">pairwise_count</span><span class="p">(</span><span class="n">word</span><span class="p">,</span><span class="w"> </span><span class="n">section</span><span class="p">,</span><span class="w"> </span><span class="n">sort</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="n">word_pairs</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span># A tibble: 796,008 x 3
   item1     item2         n
   &lt;chr&gt;     &lt;chr&gt;     &lt;dbl&gt;
 1 darcy     elizabeth   144
 2 elizabeth darcy       144
 3 miss      elizabeth   110
 4 elizabeth miss        110
 5 elizabeth jane        106
 6 jane      elizabeth   106
 7 miss      darcy        92
 8 darcy     miss         92
 9 elizabeth bingley      91
10 bingley   elizabeth    91
# i 795,998 more rows
</pre></div>
</div>
</div>
</div>
<p>Here, the input data originally had one row per word per 10-line section of <em>Pride and Prejudice</em>. After using <a class="reference external" href="https://juliasilge.github.io/widyr/reference/pairwise_count.html"><code class="docutils literal notranslate"><span class="pre">pairwise_count()</span></code></a>, the output reshapes into a tidy format where each row represents a unique pair of words and the number of times they co-occur within the same section.</p>
<p>This structure enables new kinds of analysis, such as determining which words most frequently appear alongside “Darcy.” Using simple filtering and sorting, we can quickly extract the top co-occurring words, revealing patterns of association within the text — for instance, showing that “Elizabeth” appears most often with “Darcy,” reflecting their central roles in the novel.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">word_pairs</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">filter</span><span class="p">(</span><span class="n">item1</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="s">&quot;darcy&quot;</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">print</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span># A tibble: 2,930 x 3
   item1 item2         n
   &lt;chr&gt; &lt;chr&gt;     &lt;dbl&gt;
 1 darcy elizabeth   144
 2 darcy miss         92
 3 darcy bingley      86
 4 darcy jane         46
 5 darcy bennet       45
 6 darcy sister       45
 7 darcy time         41
 8 darcy lady         38
 9 darcy friend       37
10 darcy wickham      37
# i 2,920 more rows
</pre></div>
</div>
</div>
</div>
</section>
<section id="pairwise-correlation">
<h4>Pairwise Correlation<a class="headerlink" href="#pairwise-correlation" title="Link to this heading">#</a></h4>
<p>Pairs like “Elizabeth” and “Darcy” naturally co-occur frequently, but that alone isn’t very informative since both words are so common in the text individually. Instead of just counting, we can examine <em>correlation</em>, which captures how often two words appear together <em>relative</em> to how often they appear separately.</p>
<p>One way to measure this is the <a class="reference external" href="https://en.wikipedia.org/wiki/Phi_coefficient">phi coefficient</a>, a standard statistic for binary association. It reflects how much more likely it is that both words appear together (or both are absent) than one appears without the other.</p>
<p>The idea comes from a 2x2 contingency table:</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p></p></th>
<th class="head"><p>Has word Y</p></th>
<th class="head"><p>No word Y</p></th>
<th class="head"><p>Total</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Has word X</strong></p></td>
<td><p>n₁₁</p></td>
<td><p>n₁₀</p></td>
<td><p>n₁•</p></td>
</tr>
<tr class="row-odd"><td><p><strong>No word X</strong></p></td>
<td><p>n₀₁</p></td>
<td><p>n₀₀</p></td>
<td><p>n₀•</p></td>
</tr>
<tr class="row-even"><td><p><strong>Total</strong></p></td>
<td><p>n•₁</p></td>
<td><p>n•₀</p></td>
<td><p>n</p></td>
</tr>
</tbody>
</table>
</div>
<ul class="simple">
<li><p><strong>n₁₁</strong> = sections with both word X and word Y</p></li>
<li><p><strong>n₀₀</strong> = sections with neither word</p></li>
<li><p><strong>n₁₀</strong>, <strong>n₀₁</strong> = sections where only one word appears</p></li>
</ul>
<p>The phi coefficient is calculated as:</p>
<p><strong>ϕ = (n₁₁ * n₀₀ - n₁₀ * n₀₁) / √(n₁• * n₀• * n•₀ * n•₁)</strong></p>
<p>This measure is identical to the Pearson correlation when applied to binary (present/absent) data.</p>
<p>The <a class="reference external" href="https://juliasilge.github.io/widyr/reference/pairwise_cor.html"><code class="docutils literal notranslate"><span class="pre">pairwise_cor()</span></code></a> function from the <strong>widyr</strong> package computes this for pairs of words, based on how often they co-occur within the same section of text. Its usage is similar to <a class="reference external" href="https://juliasilge.github.io/widyr/reference/pairwise_count.html"><code class="docutils literal notranslate"><span class="pre">pairwise_count()</span></code></a>, but the output gives correlation values instead of raw counts.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1"># we need to filter for at least relatively common words first</span>
<span class="n">word_cors</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">austen_section_words</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">group_by</span><span class="p">(</span><span class="n">word</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">filter</span><span class="p">(</span><span class="nf">n</span><span class="p">()</span><span class="w"> </span><span class="o">&gt;=</span><span class="w"> </span><span class="m">20</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">pairwise_cor</span><span class="p">(</span><span class="n">word</span><span class="p">,</span><span class="w"> </span><span class="n">section</span><span class="p">,</span><span class="w"> </span><span class="n">sort</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="n">word_cors</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span># A tibble: 154,842 x 3
   item1     item2     correlation
   &lt;chr&gt;     &lt;chr&gt;           &lt;dbl&gt;
 1 bourgh    de              0.951
 2 de        bourgh          0.951
 3 pounds    thousand        0.701
 4 thousand  pounds          0.701
 5 william   sir             0.664
 6 sir       william         0.664
 7 catherine lady            0.663
 8 lady      catherine       0.663
 9 forster   colonel         0.622
10 colonel   forster         0.622
# i 154,832 more rows
</pre></div>
</div>
</div>
</div>
<p>This tidy output structure makes it easy to explore word relationships. For instance, you can quickly identify which words are most strongly correlated with a word like <strong>“pounds”</strong> by simply applying a <code class="docutils literal notranslate"><span class="pre">filter()</span></code> to select rows where one of the words is “pounds,” then sorting by the correlation strength. This allows you to discover which words frequently appear in the same sections as “pounds,” adjusted for how common each word is overall.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">word_cors</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">filter</span><span class="p">(</span><span class="n">item1</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="s">&quot;pounds&quot;</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">print</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span># A tibble: 393 x 3
   item1  item2     correlation
   &lt;chr&gt;  &lt;chr&gt;           &lt;dbl&gt;
 1 pounds thousand       0.701 
 2 pounds ten            0.231 
 3 pounds fortune        0.164 
 4 pounds settled        0.149 
 5 pounds wickham&#39;s      0.142 
 6 pounds children       0.129 
 7 pounds mother&#39;s       0.119 
 8 pounds believed       0.0932
 9 pounds estate         0.0890
10 pounds ready          0.0860
# i 383 more rows
</pre></div>
</div>
</div>
</div>
<p>This approach allows us to focus on specific words of interest and identify the other words that are most strongly connected or associated with them within the text.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">word_cors</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">filter</span><span class="p">(</span><span class="n">item1</span><span class="w"> </span><span class="o">%in%</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="s">&quot;elizabeth&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;pounds&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;married&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;pride&quot;</span><span class="p">))</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">group_by</span><span class="p">(</span><span class="n">item1</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">slice_max</span><span class="p">(</span><span class="n">correlation</span><span class="p">,</span><span class="w"> </span><span class="n">n</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">6</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">ungroup</span><span class="p">()</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">mutate</span><span class="p">(</span><span class="n">item2</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">reorder</span><span class="p">(</span><span class="n">item2</span><span class="p">,</span><span class="w"> </span><span class="n">correlation</span><span class="p">))</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">ggplot</span><span class="p">(</span><span class="nf">aes</span><span class="p">(</span><span class="n">item2</span><span class="p">,</span><span class="w"> </span><span class="n">correlation</span><span class="p">))</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="nf">geom_bar</span><span class="p">(</span><span class="n">stat</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;identity&quot;</span><span class="p">)</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="nf">facet_wrap</span><span class="p">(</span><span class="o">~</span><span class="w"> </span><span class="n">item1</span><span class="p">,</span><span class="w"> </span><span class="n">scales</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;free&quot;</span><span class="p">)</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="nf">coord_flip</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<a class="reference internal image-reference" href="../../../_images/0af4641e129c5d66f976bab3903a14992931274e2b1b0e431c9325d24169643b.png"><img alt="../../../_images/0af4641e129c5d66f976bab3903a14992931274e2b1b0e431c9325d24169643b.png" src="../../../_images/0af4641e129c5d66f976bab3903a14992931274e2b1b0e431c9325d24169643b.png" style="width: 420px; height: 420px;" />
</a>
</div>
</div>
<p>Similar to how we visualized bigrams using <strong>ggraph</strong>, we can also use it to plot the word correlations and clusters discovered through the <strong>widyr</strong> package. This enables us to clearly see groups of words that tend to appear together in the text.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">set.seed</span><span class="p">(</span><span class="m">2016</span><span class="p">)</span>

<span class="n">word_cors</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">filter</span><span class="p">(</span><span class="n">correlation</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="n">.</span><span class="m">15</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">graph_from_data_frame</span><span class="p">()</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">ggraph</span><span class="p">(</span><span class="n">layout</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;fr&quot;</span><span class="p">)</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="nf">geom_edge_link</span><span class="p">(</span><span class="nf">aes</span><span class="p">(</span><span class="n">edge_alpha</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">correlation</span><span class="p">),</span><span class="w"> </span><span class="n">show.legend</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">FALSE</span><span class="p">)</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="nf">geom_node_point</span><span class="p">(</span><span class="n">color</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;lightblue&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">5</span><span class="p">)</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="nf">geom_node_text</span><span class="p">(</span><span class="nf">aes</span><span class="p">(</span><span class="n">label</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">name</span><span class="p">),</span><span class="w"> </span><span class="n">repel</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">)</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="nf">theme_void</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<a class="reference internal image-reference" href="../../../_images/3655277a21290ad6ac5feab2eab79db3fc4c87550eefa3fc5bf4e9d66c0c9cb2.png"><img alt="../../../_images/3655277a21290ad6ac5feab2eab79db3fc4c87550eefa3fc5bf4e9d66c0c9cb2.png" src="../../../_images/3655277a21290ad6ac5feab2eab79db3fc4c87550eefa3fc5bf4e9d66c0c9cb2.png" style="width: 420px; height: 420px;" />
</a>
</div>
</div>
<p>Unlike the bigram analysis, the word relationships here are <strong>symmetrical</strong>, meaning there is no direction to the connections — so we don’t use arrows. While titles and names like <em>“colonel/fitzwilliam”</em> are still common pairings, this method also highlights other word associations that frequently appear near each other, such as <em>“walk” and “park”</em> or <em>“dance” and “ball”</em>.</p>
</section>
</section>
</section>
<section id="converting-to-and-from-non-tidy-formats">
<h2>Converting To and From Non-tidy Formats<a class="headerlink" href="#converting-to-and-from-non-tidy-formats" title="Link to this heading">#</a></h2>
<p>Above, we analyzed text arranged in the tidy text format: a table with one token per document per row, typically created with the <a class="reference external" href="https://juliasilge.github.io/tidytext/reference/unnest_tokens.html"><code class="docutils literal notranslate"><span class="pre">unnest_tokens()</span></code></a> function. This format allows us to use powerful tidy tools like <strong>dplyr</strong>, <strong>tidyr</strong>, and <strong>ggplot2</strong> for exploring and visualizing text data. We have shown that many insightful text analyses can be performed with these tools.</p>
<p>However, most existing R packages for natural language processing—other than <strong>tidytext</strong>—use different input structures and produce outputs that aren’t tidy. The <a class="reference external" href="https://cran.r-project.org/web/views/NaturalLanguageProcessing.html">CRAN Task View for Natural Language Processing</a> lists many packages that expect non-tidy data formats, which are still very useful for various text mining tasks.</p>
<p>Computer scientist Hal Abelson once said, “No matter how complex and polished the individual operations are, it is often the quality of the glue that most directly determines the power of the system” <a class="reference external" href="https://www.tidytextmining.com/references#ref-Friedman:2008:EPL:1378240">(Abelson 2008)</a>. In this spirit, this chapter focuses on the “glue” that links the tidy text format with other important packages and data structures, enabling you to combine the strengths of tidy tools with other widely used text mining resources.</p>
<section id="tidying-a-document-term-matrix">
<h3>Tidying a Document-term Matrix<a class="headerlink" href="#tidying-a-document-term-matrix" title="Link to this heading">#</a></h3>
<p>A <a class="reference external" href="https://en.wikipedia.org/wiki/Document-term_matrix">document-term matrix (DTM)</a> is one of the most common data structures used in text mining. In a DTM:</p>
<ul class="simple">
<li><p>Each row corresponds to a single document (like a book or article).</p></li>
<li><p>Each column corresponds to a specific term.</p></li>
<li><p>Each cell typically holds the count of how many times that term appears in that document.</p></li>
</ul>
<p>Because most document-term pairs do not appear (resulting in many zeros), DTMs are usually stored as sparse matrices. These sparse matrices behave like regular matrices (you can access rows and columns), but they are stored more efficiently. Below, we’ll explore various implementations of sparse matrices.</p>
<p>DTMs can’t be directly used with tidy tools, just as tidy data frames aren’t suitable input for most text mining packages. To bridge this gap, the <strong>tidytext</strong> package provides two key functions:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">tidy()</span></code>: Converts a document-term matrix into a tidy data frame. This function is borrowed from the <strong>broom</strong> package <a class="reference external" href="https://www.tidytextmining.com/references#ref-R-broom">(Robinson 2017)</a>, which tidies many types of statistical objects.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">cast()</span></code>: Converts a tidy data frame (one term per row) back into a matrix. There are three variations:</p>
<ul>
<li><p><a class="reference external" href="https://juliasilge.github.io/tidytext/reference/cast_sparse.html"><code class="docutils literal notranslate"><span class="pre">cast_sparse()</span></code></a> creates a sparse matrix from the <strong>Matrix</strong> package,</p></li>
<li><p><a class="reference external" href="https://juliasilge.github.io/tidytext/reference/document_term_casters.html">`cast_dtm()</a> creates a <strong>DocumentTermMatrix</strong> object from <strong>tm</strong>,</p></li>
<li><p><a class="reference external" href="https://juliasilge.github.io/tidytext/reference/document_term_casters.html"><code class="docutils literal notranslate"><span class="pre">cast_dfm()</span></code></a> creates a <strong>dfm</strong> object from <strong>quanteda</strong>.</p></li>
</ul>
</li>
</ul>
<p>A DTM corresponds closely to a tidy data frame created by a <code class="docutils literal notranslate"><span class="pre">count()</span></code> or a <code class="docutils literal notranslate"><span class="pre">group_by()</span></code>/<code class="docutils literal notranslate"><span class="pre">summarize()</span></code> operation that tabulates counts or other statistics for each term-document pair.</p>
<section id="tidying-documenttermmatrix-objects">
<h4>Tidying DocumentTermMatrix Objects<a class="headerlink" href="#tidying-documenttermmatrix-objects" title="Link to this heading">#</a></h4>
<p>One of the most popular implementations of document-term matrices (DTMs) in R is the <strong>DocumentTermMatrix</strong> class from the <strong>tm</strong> package. Many text mining datasets you’ll encounter come in this format. For example, the <strong>topicmodels</strong> package includes a collection of Associated Press newspaper articles stored as a DocumentTermMatrix.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">library</span><span class="p">(</span><span class="n">tm</span><span class="p">)</span>

<span class="nf">data</span><span class="p">(</span><span class="s">&quot;AssociatedPress&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">package</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;topicmodels&quot;</span><span class="p">)</span>
<span class="n">AssociatedPress</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading required package: NLP


Attaching package: &#39;NLP&#39;


The following object is masked from &#39;package:ggplot2&#39;:

    annotate
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;&lt;DocumentTermMatrix (documents: 2246, terms: 10473)&gt;&gt;
Non-/sparse entries: 302031/23220327
Sparsity           : 99%
Maximal term length: 18
Weighting          : term frequency (tf)
</pre></div>
</div>
</div>
</div>
<p>This dataset contains <strong>2,246 documents</strong>, each representing an Associated Press article, and <strong>10,473 distinct terms</strong> (unique words). The matrix is <strong>99% sparse</strong>, meaning that 99% of document-term combinations contain a zero—indicating the term does not appear in that document. You can view the set of terms using the <a class="reference external" href="https://rdrr.io/cran/tm/man/Docs.html"><code class="docutils literal notranslate"><span class="pre">Terms()</span></code></a> function.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">terms</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">Terms</span><span class="p">(</span><span class="n">AssociatedPress</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="nf">head</span><span class="p">(</span><span class="n">terms</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[1] &quot;aaron&quot;      &quot;abandon&quot;    &quot;abandoned&quot;  &quot;abandoning&quot; &quot;abbott&quot;    
[6] &quot;abboud&quot;    
</pre></div>
</div>
</div>
</div>
<p>To analyze this data using tidy tools, we first need to convert it into a tidy data frame format, with one token per document per row. The <code class="docutils literal notranslate"><span class="pre">broom</span></code> package provides the <a class="reference external" href="https://generics.r-lib.org/reference/tidy.html"><code class="docutils literal notranslate"><span class="pre">tidy()</span></code></a> function, which transforms non-tidy objects into tidy data frames. The <code class="docutils literal notranslate"><span class="pre">tidytext</span></code> package extends this functionality to handle <code class="docutils literal notranslate"><span class="pre">DocumentTermMatrix</span></code> objects specifically, allowing us to easily restructure the matrix for use with the tidyverse.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">ap_td</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">tidy</span><span class="p">(</span><span class="n">AssociatedPress</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">ap_td</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span># A tibble: 302,031 x 3
   document term       count
      &lt;int&gt; &lt;chr&gt;      &lt;dbl&gt;
 1        1 adding         1
 2        1 adult          2
 3        1 ago            1
 4        1 alcohol        1
 5        1 allegedly      1
 6        1 allen          1
 7        1 apparently     2
 8        1 appeared       1
 9        1 arrested       1
10        1 assault        1
# i 302,021 more rows
</pre></div>
</div>
</div>
</div>
<p>We now have a tidy <code class="docutils literal notranslate"><span class="pre">tbl_df</span></code> with three columns: <code class="docutils literal notranslate"><span class="pre">document</span></code>, <code class="docutils literal notranslate"><span class="pre">term</span></code>, and <code class="docutils literal notranslate"><span class="pre">count</span></code>. This transformation is similar to using <a class="reference external" href="https://rdrr.io/pkg/reshape2/man/melt.html"><code class="docutils literal notranslate"><span class="pre">melt()</span></code></a> from the <code class="docutils literal notranslate"><span class="pre">reshape2</span></code> package <a class="reference external" href="https://www.tidytextmining.com/references#ref-R-reshape2">(Wickham 2007)</a> for standard (non-sparse) matrices.</p>
<p>Importantly, only non-zero values appear in the tidy output — for instance, document 1 contains terms like “adding” and “adult,” but terms such as “aaron” or “abandon” are excluded because they don’t occur in that document. As with earlier examples, this tidy structure works well for analysis using <code class="docutils literal notranslate"><span class="pre">dplyr</span></code>, <code class="docutils literal notranslate"><span class="pre">tidytext</span></code>, and <code class="docutils literal notranslate"><span class="pre">ggplot2</span></code>. You could now apply familiar techniques like sentiment analysis to these newspaper articles.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">ap_sentiments</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">ap_td</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">inner_join</span><span class="p">(</span><span class="nf">get_sentiments</span><span class="p">(</span><span class="s">&quot;bing&quot;</span><span class="p">),</span><span class="w"> </span><span class="n">by</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="n">term</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;word&quot;</span><span class="p">))</span>

<span class="nf">print</span><span class="p">(</span><span class="n">ap_sentiments</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span># A tibble: 30,094 x 4
   document term    count sentiment
      &lt;int&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt;    
 1        1 assault     1 negative 
 2        1 complex     1 negative 
 3        1 death       1 negative 
 4        1 died        1 negative 
 5        1 good        2 positive 
 6        1 illness     1 negative 
 7        1 killed      2 negative 
 8        1 like        2 positive 
 9        1 liked       1 positive 
10        1 miracle     1 positive 
# i 30,084 more rows
</pre></div>
</div>
</div>
</div>
<p>This approach allows us to visualize which words from the AP articles contributed most frequently to positive or negative sentiment. Common positive words include “like,” “work,” “support,” and “good,” while words like “killed,” “death,” and “vice” appear most often on the negative side. It’s worth noting that the word “vice” being flagged as negative is probably a misclassification by the algorithm — in most cases, it likely refers to “vice president” rather than carrying a negative connotation.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">library</span><span class="p">(</span><span class="n">ggplot2</span><span class="p">)</span>

<span class="n">ap_sentiments</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">count</span><span class="p">(</span><span class="n">sentiment</span><span class="p">,</span><span class="w"> </span><span class="n">term</span><span class="p">,</span><span class="w"> </span><span class="n">wt</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">count</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">filter</span><span class="p">(</span><span class="n">n</span><span class="w"> </span><span class="o">&gt;=</span><span class="w"> </span><span class="m">200</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">mutate</span><span class="p">(</span><span class="n">n</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">ifelse</span><span class="p">(</span><span class="n">sentiment</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="s">&quot;negative&quot;</span><span class="p">,</span><span class="w"> </span><span class="o">-</span><span class="n">n</span><span class="p">,</span><span class="w"> </span><span class="n">n</span><span class="p">))</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">mutate</span><span class="p">(</span><span class="n">term</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">reorder</span><span class="p">(</span><span class="n">term</span><span class="p">,</span><span class="w"> </span><span class="n">n</span><span class="p">))</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">ggplot</span><span class="p">(</span><span class="nf">aes</span><span class="p">(</span><span class="n">n</span><span class="p">,</span><span class="w"> </span><span class="n">term</span><span class="p">,</span><span class="w"> </span><span class="n">fill</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">sentiment</span><span class="p">))</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="nf">geom_col</span><span class="p">()</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="nf">labs</span><span class="p">(</span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;Contribution to sentiment&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">NULL</span><span class="p">)</span><span class="w"> </span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<a class="reference internal image-reference" href="../../../_images/79faf97138f665ad3d79c8a6a644a649e2d6bc9e0d746e06bd0ebd881974edc8.png"><img alt="../../../_images/79faf97138f665ad3d79c8a6a644a649e2d6bc9e0d746e06bd0ebd881974edc8.png" src="../../../_images/79faf97138f665ad3d79c8a6a644a649e2d6bc9e0d746e06bd0ebd881974edc8.png" style="width: 420px; height: 420px;" />
</a>
</div>
</div>
</section>
<section id="tidying-dfm-objects">
<h4>Tidying dfm Objects<a class="headerlink" href="#tidying-dfm-objects" title="Link to this heading">#</a></h4>
<p>Other text mining packages offer different formats for document-term matrices, such as the <em>dfm</em> (document-feature matrix) from the <strong>quanteda</strong> package <a class="reference external" href="https://www.tidytextmining.com/references#ref-R-quanteda">(Benoit and Nulty, 2016)</a>. For instance, <strong>quanteda</strong> includes a built-in corpus of U.S. presidential inauguration speeches, which can be easily transformed into a dfm using the package’s functions.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">data</span><span class="p">(</span><span class="s">&quot;data_corpus_inaugural&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">package</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;quanteda&quot;</span><span class="p">)</span>
<span class="n">inaug_dfm</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">data_corpus_inaugural</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="n">quanteda</span><span class="o">::</span><span class="nf">tokens</span><span class="p">()</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="n">quanteda</span><span class="o">::</span><span class="nf">dfm</span><span class="p">(</span><span class="n">verbose</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">FALSE</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">inaug_dfm</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Warning message in (function (n) :
&quot;strings not representable in native encoding will be translated to UTF-8&quot;
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Document-feature matrix of: 60 documents, 9,437 features (91.84% sparse) and 4 docvars.
                 features
docs              fellow-citizens  of the senate and house representatives :
  1789-Washington               1  71 116      1  48     2               2 1
  1793-Washington               0  11  13      0   2     0               0 1
  1797-Adams                    3 140 163      1 130     0               2 0
  1801-Jefferson                2 104 130      0  81     0               0 1
  1805-Jefferson                0 101 143      0  93     0               0 0
  1809-Madison                  1  69 104      0  43     0               0 0
                 features
docs              among vicissitudes
  1789-Washington     1            1
  1793-Washington     0            0
  1797-Adams          4            0
  1801-Jefferson      1            0
  1805-Jefferson      7            0
  1809-Madison        0            0
[ reached max_ndoc ... 54 more documents, reached max_nfeat ... 9,427 more features ]
</pre></div>
</div>
</div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">tidy()</span></code> function also works on these document-feature matrices, converting them into a tidy table where each row represents one token for each document.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">inaug_td</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">tidy</span><span class="p">(</span><span class="n">inaug_dfm</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">inaug_td</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span># A tibble: 46,196 x 3
   document        term            count
   &lt;chr&gt;           &lt;chr&gt;           &lt;dbl&gt;
 1 1789-Washington fellow-citizens     1
 2 1797-Adams      fellow-citizens     3
 3 1801-Jefferson  fellow-citizens     2
 4 1809-Madison    fellow-citizens     1
 5 1813-Madison    fellow-citizens     1
 6 1817-Monroe     fellow-citizens     5
 7 1821-Monroe     fellow-citizens     1
 8 1841-Harrison   fellow-citizens    11
 9 1845-Polk       fellow-citizens     1
10 1849-Taylor     fellow-citizens     1
# i 46,186 more rows
</pre></div>
</div>
</div>
</div>
<p>To identify the words most specific to each inaugural speech, we can calculate the tf-idf for each term-speech pair using the <a class="reference external" href="https://juliasilge.github.io/tidytext/reference/bind_tf_idf.html"><code class="docutils literal notranslate"><span class="pre">bind_tf_idf()</span></code></a> function.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">inaug_tf_idf</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">inaug_td</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">bind_tf_idf</span><span class="p">(</span><span class="n">term</span><span class="p">,</span><span class="w"> </span><span class="n">document</span><span class="p">,</span><span class="w"> </span><span class="n">count</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">arrange</span><span class="p">(</span><span class="nf">desc</span><span class="p">(</span><span class="n">tf_idf</span><span class="p">))</span>

<span class="nf">print</span><span class="p">(</span><span class="n">inaug_tf_idf</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span># A tibble: 46,196 x 6
   document        term        count      tf   idf tf_idf
   &lt;chr&gt;           &lt;chr&gt;       &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;
 1 1793-Washington arrive          1 0.00680  4.09 0.0279
 2 1793-Washington upbraidings     1 0.00680  4.09 0.0279
 3 1793-Washington violated        1 0.00680  3.40 0.0231
 4 1793-Washington willingly       1 0.00680  3.40 0.0231
 5 1793-Washington incurring       1 0.00680  3.40 0.0231
 6 1793-Washington previous        1 0.00680  3.00 0.0204
 7 1793-Washington knowingly       1 0.00680  3.00 0.0204
 8 1793-Washington injunctions     1 0.00680  3.00 0.0204
 9 1793-Washington witnesses       1 0.00680  3.00 0.0204
10 1793-Washington besides         1 0.00680  2.71 0.0184
# i 46,186 more rows
</pre></div>
</div>
</div>
</div>
<p>This data allows us to select four significant inaugural speeches—by Presidents Lincoln, Roosevelt, Kennedy, and Obama—and visualize the words with the highest tf-idf scores for each, highlighting the terms most characteristic of each speech.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">speeches</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="s">&quot;1933-Roosevelt&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;1861-Lincoln&quot;</span><span class="p">,</span>
<span class="w">              </span><span class="s">&quot;1961-Kennedy&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;2009-Obama&quot;</span><span class="p">)</span>

<span class="n">inaug_tf_idf</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">filter</span><span class="p">(</span><span class="n">document</span><span class="w"> </span><span class="o">%in%</span><span class="w"> </span><span class="n">speeches</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">group_by</span><span class="p">(</span><span class="n">document</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">slice_max</span><span class="p">(</span><span class="n">tf_idf</span><span class="p">,</span><span class="w"> </span><span class="n">n</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">10</span><span class="p">,</span><span class="w"> </span><span class="n">with_ties</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">FALSE</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">ungroup</span><span class="p">()</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">mutate</span><span class="p">(</span><span class="n">term</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">reorder_within</span><span class="p">(</span><span class="n">term</span><span class="p">,</span><span class="w"> </span><span class="n">tf_idf</span><span class="p">,</span><span class="w"> </span><span class="n">document</span><span class="p">))</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">ggplot</span><span class="p">(</span><span class="nf">aes</span><span class="p">(</span><span class="n">term</span><span class="p">,</span><span class="w"> </span><span class="n">tf_idf</span><span class="p">,</span><span class="w"> </span><span class="n">fill</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">document</span><span class="p">))</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="nf">geom_col</span><span class="p">(</span><span class="n">show.legend</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">FALSE</span><span class="p">)</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="nf">facet_wrap</span><span class="p">(</span><span class="o">~</span><span class="w"> </span><span class="n">document</span><span class="p">,</span><span class="w"> </span><span class="n">scales</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;free&quot;</span><span class="p">)</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="nf">coord_flip</span><span class="p">()</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="nf">scale_x_reordered</span><span class="p">()</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="nf">labs</span><span class="p">(</span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">NULL</span><span class="p">,</span>
<span class="w">       </span><span class="n">y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;tf-idf&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<a class="reference internal image-reference" href="../../../_images/c82dbe8b7639f6b199c6777f428ba217ff033ef4962babc9f50ce66e9b5a64e4.png"><img alt="../../../_images/c82dbe8b7639f6b199c6777f428ba217ff033ef4962babc9f50ce66e9b5a64e4.png" src="../../../_images/c82dbe8b7639f6b199c6777f428ba217ff033ef4962babc9f50ce66e9b5a64e4.png" style="width: 420px; height: 420px;" />
</a>
</div>
</div>
<p>Here’s another example of a visualization enabled by tidy data: we can extract the year from each document’s title and calculate the total word count for each year.</p>
<p>Note that we use tidyr’s <a class="reference external" href="https://tidyr.tidyverse.org/reference/complete.html"><code class="docutils literal notranslate"><span class="pre">complete()</span></code></a> function to fill in zeros for any years where a particular word does not appear, ensuring the table is complete.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">library</span><span class="p">(</span><span class="n">tidyr</span><span class="p">)</span>

<span class="n">year_term_counts</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">inaug_td</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">extract</span><span class="p">(</span><span class="n">document</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;year&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;(\\d+)&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">convert</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">complete</span><span class="p">(</span><span class="n">year</span><span class="p">,</span><span class="w"> </span><span class="n">term</span><span class="p">,</span><span class="w"> </span><span class="n">fill</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">list</span><span class="p">(</span><span class="n">count</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0</span><span class="p">))</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">group_by</span><span class="p">(</span><span class="n">year</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">mutate</span><span class="p">(</span><span class="n">year_total</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">sum</span><span class="p">(</span><span class="n">count</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<p>This allows us to select several words and visualize how their frequencies have changed over time. We observe that, over time, American presidents increasingly favored the term “America” over “Union.” Additionally, references to the “constitution” and “foreign” countries declined, while mentions of “freedom” and “God” became more common.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">year_term_counts</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">filter</span><span class="p">(</span><span class="n">term</span><span class="w"> </span><span class="o">%in%</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="s">&quot;god&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;america&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;foreign&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;union&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;constitution&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;freedom&quot;</span><span class="p">))</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">ggplot</span><span class="p">(</span><span class="nf">aes</span><span class="p">(</span><span class="n">year</span><span class="p">,</span><span class="w"> </span><span class="n">count</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">year_total</span><span class="p">))</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="nf">geom_point</span><span class="p">()</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="nf">geom_smooth</span><span class="p">()</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="nf">facet_wrap</span><span class="p">(</span><span class="o">~</span><span class="w"> </span><span class="n">term</span><span class="p">,</span><span class="w"> </span><span class="n">scales</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;free_y&quot;</span><span class="p">)</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="nf">scale_y_continuous</span><span class="p">(</span><span class="n">labels</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">scales</span><span class="o">::</span><span class="nf">percent_format</span><span class="p">())</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="nf">labs</span><span class="p">(</span><span class="n">y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;% frequency of word in inaugural address&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>`geom_smooth()` using method = &#39;loess&#39; and formula = &#39;y ~ x&#39;
</pre></div>
</div>
<a class="reference internal image-reference" href="../../../_images/64ac9ae0553dce5f13a135e4307f97d58732b358cde374bad8c355eee1e188ee.png"><img alt="../../../_images/64ac9ae0553dce5f13a135e4307f97d58732b358cde374bad8c355eee1e188ee.png" src="../../../_images/64ac9ae0553dce5f13a135e4307f97d58732b358cde374bad8c355eee1e188ee.png" style="width: 420px; height: 420px;" />
</a>
</div>
</div>
<p>These examples demonstrate how tidytext, along with its ecosystem of tidy tools, enables you to analyze texts effectively—even when the original data isn’t in a tidy format.</p>
</section>
</section>
<section id="casting-tidy-text-data-into-a-matrix">
<h3>Casting Tidy Text Data Into a Matrix<a class="headerlink" href="#casting-tidy-text-data-into-a-matrix" title="Link to this heading">#</a></h3>
<p>Just as some text mining packages offer document-term matrices as sample data or output, certain algorithms require these matrices as input. To accommodate this, tidytext provides <code class="docutils literal notranslate"><span class="pre">cast\_</span></code> functions that convert tidy data back into such matrix formats.</p>
<p>For instance, we can convert the tidied AP dataset back into a document-term matrix using the <a class="reference external" href="https://juliasilge.github.io/tidytext/reference/document_term_casters.html"><code class="docutils literal notranslate"><span class="pre">cast\_dtm()</span></code></a> function.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">ap_td</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">cast_dtm</span><span class="p">(</span><span class="n">document</span><span class="p">,</span><span class="w"> </span><span class="n">term</span><span class="p">,</span><span class="w"> </span><span class="n">count</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;&lt;DocumentTermMatrix (documents: 2246, terms: 10473)&gt;&gt;
Non-/sparse entries: 302031/23220327
Sparsity           : 99%
Maximal term length: 18
Weighting          : term frequency (tf)
</pre></div>
</div>
</div>
</div>
<p>Similarly, we can convert the table into a dfm object from the quanteda package using the <a class="reference external" href="https://juliasilge.github.io/tidytext/reference/document_term_casters.html"><code class="docutils literal notranslate"><span class="pre">cast\_dfm()</span></code></a> function.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">ap_td</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">cast_dfm</span><span class="p">(</span><span class="n">document</span><span class="p">,</span><span class="w"> </span><span class="n">term</span><span class="p">,</span><span class="w"> </span><span class="n">count</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Document-feature matrix of: 2,246 documents, 10,473 features (98.72% sparse) and 0 docvars.
    features
docs adding adult ago alcohol allegedly allen apparently appeared arrested
   1      1     2   1       1         1     1          2        1        1
   2      0     0   0       0         0     0          0        1        0
   3      0     0   1       0         0     0          0        1        0
   4      0     0   3       0         0     0          0        0        0
   5      0     0   0       0         0     0          0        0        0
   6      0     0   2       0         0     0          0        0        0
    features
docs assault
   1       1
   2       0
   3       0
   4       0
   5       0
   6       0
[ reached max_ndoc ... 2,240 more documents, reached max_nfeat ... 10,463 more features ]
</pre></div>
</div>
</div>
</div>
<p>Some tools simply require a sparse matrix:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">library</span><span class="p">(</span><span class="n">Matrix</span><span class="p">)</span>

<span class="c1"># cast into a Matrix object</span>
<span class="n">m</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">ap_td</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">cast_sparse</span><span class="p">(</span><span class="n">document</span><span class="p">,</span><span class="w"> </span><span class="n">term</span><span class="p">,</span><span class="w"> </span><span class="n">count</span><span class="p">)</span>

<span class="nf">class</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>

<span class="nf">dim</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html">'dgCMatrix'</div><div class="output text_html"><style>
.list-inline {list-style: none; margin:0; padding: 0}
.list-inline>li {display: inline-block}
.list-inline>li:not(:last-child)::after {content: "\00b7"; padding: 0 .5ex}
</style>
<ol class=list-inline><li>2246</li><li>10473</li></ol>
</div></div>
</div>
<p>This type of conversion can be easily performed on any tidy text data we’ve worked with throughout this book. For instance, creating a document-term matrix (DTM) of Jane Austen’s novels can be done in just a few lines of code.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">library</span><span class="p">(</span><span class="n">janeaustenr</span><span class="p">)</span>

<span class="n">austen_dtm</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">austen_books</span><span class="p">()</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">unnest_tokens</span><span class="p">(</span><span class="n">word</span><span class="p">,</span><span class="w"> </span><span class="n">text</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">count</span><span class="p">(</span><span class="n">book</span><span class="p">,</span><span class="w"> </span><span class="n">word</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">cast_dtm</span><span class="p">(</span><span class="n">book</span><span class="p">,</span><span class="w"> </span><span class="n">word</span><span class="p">,</span><span class="w"> </span><span class="n">n</span><span class="p">)</span>

<span class="n">austen_dtm</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;&lt;DocumentTermMatrix (documents: 6, terms: 14516)&gt;&gt;
Non-/sparse entries: 40378/46718
Sparsity           : 54%
Maximal term length: 19
Weighting          : term frequency (tf)
</pre></div>
</div>
</div>
</div>
<p>This casting process enables you to perform reading, filtering, and manipulation with dplyr and other tidy tools, and then convert the resulting data into a document-term matrix suitable for machine learning tasks. In Chapter 6, we will explore examples where tidy-text datasets are transformed into DocumentTermMatrix objects for further analysis and modeling.</p>
</section>
<section id="tidying-corpus-objects-with-metadata">
<h3>Tidying Corpus Objects with Metadata<a class="headerlink" href="#tidying-corpus-objects-with-metadata" title="Link to this heading">#</a></h3>
<p>Some data structures are specifically designed to hold collections of documents prior to tokenization and are often referred to as a “corpus.” A common example in R is the <strong>Corpus</strong> object from the <strong>tm</strong> package. These objects store the raw text along with associated metadata such as document IDs, timestamps, titles, or language information for each document.</p>
<p>For instance, the <strong>tm</strong> package includes the <strong>acq</strong> corpus, which contains 50 news articles from the Reuters news service.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">data</span><span class="p">(</span><span class="s">&quot;acq&quot;</span><span class="p">)</span>
<span class="n">acq</span>

<span class="c1"># first document</span>
<span class="n">acq</span><span class="p">[[</span><span class="m">1</span><span class="p">]]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;&lt;VCorpus&gt;&gt;
Metadata:  corpus specific: 0, document level (indexed): 0
Content:  documents: 50
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;&lt;PlainTextDocument&gt;&gt;
Metadata:  15
Content:  chars: 1287
</pre></div>
</div>
</div>
</div>
<p>A corpus object is organized like a list, where each element holds both the text and its metadata (for more details, see the tm package documentation on working with Corpus objects). While this structure is flexible for storing documents, it is not directly compatible with tidy tools for analysis.</p>
<p>To bridge this gap, we can apply the <a class="reference external" href="https://generics.r-lib.org/reference/tidy.html"><code class="docutils literal notranslate"><span class="pre">tidy()</span></code></a> method, which converts the corpus into a table format with one row per document. This table includes metadata columns—such as document ID and timestamp—alongside the corresponding text, making it much easier to work with using tidyverse tools.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">acq_td</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">tidy</span><span class="p">(</span><span class="n">acq</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">acq_td</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span># A tibble: 50 x 16
   author   datetimestamp       description heading id    language origin topics
   &lt;chr&gt;    &lt;dttm&gt;              &lt;chr&gt;       &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;    &lt;chr&gt;  &lt;chr&gt; 
 1 <span class=" -Color -Color-Red">NA</span>       1987-02-26 15:18:06 &quot;&quot;          COMPUT~ 10    en       Reute~ YES   
 2 <span class=" -Color -Color-Red">NA</span>       1987-02-26 15:19:15 &quot;&quot;          OHIO M~ 12    en       Reute~ YES   
 3 <span class=" -Color -Color-Red">NA</span>       1987-02-26 15:49:56 &quot;&quot;          MCLEAN~ 44    en       Reute~ YES   
 4 By Cal ~ 1987-02-26 15:51:17 &quot;&quot;          CHEMLA~ 45    en       Reute~ YES   
 5 <span class=" -Color -Color-Red">NA</span>       1987-02-26 16:08:33 &quot;&quot;          &lt;COFAB~ 68    en       Reute~ YES   
 6 <span class=" -Color -Color-Red">NA</span>       1987-02-26 16:32:37 &quot;&quot;          INVEST~ 96    en       Reute~ YES   
 7 By Patt~ 1987-02-26 16:43:13 &quot;&quot;          AMERIC~ 110   en       Reute~ YES   
 8 <span class=" -Color -Color-Red">NA</span>       1987-02-26 16:59:25 &quot;&quot;          HONG K~ 125   en       Reute~ YES   
 9 <span class=" -Color -Color-Red">NA</span>       1987-02-26 17:01:28 &quot;&quot;          LIEBER~ 128   en       Reute~ YES   
10 <span class=" -Color -Color-Red">NA</span>       1987-02-26 17:08:27 &quot;&quot;          GULF A~ 134   en       Reute~ YES   
# i 40 more rows
# i 8 more variables: lewissplit &lt;chr&gt;, cgisplit &lt;chr&gt;, oldid &lt;chr&gt;,
#   places &lt;named list&gt;, people &lt;lgl&gt;, orgs &lt;lgl&gt;, exchanges &lt;lgl&gt;, text &lt;chr&gt;
</pre></div>
</div>
</div>
</div>
<p>Once converted into this tidy table format, you can apply <a class="reference external" href="https://juliasilge.github.io/tidytext/reference/unnest_tokens.html"><code class="docutils literal notranslate"><span class="pre">unnest_tokens()</span></code></a> to break the text into individual tokens (like words). This allows you to perform analyses such as identifying the most frequent words across all 50 Reuters articles or finding words that are particularly distinctive to each article.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">acq_tokens</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">acq_td</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">select</span><span class="p">(</span><span class="o">-</span><span class="n">places</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">unnest_tokens</span><span class="p">(</span><span class="n">word</span><span class="p">,</span><span class="w"> </span><span class="n">text</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">anti_join</span><span class="p">(</span><span class="n">stop_words</span><span class="p">,</span><span class="w"> </span><span class="n">by</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;word&quot;</span><span class="p">)</span>

<span class="c1"># most common words</span>
<span class="n">acq_tokens</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">count</span><span class="p">(</span><span class="n">word</span><span class="p">,</span><span class="w"> </span><span class="n">sort</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">print</span><span class="p">()</span>

<span class="c1"># tf-idf</span>
<span class="n">acq_tokens</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">count</span><span class="p">(</span><span class="n">id</span><span class="p">,</span><span class="w"> </span><span class="n">word</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">bind_tf_idf</span><span class="p">(</span><span class="n">word</span><span class="p">,</span><span class="w"> </span><span class="n">id</span><span class="p">,</span><span class="w"> </span><span class="n">n</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">arrange</span><span class="p">(</span><span class="nf">desc</span><span class="p">(</span><span class="n">tf_idf</span><span class="p">))</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">print</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span># A tibble: 1,559 x 2
   word         n
   &lt;chr&gt;    &lt;int&gt;
 1 dlrs       100
 2 pct         70
 3 mln         65
 4 company     63
 5 shares      52
 6 reuter      50
 7 stock       46
 8 offer       34
 9 share       34
10 american    28
# i 1,549 more rows
# A tibble: 2,833 x 6
   id    word         n     tf   idf tf_idf
   &lt;chr&gt; &lt;chr&gt;    &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;
 1 186   groupe       2 0.133   3.91  0.522
 2 128   liebert      3 0.130   3.91  0.510
 3 474   esselte      5 0.114   3.91  0.445
 4 371   burdett      6 0.103   3.91  0.405
 5 442   hazleton     4 0.103   3.91  0.401
 6 199   circuit      5 0.102   3.91  0.399
 7 441   rmj          8 0.123   3.22  0.396
 8 162   suffield     2 0.1     3.91  0.391
 9 498   west         3 0.1     3.91  0.391
10 467   nursery      3 0.0968  3.91  0.379
# i 2,823 more rows
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="topic-modeling">
<h2>Topic Modeling<a class="headerlink" href="#topic-modeling" title="Link to this heading">#</a></h2>
<p>In text mining, we often work with collections of documents, like news articles or blog posts, that we want to separate into meaningful groups to better analyze them. Topic modeling provides a way to automatically classify these documents without supervision, much like clustering with numerical data, helping reveal natural groupings even if we don’t know what to expect.</p>
<p>Latent Dirichlet Allocation (LDA) is one of the most widely used techniques for building a topic model. It represents each document as a blend of multiple topics and each topic as a mixture of words. This approach allows documents to share content across topics, rather than being divided into distinct, rigid categories, reflecting how language typically functions.</p>
<p>Tidy text principles let us apply the same tidy tools we’ve used throughout this book to topic modeling tasks. In this chapter, we’ll focus on working with LDA models from the <a class="reference external" href="https://cran.r-project.org/web/packages/topicmodels/index.html">topicmodels package</a>, particularly tidying them so they integrate smoothly with ggplot2 and dplyr. We’ll also walk through an example of clustering chapters from different books, where we’ll see how a topic model can “learn” to distinguish among the four books based on their textual content.</p>
<section id="latent-dirichlet-allocation">
<h3>Latent Dirichlet Allocation<a class="headerlink" href="#latent-dirichlet-allocation" title="Link to this heading">#</a></h3>
<p>Latent Dirichlet Allocation (LDA) is one of the most widely used algorithms for topic modeling. Without getting into the mathematical details, we can understand LDA through two core ideas.</p>
<p>Each document is made up of a combination of topics. We assume that every document contains words from several topics, each appearing in specific proportions. For instance, with a two-topic model, we might say, “Document 1 consists of 90% Topic A and 10% Topic B, while Document 2 contains 30% Topic A and 70% Topic B.”</p>
<p>Each topic is made up of a mixture of words. For example, imagine a two-topic model focused on U.S. news, with one topic representing “politics” and another representing “entertainment.” The politics topic might include words like “President,” “Congress,” and “government,” while the entertainment topic might include terms such as “movies,” “television,” and “actor.” It’s also important to note that some words can appear in both topics; for example, the word “budget” might be relevant to both.</p>
<p>LDA is a statistical approach that estimates these relationships simultaneously: determining the mixture of words associated with each topic, while also identifying the mixture of topics present within each document. Several software implementations of this technique exist, and we’ll take a closer look at one of them.</p>
<p>Above, we briefly discussed the <code class="docutils literal notranslate"><span class="pre">AssociatedPress</span></code> dataset included with the topicmodels package, which serves as an example of a Document-Term Matrix. This dataset contains 2,246 news articles from an American news organization, mostly dating from around 1988.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">library</span><span class="p">(</span><span class="n">topicmodels</span><span class="p">)</span>

<span class="nf">data</span><span class="p">(</span><span class="s">&quot;AssociatedPress&quot;</span><span class="p">)</span>
<span class="n">AssociatedPress</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;&lt;DocumentTermMatrix (documents: 2246, terms: 10473)&gt;&gt;
Non-/sparse entries: 302031/23220327
Sparsity           : 99%
Maximal term length: 18
Weighting          : term frequency (tf)
</pre></div>
</div>
</div>
</div>
<p>We can apply the <a class="reference external" href="https://rdrr.io/cran/topicmodels/man/lda.html"><code class="docutils literal notranslate"><span class="pre">LDA()</span></code></a> function from the <strong>topicmodels</strong> package, specifying <code class="docutils literal notranslate"><span class="pre">k</span> <span class="pre">=</span> <span class="pre">2</span></code>, to build a two-topic LDA model.</p>
<p>In most real-world applications, topic models typically use a larger value of <code class="docutils literal notranslate"><span class="pre">k</span></code>, but as we’ll see, the same analysis process applies when working with more topics.</p>
<p>This function returns an object that holds all the details of the fitted model, including the relationships between words and topics, as well as between topics and documents.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1"># set a seed so that the output of the model is predictable</span>
<span class="n">ap_lda</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">LDA</span><span class="p">(</span><span class="n">AssociatedPress</span><span class="p">,</span><span class="w"> </span><span class="n">k</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">2</span><span class="p">,</span><span class="w"> </span><span class="n">control</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">list</span><span class="p">(</span><span class="n">seed</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1234</span><span class="p">))</span>
<span class="n">ap_lda</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>A LDA_VEM topic model with 2 topics.
</pre></div>
</div>
</div>
</div>
<p>Fitting the model was the straightforward step; the real work lies in exploring and interpreting it using tidying functions from the tidytext package.</p>
<section id="word-topic-probabilities">
<h4>Word-topic Probabilities<a class="headerlink" href="#word-topic-probabilities" title="Link to this heading">#</a></h4>
<p>Above we introduced the <a class="reference external" href="https://generics.r-lib.org/reference/tidy.html"><code class="docutils literal notranslate"><span class="pre">tidy()</span></code></a> method, originally from the broom package <a class="reference external" href="https://www.tidytextmining.com/references#ref-R-broom">(Robinson 2017)</a>, for tidying model objects. The tidytext package provides this method for extracting the per-topic-per-word probabilities, called β (“beta”), from the model.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">library</span><span class="p">(</span><span class="n">tidytext</span><span class="p">)</span>

<span class="n">ap_topics</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">tidy</span><span class="p">(</span><span class="n">ap_lda</span><span class="p">,</span><span class="w"> </span><span class="n">matrix</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;beta&quot;</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">ap_topics</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span># A tibble: 20,946 x 3
   topic term           beta
   &lt;int&gt; &lt;chr&gt;         &lt;dbl&gt;
 1     1 aaron      1.69e<span class=" -Color -Color-Red">-12</span>
 2     2 aaron      3.90e<span class=" -Color -Color-Red">- 5</span>
 3     1 abandon    2.65e<span class=" -Color -Color-Red">- 5</span>
 4     2 abandon    3.99e<span class=" -Color -Color-Red">- 5</span>
 5     1 abandoned  1.39e<span class=" -Color -Color-Red">- 4</span>
 6     2 abandoned  5.88e<span class=" -Color -Color-Red">- 5</span>
 7     1 abandoning 2.45e<span class=" -Color -Color-Red">-33</span>
 8     2 abandoning 2.34e<span class=" -Color -Color-Red">- 5</span>
 9     1 abbott     2.13e<span class=" -Color -Color-Red">- 6</span>
10     2 abbott     2.97e<span class=" -Color -Color-Red">- 5</span>
# i 20,936 more rows
</pre></div>
</div>
</div>
</div>
<p>Notice that this transforms the model into a format with one topic-term pair per row. For each pair, the model calculates the probability that the term is generated by that topic. For example, the term “aaron” has a probability of <span class="math notranslate nohighlight">\(1.686917 \times 10^{-12}\)</span> of being generated by topic 1, but a probability of
<span class="math notranslate nohighlight">\(3.8959408 \times 10^{-5}\)</span> of coming from topic 2.</p>
<p>We can use dplyr’s <a class="reference external" href="https://dplyr.tidyverse.org/reference/slice.html"><code class="docutils literal notranslate"><span class="pre">slice_max()</span></code></a> to identify the top 10 terms most strongly associated with each topic. As a tidy data frame, this structure is well suited for visualization using ggplot2.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">library</span><span class="p">(</span><span class="n">ggplot2</span><span class="p">)</span>
<span class="nf">library</span><span class="p">(</span><span class="n">dplyr</span><span class="p">)</span>

<span class="n">ap_top_terms</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">ap_topics</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">group_by</span><span class="p">(</span><span class="n">topic</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">slice_max</span><span class="p">(</span><span class="n">beta</span><span class="p">,</span><span class="w"> </span><span class="n">n</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">10</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span>
<span class="w">  </span><span class="nf">ungroup</span><span class="p">()</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">arrange</span><span class="p">(</span><span class="n">topic</span><span class="p">,</span><span class="w"> </span><span class="o">-</span><span class="n">beta</span><span class="p">)</span>

<span class="n">ap_top_terms</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">mutate</span><span class="p">(</span><span class="n">term</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">reorder_within</span><span class="p">(</span><span class="n">term</span><span class="p">,</span><span class="w"> </span><span class="n">beta</span><span class="p">,</span><span class="w"> </span><span class="n">topic</span><span class="p">))</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">ggplot</span><span class="p">(</span><span class="nf">aes</span><span class="p">(</span><span class="n">beta</span><span class="p">,</span><span class="w"> </span><span class="n">term</span><span class="p">,</span><span class="w"> </span><span class="n">fill</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">factor</span><span class="p">(</span><span class="n">topic</span><span class="p">)))</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="nf">geom_col</span><span class="p">(</span><span class="n">show.legend</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">FALSE</span><span class="p">)</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="nf">facet_wrap</span><span class="p">(</span><span class="o">~</span><span class="w"> </span><span class="n">topic</span><span class="p">,</span><span class="w"> </span><span class="n">scales</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;free&quot;</span><span class="p">)</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="nf">scale_y_reordered</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<a class="reference internal image-reference" href="../../../_images/892000b81172041aa51dc409ad52b217c71b97b389423cc1697ad7717eec81bb.png"><img alt="../../../_images/892000b81172041aa51dc409ad52b217c71b97b389423cc1697ad7717eec81bb.png" src="../../../_images/892000b81172041aa51dc409ad52b217c71b97b389423cc1697ad7717eec81bb.png" style="width: 420px; height: 420px;" />
</a>
</div>
</div>
<p>This visualization helps us interpret the two topics extracted from the articles. The most frequent words in topic 1 include “percent,” “million,” “billion,” and “company,” indicating it likely relates to business or financial news. In contrast, topic 2 features common words like “president,” “government,” and “soviet,” suggesting it represents political news. Notably, some words such as “new” and “people” appear frequently in both topics. This overlap is a key advantage of topic modeling compared to “hard clustering” methods, as topics in natural language often share vocabulary.</p>
<p>Alternatively, we can focus on terms with the largest difference in <span class="math notranslate nohighlight">\(\beta\)</span> between topic 1 and topic 2. This difference can be measured using the log ratio:
<span class="math notranslate nohighlight">\(\log_2(\frac{\beta_2}{\beta_1})\)</span> (log ratios are helpful because they create symmetry — if <span class="math notranslate nohighlight">\(\beta_2\)</span> is twice <span class="math notranslate nohighlight">\(\beta_1\)</span>, the ratio is 1; if <span class="math notranslate nohighlight">\(\beta_1\)</span> is twice <span class="math notranslate nohighlight">\(\beta_2\)</span>, it’s -1). To focus on especially relevant words, we can filter for terms with <span class="math notranslate nohighlight">\(\beta\)</span> values greater than 1/1000 in at least one topic.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">library</span><span class="p">(</span><span class="n">tidyr</span><span class="p">)</span>

<span class="n">beta_wide</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">ap_topics</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">mutate</span><span class="p">(</span><span class="n">topic</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">paste0</span><span class="p">(</span><span class="s">&quot;topic&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">topic</span><span class="p">))</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">pivot_wider</span><span class="p">(</span><span class="n">names_from</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">topic</span><span class="p">,</span><span class="w"> </span><span class="n">values_from</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">beta</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span>
<span class="w">  </span><span class="nf">filter</span><span class="p">(</span><span class="n">topic1</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="n">.</span><span class="m">001</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">topic2</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="n">.</span><span class="m">001</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">mutate</span><span class="p">(</span><span class="n">log_ratio</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">log2</span><span class="p">(</span><span class="n">topic2</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">topic1</span><span class="p">))</span>

<span class="nf">print</span><span class="p">(</span><span class="n">beta_wide</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span># A tibble: 198 x 4
   term              topic1      topic2 log_ratio
   &lt;chr&gt;              &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt;
 1 administration 0.000431  0.00138         1.68 
 2 ago            0.00107   0.000842       -<span class=" -Color -Color-Red">0.339</span>
 3 agreement      0.000671  0.00104         0.630
 4 aid            0.0000476 0.00105         4.46 
 5 air            0.00214   0.000297       -<span class=" -Color -Color-Red">2.85</span> 
 6 american       0.00203   0.00168        -<span class=" -Color -Color-Red">0.270</span>
 7 analysts       0.00109   0.000000578   -<span class=" -Color -Color-Red">10.9</span>  
 8 area           0.00137   0.000231       -<span class=" -Color -Color-Red">2.57</span> 
 9 army           0.000262  0.00105         2.00 
10 asked          0.000189  0.00156         3.05 
# i 188 more rows
</pre></div>
</div>
</div>
</div>
<p>The words with the greatest differences between the two topics are visualized below.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">beta_wide</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">group_by</span><span class="p">(</span><span class="n">direction</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">log_ratio</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="m">0</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">slice_max</span><span class="p">(</span><span class="nf">abs</span><span class="p">(</span><span class="n">log_ratio</span><span class="p">),</span><span class="w"> </span><span class="n">n</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">10</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span>
<span class="w">  </span><span class="nf">ungroup</span><span class="p">()</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">mutate</span><span class="p">(</span><span class="n">term</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">reorder</span><span class="p">(</span><span class="n">term</span><span class="p">,</span><span class="w"> </span><span class="n">log_ratio</span><span class="p">))</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">ggplot</span><span class="p">(</span><span class="nf">aes</span><span class="p">(</span><span class="n">log_ratio</span><span class="p">,</span><span class="w"> </span><span class="n">term</span><span class="p">))</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="nf">geom_col</span><span class="p">()</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="nf">labs</span><span class="p">(</span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;Log2 ratio of beta in topic 2 / topic 1&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">NULL</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<a class="reference internal image-reference" href="../../../_images/5b8ab5447196db9cbc0343b40d6ae9cec6cfa3be7d86582fcc2c551b7fd62a77.png"><img alt="../../../_images/5b8ab5447196db9cbc0343b40d6ae9cec6cfa3be7d86582fcc2c551b7fd62a77.png" src="../../../_images/5b8ab5447196db9cbc0343b40d6ae9cec6cfa3be7d86582fcc2c551b7fd62a77.png" style="width: 420px; height: 420px;" />
</a>
</div>
</div>
<p>We observe that words more prevalent in topic 2 include political parties like “democratic” and “republican,” along with politicians’ names such as “dukakis” and “gorbachev.” Topic 1, on the other hand, is distinguished by currency terms like “yen” and “dollar,” as well as financial words such as “index,” “prices,” and “rates.” This supports the conclusion that the two topics identified by the algorithm correspond to political and financial news.</p>
</section>
<section id="document-topic-probabilities">
<h4>Document-topic Probabilities<a class="headerlink" href="#document-topic-probabilities" title="Link to this heading">#</a></h4>
<p>In addition to representing each topic as a mixture of words, LDA also models each document as a mixture of topics. We can explore the per-document-per-topic probabilities, known as <span class="math notranslate nohighlight">\(\gamma\)</span> (“gamma”), by using the <code class="docutils literal notranslate"><span class="pre">matrix</span> <span class="pre">=</span> <span class="pre">&quot;gamma&quot;</span></code> argument with the <a class="reference external" href="https://generics.r-lib.org/reference/tidy.html"><code class="docutils literal notranslate"><span class="pre">tidy()</span></code></a> function.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">ap_documents</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">tidy</span><span class="p">(</span><span class="n">ap_lda</span><span class="p">,</span><span class="w"> </span><span class="n">matrix</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;gamma&quot;</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">ap_documents</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span># A tibble: 4,492 x 3
   document topic    gamma
      &lt;int&gt; &lt;int&gt;    &lt;dbl&gt;
 1        1     1 0.248   
 2        2     1 0.362   
 3        3     1 0.527   
 4        4     1 0.357   
 5        5     1 0.181   
 6        6     1 0.000588
 7        7     1 0.773   
 8        8     1 0.00445 
 9        9     1 0.967   
10       10     1 0.147   
# i 4,482 more rows
</pre></div>
</div>
</div>
</div>
<p>Each value represents the estimated proportion of words in a document generated by a specific topic. For instance, the model estimates that about 25% of the words in document 1 come from topic 1.</p>
<p>We can observe that many documents contain a mixture of both topics, while document 6 is almost entirely from topic 2, with a <span class="math notranslate nohighlight">\(\gamma\)</span> value for topic 1 near zero. To verify this, we could <a class="reference external" href="https://generics.r-lib.org/reference/tidy.html"><code class="docutils literal notranslate"><span class="pre">tidy()</span></code></a> the document-term matrix and examine the most frequent words in that document.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">tidy</span><span class="p">(</span><span class="n">AssociatedPress</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">filter</span><span class="p">(</span><span class="n">document</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="m">6</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">arrange</span><span class="p">(</span><span class="nf">desc</span><span class="p">(</span><span class="n">count</span><span class="p">))</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">print</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span># A tibble: 287 x 3
   document term           count
      &lt;int&gt; &lt;chr&gt;          &lt;dbl&gt;
 1        6 noriega           16
 2        6 panama            12
 3        6 jackson            6
 4        6 powell             6
 5        6 administration     5
 6        6 economic           5
 7        6 general            5
 8        6 i                  5
 9        6 panamanian         5
10        6 american           4
# i 277 more rows
</pre></div>
</div>
</div>
</div>
<p>Judging by the most frequent words, this article seems to discuss the relationship between the American government and Panamanian dictator Manuel Noriega, confirming that the algorithm correctly assigned it to topic 2 (political/national news).</p>
</section>
</section>
<section id="example-the-great-library-heist">
<h3>Example: The Great Library Heist<a class="headerlink" href="#example-the-great-library-heist" title="Link to this heading">#</a></h3>
<p>When testing a statistical method, it’s helpful to apply it to a simple case where the “correct answer” is known. For example, we could gather a set of documents clearly related to four distinct topics, then use topic modeling to see if the algorithm correctly separates these groups. This allows us to verify the method’s effectiveness and understand its limitations. We’ll explore this using classic literature.</p>
<p>Imagine a vandal broke into your study and tore apart four books:</p>
<ul class="simple">
<li><p><em>Great Expectations</em> by Charles Dickens</p></li>
<li><p><em>The War of the Worlds</em> by H.G. Wells</p></li>
<li><p><em>Twenty Thousand Leagues Under the Sea</em> by Jules Verne</p></li>
<li><p><em>Pride and Prejudice</em> by Jane Austen</p></li>
</ul>
<p>The vandal has shredded the books into individual chapters and dumped them all together in a pile. How can we put the chapters back in order by their original books? This is a difficult task because the chapters are unlabeled—we don’t know which words will differentiate the groups. We’ll use topic modeling to uncover how the chapters cluster into distinct topics, each presumably corresponding to one of the four books.</p>
<p>To do this, we’ll retrieve the texts of these books using the <strong>gutenbergr</strong> package introduced earlier.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">titles</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="s">&quot;Twenty Thousand Leagues under the Sea&quot;</span><span class="p">,</span><span class="w"> </span>
<span class="w">            </span><span class="s">&quot;The War of the Worlds&quot;</span><span class="p">,</span>
<span class="w">            </span><span class="s">&quot;Pride and Prejudice&quot;</span><span class="p">,</span><span class="w"> </span>
<span class="w">            </span><span class="s">&quot;Great Expectations&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">library</span><span class="p">(</span><span class="n">gutenbergr</span><span class="p">)</span>

<span class="n">books</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">gutenberg_works</span><span class="p">(</span><span class="n">title</span><span class="w"> </span><span class="o">%in%</span><span class="w"> </span><span class="n">titles</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">gutenberg_download</span><span class="p">(</span><span class="n">meta_fields</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;title&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>For pre-processing, we split the texts into chapters, then use tidytext’s <a class="reference external" href="https://juliasilge.github.io/tidytext/reference/unnest_tokens.html"><code class="docutils literal notranslate"><span class="pre">unnest_tokens()</span></code></a> to break them down into individual words, followed by removing stop words. Each chapter is treated as its own “document,” labeled with names like <code class="docutils literal notranslate"><span class="pre">Great</span> <span class="pre">Expectations\_1</span></code> or <code class="docutils literal notranslate"><span class="pre">Pride</span> <span class="pre">and</span> <span class="pre">Prejudice\_11</span></code>. (In other contexts, a document might be a single news article or blog post.)</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">library</span><span class="p">(</span><span class="n">stringr</span><span class="p">)</span>

<span class="c1"># divide into documents, each representing one chapter</span>
<span class="n">by_chapter</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">books</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">group_by</span><span class="p">(</span><span class="n">title</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">mutate</span><span class="p">(</span><span class="n">chapter</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">cumsum</span><span class="p">(</span><span class="nf">str_detect</span><span class="p">(</span>
<span class="w">    </span><span class="n">text</span><span class="p">,</span><span class="w"> </span><span class="nf">regex</span><span class="p">(</span><span class="s">&quot;^chapter\\s+[ivxlcdm]+\\.?$|^[ivxlcdm]{2,}\\.$&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">ignore_case</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">)</span>
<span class="w">  </span><span class="p">)))</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">ungroup</span><span class="p">()</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">filter</span><span class="p">(</span><span class="n">chapter</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="m">0</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">unite</span><span class="p">(</span><span class="n">document</span><span class="p">,</span><span class="w"> </span><span class="n">title</span><span class="p">,</span><span class="w"> </span><span class="n">chapter</span><span class="p">)</span>

<span class="c1"># split into words</span>
<span class="n">by_chapter_word</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">by_chapter</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">unnest_tokens</span><span class="p">(</span><span class="n">word</span><span class="p">,</span><span class="w"> </span><span class="n">text</span><span class="p">)</span>

<span class="c1"># find document-word counts</span>
<span class="n">word_counts</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">by_chapter_word</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">anti_join</span><span class="p">(</span><span class="n">stop_words</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">count</span><span class="p">(</span><span class="n">document</span><span class="p">,</span><span class="w"> </span><span class="n">word</span><span class="p">,</span><span class="w"> </span><span class="n">sort</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="n">word_counts</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Joining with `by = join_by(word)`
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span># A tibble: 105,201 x 3
   document                 word        n
   &lt;chr&gt;                    &lt;chr&gt;   &lt;int&gt;
 1 Great Expectations_58    joe        88
 2 Great Expectations_7     joe        70
 3 Great Expectations_17    biddy      63
 4 Great Expectations_28    joe        58
 5 Great Expectations_39    estella    58
 6 Great Expectations_2     joe        56
 7 Great Expectations_24    pocket     53
 8 Great Expectations_15    joe        50
 9 Great Expectations_18    joe        50
10 The War of the Worlds_13 brother    50
# i 105,191 more rows
</pre></div>
</div>
</div>
</div>
<section id="lda-on-chapters">
<h4>LDA on Chapters<a class="headerlink" href="#lda-on-chapters" title="Link to this heading">#</a></h4>
<p>Currently, our data frame <code class="docutils literal notranslate"><span class="pre">word_counts</span></code> is in tidy format, with one term per document per row. However, the <strong>topicmodels</strong> package expects a <code class="docutils literal notranslate"><span class="pre">DocumentTermMatrix</span></code>. We can convert a one-token-per-row table into a <code class="docutils literal notranslate"><span class="pre">DocumentTermMatrix</span></code> using tidytext’s <a class="reference external" href="https://juliasilge.github.io/tidytext/reference/document_term_casters.html"><code class="docutils literal notranslate"><span class="pre">cast_dtm()</span></code></a> function.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">chapters_dtm</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">word_counts</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">cast_dtm</span><span class="p">(</span><span class="n">document</span><span class="p">,</span><span class="w"> </span><span class="n">word</span><span class="p">,</span><span class="w"> </span><span class="n">n</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="n">chapters_dtm</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;&lt;DocumentTermMatrix (documents: 187, terms: 18316)&gt;&gt;
Non-/sparse entries: 105201/3319891
Sparsity           : 97%
Maximal term length: 19
Weighting          : term frequency (tf)
</pre></div>
</div>
</div>
</div>
<p>We can then apply the <a class="reference external" href="https://rdrr.io/cran/topicmodels/man/lda.html"><code class="docutils literal notranslate"><span class="pre">LDA()</span></code></a> function to build a four-topic model. In this example, we know to set four topics since we’re working with four books; in other situations, we may need to experiment with different values of <code class="docutils literal notranslate"><span class="pre">k</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">chapters_lda</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">LDA</span><span class="p">(</span><span class="n">chapters_dtm</span><span class="p">,</span><span class="w"> </span><span class="n">k</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">4</span><span class="p">,</span><span class="w"> </span><span class="n">control</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">list</span><span class="p">(</span><span class="n">seed</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1234</span><span class="p">))</span>
<span class="nf">print</span><span class="p">(</span><span class="n">chapters_lda</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>A LDA_VEM topic model with 4 topics.
</pre></div>
</div>
</div>
</div>
<p>Much as we did on the Associated Press data, we can examine per-topic-per-word probabilities.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">chapter_topics</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">tidy</span><span class="p">(</span><span class="n">chapters_lda</span><span class="p">,</span><span class="w"> </span><span class="n">matrix</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;beta&quot;</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">chapter_topics</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span># A tibble: 73,264 x 3
   topic term         beta
   &lt;int&gt; &lt;chr&gt;       &lt;dbl&gt;
 1     1 joe     1.58e<span class=" -Color -Color-Red">- 41</span>
 2     2 joe     1.32e<span class=" -Color -Color-Red">-  2</span>
 3     3 joe     9.39e<span class=" -Color -Color-Red">- 33</span>
 4     4 joe     1.25e<span class=" -Color -Color-Red">- 65</span>
 5     1 biddy   1.79e<span class=" -Color -Color-Red">- 49</span>
 6     2 biddy   4.34e<span class=" -Color -Color-Red">-  3</span>
 7     3 biddy   1.74e<span class=" -Color -Color-Red">- 33</span>
 8     4 biddy   5.79e<span class=" -Color -Color-Red">-251</span>
 9     1 estella 9.70e<span class=" -Color -Color-Red">- 21</span>
10     2 estella 4.52e<span class=" -Color -Color-Red">-  3</span>
# i 73,254 more rows
</pre></div>
</div>
</div>
</div>
<p>You’ll notice the model is now in a one-topic-per-term-per-row format. For each term-topic pair, the model estimates the probability that the term was generated from that topic. For instance, the term “joe” has near-zero probability for topics 1, 2, and 3, but contributes around 0% to topic 4.</p>
<p>We can use <a class="reference external" href="https://dplyr.tidyverse.org/reference/slice.html"><code class="docutils literal notranslate"><span class="pre">slice_max()</span></code></a> from <strong>dplyr</strong> to extract the top 5 terms for each topic.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">top_terms</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">chapter_topics</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">group_by</span><span class="p">(</span><span class="n">topic</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">slice_max</span><span class="p">(</span><span class="n">beta</span><span class="p">,</span><span class="w"> </span><span class="n">n</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">5</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span>
<span class="w">  </span><span class="nf">ungroup</span><span class="p">()</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">arrange</span><span class="p">(</span><span class="n">topic</span><span class="p">,</span><span class="w"> </span><span class="o">-</span><span class="n">beta</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="n">top_terms</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span># A tibble: 20 x 3
   topic term          beta
   &lt;int&gt; &lt;chr&gt;        &lt;dbl&gt;
 1     1 captain    0.0156 
 2     1 _nautilus_ 0.0129 
 3     1 sea        0.00915
 4     1 nemo       0.00890
 5     1 ned        0.00816
 6     2 joe        0.0132 
 7     2 miss       0.00727
 8     2 time       0.00657
 9     2 pip        0.00621
10     2 looked     0.00591
11     3 people     0.00614
12     3 martians   0.00581
13     3 time       0.00542
14     3 black      0.00516
15     3 night      0.00438
16     4 elizabeth  0.0156 
17     4 darcy      0.00980
18     4 bennet     0.00763
19     4 miss       0.00750
20     4 jane       0.00718
</pre></div>
</div>
</div>
</div>
<p>This tidy output lends itself well to a ggplot2 visualization.</p>
<p><strong>THESE ARE OUT OF ORDER SO FIX ACCORDINGLY</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">library</span><span class="p">(</span><span class="n">ggplot2</span><span class="p">)</span>

<span class="n">top_terms</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">mutate</span><span class="p">(</span><span class="n">term</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">reorder_within</span><span class="p">(</span><span class="n">term</span><span class="p">,</span><span class="w"> </span><span class="n">beta</span><span class="p">,</span><span class="w"> </span><span class="n">topic</span><span class="p">))</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">ggplot</span><span class="p">(</span><span class="nf">aes</span><span class="p">(</span><span class="n">beta</span><span class="p">,</span><span class="w"> </span><span class="n">term</span><span class="p">,</span><span class="w"> </span><span class="n">fill</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">factor</span><span class="p">(</span><span class="n">topic</span><span class="p">)))</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="nf">geom_col</span><span class="p">(</span><span class="n">show.legend</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">FALSE</span><span class="p">)</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="nf">facet_wrap</span><span class="p">(</span><span class="o">~</span><span class="w"> </span><span class="n">topic</span><span class="p">,</span><span class="w"> </span><span class="n">scales</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;free&quot;</span><span class="p">)</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="nf">scale_y_reordered</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<a class="reference internal image-reference" href="../../../_images/752bcba264f2404d3972e820e5708365f3d7c06b57e9f848ce8b4fb16f4520eb.png"><img alt="../../../_images/752bcba264f2404d3972e820e5708365f3d7c06b57e9f848ce8b4fb16f4520eb.png" src="../../../_images/752bcba264f2404d3972e820e5708365f3d7c06b57e9f848ce8b4fb16f4520eb.png" style="width: 420px; height: 420px;" />
</a>
</div>
</div>
<p>These topics clearly align with the four books! It’s obvious that terms like “captain,” “nautilus,” “sea,” and “nemo” belong to <em>Twenty Thousand Leagues Under the Sea</em>, while “jane,” “darcy,” and “elizabeth” point to <em>Pride and Prejudice</em>. We see “pip” and “joe” from <em>Great Expectations</em>, and “martians,” “black,” and “night” from <em>The War of the Worlds</em>. We can also observe that, as expected with LDA being a form of “fuzzy clustering,” some words appear across multiple topics—like “miss” in both topics 2 and 4..</p>
</section>
<section id="per-document-classification">
<h4>Per-document classification<a class="headerlink" href="#per-document-classification" title="Link to this heading">#</a></h4>
<p>In this analysis, each document corresponds to a single chapter. So, we might want to determine which topics are linked to each chapter. Can we use that information to correctly reassemble the chapters into their original books? To do this, we can examine the per-document-per-topic probabilities, known as <span class="math notranslate nohighlight">\(\gamma\)</span> (“gamma”).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">chapters_gamma</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">tidy</span><span class="p">(</span><span class="n">chapters_lda</span><span class="p">,</span><span class="w"> </span><span class="n">matrix</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;gamma&quot;</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">chapters_gamma</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span># A tibble: 748 x 3
   document                 topic     gamma
   &lt;chr&gt;                    &lt;int&gt;     &lt;dbl&gt;
 1 Great Expectations_58        1 0.0000175
 2 Great Expectations_7         1 0.0000185
 3 Great Expectations_17        1 0.0000271
 4 Great Expectations_28        1 0.0000247
 5 Great Expectations_39        1 0.0241   
 6 Great Expectations_2         1 0.0000218
 7 Great Expectations_24        1 0.0000240
 8 Great Expectations_15        1 0.00115  
 9 Great Expectations_18        1 0.0000163
10 The War of the Worlds_13     1 0.0000143
# i 738 more rows
</pre></div>
</div>
</div>
</div>
<p>Each of these values estimates the proportion of words in a document that were generated by a particular topic.</p>
<p>With these topic probabilities, we can evaluate how effectively our unsupervised model distinguished between the four books. Ideally, we’d expect most chapters within a given book to be primarily (or entirely) generated from the matching topic.</p>
<p>To explore this, we’ll first split the document name back into the book title and chapter number, then visualize the per-document-per-topic probabilities for each.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">chapters_gamma</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">chapters_gamma</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">separate</span><span class="p">(</span><span class="n">document</span><span class="p">,</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="s">&quot;title&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;chapter&quot;</span><span class="p">),</span><span class="w"> </span><span class="n">sep</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;_&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">convert</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="n">chapters_gamma</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span># A tibble: 748 x 4
   title                 chapter topic     gamma
   &lt;chr&gt;                   &lt;int&gt; &lt;int&gt;     &lt;dbl&gt;
 1 Great Expectations         58     1 0.0000175
 2 Great Expectations          7     1 0.0000185
 3 Great Expectations         17     1 0.0000271
 4 Great Expectations         28     1 0.0000247
 5 Great Expectations         39     1 0.0241   
 6 Great Expectations          2     1 0.0000218
 7 Great Expectations         24     1 0.0000240
 8 Great Expectations         15     1 0.00115  
 9 Great Expectations         18     1 0.0000163
10 The War of the Worlds      13     1 0.0000143
# i 738 more rows
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1"># reorder titles in order of topic 1, topic 2, etc before plotting</span>
<span class="n">chapters_gamma</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">mutate</span><span class="p">(</span><span class="n">title</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">reorder</span><span class="p">(</span><span class="n">title</span><span class="p">,</span><span class="w"> </span><span class="n">gamma</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">topic</span><span class="p">))</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">ggplot</span><span class="p">(</span><span class="nf">aes</span><span class="p">(</span><span class="nf">factor</span><span class="p">(</span><span class="n">topic</span><span class="p">),</span><span class="w"> </span><span class="n">gamma</span><span class="p">))</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="nf">geom_boxplot</span><span class="p">()</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="nf">facet_wrap</span><span class="p">(</span><span class="o">~</span><span class="w"> </span><span class="n">title</span><span class="p">)</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="nf">labs</span><span class="p">(</span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;topic&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">expression</span><span class="p">(</span><span class="n">gamma</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<a class="reference internal image-reference" href="../../../_images/3906bc03c6d716e46cfb06b07c026659d1e84bf87d2a636950751f7b09fbae72.png"><img alt="../../../_images/3906bc03c6d716e46cfb06b07c026659d1e84bf87d2a636950751f7b09fbae72.png" src="../../../_images/3906bc03c6d716e46cfb06b07c026659d1e84bf87d2a636950751f7b09fbae72.png" style="width: 420px; height: 420px;" />
</a>
</div>
</div>
<p>We can see that nearly all chapters from <em>Pride and Prejudice</em>, <em>The War of the Worlds</em>, and <em>Twenty Thousand Leagues Under the Sea</em> were clearly matched to a single, distinct topic.</p>
<p>However, a few chapters from <em>Great Expectations</em> show some association with other topics. Are there any cases where a chapter’s strongest topic actually aligns with a different book? To find out, we can use <a class="reference external" href="https://dplyr.tidyverse.org/reference/slice.html"><code class="docutils literal notranslate"><span class="pre">slice_max()</span></code></a> to select the most dominant topic for each chapter—this serves as the model’s effective “classification” for that chapter.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">chapter_classifications</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">chapters_gamma</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">group_by</span><span class="p">(</span><span class="n">title</span><span class="p">,</span><span class="w"> </span><span class="n">chapter</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">slice_max</span><span class="p">(</span><span class="n">gamma</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">ungroup</span><span class="p">()</span>

<span class="nf">print</span><span class="p">(</span><span class="n">chapter_classifications</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span># A tibble: 187 x 4
   title              chapter topic gamma
   &lt;chr&gt;                &lt;int&gt; &lt;int&gt; &lt;dbl&gt;
 1 Great Expectations       1     2 0.804
 2 Great Expectations       2     2 1.00 
 3 Great Expectations       3     2 0.823
 4 Great Expectations       4     2 1.00 
 5 Great Expectations       5     2 0.943
 6 Great Expectations       6     2 1.00 
 7 Great Expectations       7     2 1.00 
 8 Great Expectations       8     2 0.934
 9 Great Expectations       9     2 1.00 
10 Great Expectations      10     2 1.00 
# i 177 more rows
</pre></div>
</div>
</div>
</div>
<p>Next, we can compare each chapter’s assigned topic to the “consensus” topic for its book—the topic that appears most frequently across its chapters—and check which chapters were most commonly misclassified.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">book_topics</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">chapter_classifications</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">count</span><span class="p">(</span><span class="n">title</span><span class="p">,</span><span class="w"> </span><span class="n">topic</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">group_by</span><span class="p">(</span><span class="n">title</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">slice_max</span><span class="p">(</span><span class="n">n</span><span class="p">,</span><span class="w"> </span><span class="n">n</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span>
<span class="w">  </span><span class="nf">ungroup</span><span class="p">()</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">transmute</span><span class="p">(</span><span class="n">consensus</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">title</span><span class="p">,</span><span class="w"> </span><span class="n">topic</span><span class="p">)</span>

<span class="n">chapter_classifications</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">inner_join</span><span class="p">(</span><span class="n">book_topics</span><span class="p">,</span><span class="w"> </span><span class="n">by</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;topic&quot;</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">filter</span><span class="p">(</span><span class="n">title</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">consensus</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><table class="dataframe">
<caption>A tibble: 1 x 5</caption>
<thead>
	<tr><th scope=col>title</th><th scope=col>chapter</th><th scope=col>topic</th><th scope=col>gamma</th><th scope=col>consensus</th></tr>
	<tr><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;chr&gt;</th></tr>
</thead>
<tbody>
	<tr><td>Great Expectations</td><td>55</td><td>3</td><td>0.7430811</td><td>The War of the Worlds</td></tr>
</tbody>
</table>
</div></div>
</div>
<p>We see that some chapters from Great Expectations were misclassified. That’s not bad for unsupervised clustering!</p>
</section>
<section id="by-word-assignments-augment">
<h4>By Word Assignments: <code class="docutils literal notranslate"><span class="pre">augment</span></code><a class="headerlink" href="#by-word-assignments-augment" title="Link to this heading">#</a></h4>
<p>A key step in the LDA algorithm is assigning each word in every document to a specific topic. Generally, the more words in a document assigned to a given topic, the higher the weight (<code class="docutils literal notranslate"><span class="pre">gamma</span></code>) for that document-topic pairing.</p>
<p>To explore this, we might want to link each original document-word pair to the topic it was assigned. This is where the <a class="reference external" href="https://generics.r-lib.org/reference/augment.html"><code class="docutils literal notranslate"><span class="pre">augment()</span></code></a> function comes in—originally from the <strong>broom</strong> package, it’s designed to tidy model outputs by adding information back onto the original observations. While <a class="reference external" href="https://generics.r-lib.org/reference/tidy.html"><code class="docutils literal notranslate"><span class="pre">tidy()</span></code></a> extracts summary statistics from the model, <a class="reference external" href="https://generics.r-lib.org/reference/augment.html"><code class="docutils literal notranslate"><span class="pre">augment()</span></code></a> enriches the original data with model-based details.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">assignments</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">augment</span><span class="p">(</span><span class="n">chapters_lda</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">chapters_dtm</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">assignments</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span># A tibble: 105,201 x 4
   document              term  count .topic
   &lt;chr&gt;                 &lt;chr&gt; &lt;dbl&gt;  &lt;dbl&gt;
 1 Great Expectations_58 joe      88      2
 2 Great Expectations_7  joe      70      2
 3 Great Expectations_17 joe       5      2
 4 Great Expectations_28 joe      58      2
 5 Great Expectations_2  joe      56      2
 6 Great Expectations_24 joe       1      2
 7 Great Expectations_15 joe      50      2
 8 Great Expectations_18 joe      50      2
 9 Great Expectations_9  joe      44      2
10 Great Expectations_13 joe      40      2
# i 105,191 more rows
</pre></div>
</div>
</div>
</div>
<p>This produces a tidy data frame of book-term counts with an additional column, <code class="docutils literal notranslate"><span class="pre">.topic</span></code>, indicating the topic assigned to each term within each document. (Note that columns added by <code class="docutils literal notranslate"><span class="pre">augment</span></code> always begin with a <code class="docutils literal notranslate"><span class="pre">.</span></code> to avoid overwriting existing columns.) By combining this <code class="docutils literal notranslate"><span class="pre">assignments</span></code> table with the consensus book titles, we can identify which words were misclassified.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">assignments</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">assignments</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">separate</span><span class="p">(</span><span class="n">document</span><span class="p">,</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="s">&quot;title&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;chapter&quot;</span><span class="p">),</span><span class="w"> </span>
<span class="w">           </span><span class="n">sep</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;_&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">convert</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">inner_join</span><span class="p">(</span><span class="n">book_topics</span><span class="p">,</span><span class="w"> </span><span class="n">by</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="s">&quot;.topic&quot;</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;topic&quot;</span><span class="p">))</span>

<span class="nf">print</span><span class="p">(</span><span class="n">assignments</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span># A tibble: 105,201 x 6
   title              chapter term  count .topic consensus         
   &lt;chr&gt;                &lt;int&gt; &lt;chr&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;             
 1 Great Expectations      58 joe      88      2 Great Expectations
 2 Great Expectations       7 joe      70      2 Great Expectations
 3 Great Expectations      17 joe       5      2 Great Expectations
 4 Great Expectations      28 joe      58      2 Great Expectations
 5 Great Expectations       2 joe      56      2 Great Expectations
 6 Great Expectations      24 joe       1      2 Great Expectations
 7 Great Expectations      15 joe      50      2 Great Expectations
 8 Great Expectations      18 joe      50      2 Great Expectations
 9 Great Expectations       9 joe      44      2 Great Expectations
10 Great Expectations      13 joe      40      2 Great Expectations
# i 105,191 more rows
</pre></div>
</div>
</div>
</div>
<p>Combining the true book titles with their assigned consensus topics provides a valuable basis for deeper analysis. For instance, we can create a <strong>confusion matrix</strong> that reveals how frequently words from one book were assigned to another, using <a class="reference external" href="https://dplyr.tidyverse.org/reference/count.html"><code class="docutils literal notranslate"><span class="pre">count()</span></code></a> from <strong>dplyr</strong> and <code class="docutils literal notranslate"><span class="pre">geom_tile()</span></code> from <strong>ggplot2</strong> for visualization.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">library</span><span class="p">(</span><span class="n">scales</span><span class="p">)</span>

<span class="n">assignments</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">count</span><span class="p">(</span><span class="n">title</span><span class="p">,</span><span class="w"> </span><span class="n">consensus</span><span class="p">,</span><span class="w"> </span><span class="n">wt</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">count</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">mutate</span><span class="p">(</span><span class="nf">across</span><span class="p">(</span><span class="nf">c</span><span class="p">(</span><span class="n">title</span><span class="p">,</span><span class="w"> </span><span class="n">consensus</span><span class="p">),</span><span class="w"> </span><span class="o">~</span><span class="nf">str_wrap</span><span class="p">(</span><span class="n">.</span><span class="p">,</span><span class="w"> </span><span class="m">20</span><span class="p">)))</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">group_by</span><span class="p">(</span><span class="n">title</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">mutate</span><span class="p">(</span><span class="n">percent</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">n</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="nf">sum</span><span class="p">(</span><span class="n">n</span><span class="p">))</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">ggplot</span><span class="p">(</span><span class="nf">aes</span><span class="p">(</span><span class="n">consensus</span><span class="p">,</span><span class="w"> </span><span class="n">title</span><span class="p">,</span><span class="w"> </span><span class="n">fill</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">percent</span><span class="p">))</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="nf">geom_tile</span><span class="p">()</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="nf">scale_fill_gradient2</span><span class="p">(</span><span class="n">high</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;darkred&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">label</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">percent_format</span><span class="p">())</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="nf">theme_minimal</span><span class="p">()</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="nf">theme</span><span class="p">(</span><span class="n">axis.text.x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">element_text</span><span class="p">(</span><span class="n">angle</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">90</span><span class="p">,</span><span class="w"> </span><span class="n">hjust</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1</span><span class="p">),</span>
<span class="w">        </span><span class="n">panel.grid</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">element_blank</span><span class="p">())</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="nf">labs</span><span class="p">(</span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;Book words were assigned to&quot;</span><span class="p">,</span>
<span class="w">       </span><span class="n">y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;Book words came from&quot;</span><span class="p">,</span>
<span class="w">       </span><span class="n">fill</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;% of assignments&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<a class="reference internal image-reference" href="../../../_images/5e2606dab3753d8e58cbfb088054aa331d71e3fd5c63a0670853e97faeec8a1f.png"><img alt="../../../_images/5e2606dab3753d8e58cbfb088054aa331d71e3fd5c63a0670853e97faeec8a1f.png" src="../../../_images/5e2606dab3753d8e58cbfb088054aa331d71e3fd5c63a0670853e97faeec8a1f.png" style="width: 420px; height: 420px;" />
</a>
</div>
</div>
<p>We observe that nearly all words from <em>Pride and Prejudice</em>, <em>Twenty Thousand Leagues Under the Sea</em>, and <em>The War of the Worlds</em> were accurately assigned, whereas <em>Great Expectations</em> had a notable number of misclassified words—contributing to the misclassification of two chapters, as mentioned earlier.</p>
<p>Which words were most frequently misassigned?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">wrong_words</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">assignments</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">filter</span><span class="p">(</span><span class="n">title</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">consensus</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="n">wrong_words</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span># A tibble: 3,532 x 6
   title                                 chapter term     count .topic consensus
   &lt;chr&gt;                                   &lt;int&gt; &lt;chr&gt;    &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;    
 1 Great Expectations                         53 brother      2      3 The War ~
 2 Twenty Thousand Leagues under the Sea       8 miss         1      4 Pride an~
 3 Great Expectations                         43 compeys~    36      3 The War ~
 4 Great Expectations                         47 compeys~     1      3 The War ~
 5 Great Expectations                         55 compeys~     1      3 The War ~
 6 Great Expectations                         54 compeys~     1      3 The War ~
 7 Great Expectations                         47 captain      1      3 The War ~
 8 Great Expectations                         33 captain      1      1 Twenty T~
 9 Great Expectations                         15 sea          1      1 Twenty T~
10 Great Expectations                         31 sea          1      1 Twenty T~
# i 3,522 more rows
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">wrong_words</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">count</span><span class="p">(</span><span class="n">title</span><span class="p">,</span><span class="w"> </span><span class="n">consensus</span><span class="p">,</span><span class="w"> </span><span class="n">term</span><span class="p">,</span><span class="w"> </span><span class="n">wt</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">count</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">ungroup</span><span class="p">()</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">arrange</span><span class="p">(</span><span class="nf">desc</span><span class="p">(</span><span class="n">n</span><span class="p">))</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">print</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span># A tibble: 2,941 x 4
   title              consensus             term          n
   &lt;chr&gt;              &lt;chr&gt;                 &lt;chr&gt;     &lt;dbl&gt;
 1 Great Expectations The War of the Worlds compeyson    39
 2 Great Expectations The War of the Worlds boat         30
 3 Great Expectations The War of the Worlds tide         27
 4 Great Expectations The War of the Worlds magwitch     23
 5 Great Expectations The War of the Worlds black        22
 6 Great Expectations The War of the Worlds water        20
 7 Great Expectations The War of the Worlds lay          19
 8 Great Expectations The War of the Worlds river        19
 9 Great Expectations The War of the Worlds death        18
10 Great Expectations The War of the Worlds jack         18
# i 2,931 more rows
</pre></div>
</div>
</div>
</div>
<p>We notice that several words from <em>Great Expectations</em> were frequently assigned to the <em>Pride and Prejudice</em> or <em>War of the Worlds</em> clusters. For some words, like “love” and “lady,” this makes sense since they are more common in <em>Pride and Prejudice</em> (which we could verify by checking their counts).</p>
<p>However, a few misassigned words never actually appear in the books they were attributed to. For instance, “flopson” occurs only in <em>Great Expectations</em>, yet it was assigned to the <em>Pride and Prejudice</em> cluster.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">word_counts</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">filter</span><span class="p">(</span><span class="n">word</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="s">&quot;flopson&quot;</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">print</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span># A tibble: 3 x 3
  document              word        n
  &lt;chr&gt;                 &lt;chr&gt;   &lt;int&gt;
1 Great Expectations_23 flopson    10
2 Great Expectations_24 flopson     7
3 Great Expectations_34 flopson     1
</pre></div>
</div>
</div>
</div>
<p>Because the LDA algorithm is stochastic, it can sometimes produce topics that cover content from multiple books by chance.</p>
</section>
</section>
<section id="alternative-lda-implementations">
<h3>Alternative LDA Implementations<a class="headerlink" href="#alternative-lda-implementations" title="Link to this heading">#</a></h3>
<p>The <a class="reference external" href="https://rdrr.io/cran/topicmodels/man/lda.html"><code class="docutils literal notranslate"><span class="pre">LDA()</span></code></a> function in the <strong>topicmodels</strong> package is just one way to implement latent Dirichlet allocation. For example, the <a class="reference external" href="https://cran.r-project.org/web/packages/mallet/index.html">mallet</a> package <a class="reference external" href="https://www.tidytextmining.com/references#ref-R-mallet">(Mimno 2013)</a> provides a wrapper around the <a class="reference external" href="https://mimno.github.io/Mallet/index">MALLET</a> Java toolkit for text classification, and tidytext includes tidiers for its output.</p>
<p>The mallet package differs in how it handles input: it expects non-tokenized documents and performs tokenization internally. It also requires a separate stopword file. This means we need to combine the text into a single string per document before running LDA with mallet.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">library</span><span class="p">(</span><span class="n">mallet</span><span class="p">)</span>

<span class="c1"># create a vector with one string per chapter</span>
<span class="n">collapsed</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">by_chapter_word</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">anti_join</span><span class="p">(</span><span class="n">stop_words</span><span class="p">,</span><span class="w"> </span><span class="n">by</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;word&quot;</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">mutate</span><span class="p">(</span><span class="n">word</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">str_replace</span><span class="p">(</span><span class="n">word</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;&#39;&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;&quot;</span><span class="p">))</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">group_by</span><span class="p">(</span><span class="n">document</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">summarize</span><span class="p">(</span><span class="n">text</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">paste</span><span class="p">(</span><span class="n">word</span><span class="p">,</span><span class="w"> </span><span class="n">collapse</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot; &quot;</span><span class="p">))</span>

<span class="c1"># create an empty file of &quot;stopwords&quot;</span>
<span class="nf">file.create</span><span class="p">(</span><span class="n">empty_file</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">tempfile</span><span class="p">())</span>
<span class="n">docs</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">mallet.import</span><span class="p">(</span><span class="n">collapsed</span><span class="o">$</span><span class="n">document</span><span class="p">,</span><span class="w"> </span><span class="n">collapsed</span><span class="o">$</span><span class="n">text</span><span class="p">,</span><span class="w"> </span><span class="n">empty_file</span><span class="p">)</span>

<span class="n">mallet_model</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">MalletLDA</span><span class="p">(</span><span class="n">num.topics</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">4</span><span class="p">)</span>
<span class="n">mallet_model</span><span class="o">$</span><span class="nf">loadDocuments</span><span class="p">(</span><span class="n">docs</span><span class="p">)</span>
<span class="n">mallet_model</span><span class="o">$</span><span class="nf">train</span><span class="p">(</span><span class="m">100</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html">TRUE</div></div>
</div>
<p>After fitting the model, we can apply the <a class="reference external" href="https://generics.r-lib.org/reference/tidy.html"><code class="docutils literal notranslate"><span class="pre">tidy()</span></code></a> and <a class="reference external" href="https://generics.r-lib.org/reference/augment.html"><code class="docutils literal notranslate"><span class="pre">augment()</span></code></a> functions just as described earlier. This allows us to extract word-topic probabilities and document-topic associations in much the same way as with other LDA implementations.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1"># word-topic pairs</span>
<span class="nf">print</span><span class="p">(</span><span class="nf">tidy</span><span class="p">(</span><span class="n">mallet_model</span><span class="p">))</span>

<span class="c1"># document-topic pairs</span>
<span class="nf">print</span><span class="p">(</span><span class="nf">tidy</span><span class="p">(</span><span class="n">mallet_model</span><span class="p">,</span><span class="w"> </span><span class="n">matrix</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;gamma&quot;</span><span class="p">))</span>

<span class="c1"># column needs to be named &quot;term&quot; for &quot;augment&quot;</span>
<span class="n">term_counts</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">rename</span><span class="p">(</span><span class="n">word_counts</span><span class="p">,</span><span class="w"> </span><span class="n">term</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">word</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="nf">augment</span><span class="p">(</span><span class="n">mallet_model</span><span class="p">,</span><span class="w"> </span><span class="n">term_counts</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span># A tibble: 70,628 x 3
   topic term           beta
   &lt;int&gt; &lt;chr&gt;         &lt;dbl&gt;
 1     1 chapter 0.00185    
 2     2 chapter 0.000943   
 3     3 chapter 0.000000286
 4     4 chapter 0.00119    
 5     1 father  0.00432    
 6     2 father  0.000000242
 7     3 father  0.000000286
 8     4 father  0.000000270
 9     1 s       0.0184     
10     2 s       0.0343     
# i 70,618 more rows
# A tibble: 748 x 3
   document              topic gamma
   &lt;chr&gt;                 &lt;int&gt; &lt;dbl&gt;
 1 Great Expectations_1      1 0.115
 2 Great Expectations_10     1 0.168
 3 Great Expectations_11     1 0.180
 4 Great Expectations_12     1 0.190
 5 Great Expectations_13     1 0.228
 6 Great Expectations_14     1 0.241
 7 Great Expectations_15     1 0.159
 8 Great Expectations_16     1 0.269
 9 Great Expectations_17     1 0.252
10 Great Expectations_18     1 0.211
# i 738 more rows
# A tibble: 105,201 x 4
   document                 term        n .topic
   &lt;chr&gt;                    &lt;chr&gt;   &lt;int&gt;  &lt;int&gt;
 1 Great Expectations_58    joe        88      2
 2 Great Expectations_7     joe        70      2
 3 Great Expectations_17    biddy      63      2
 4 Great Expectations_28    joe        58      2
 5 Great Expectations_39    estella    58      2
 6 Great Expectations_2     joe        56      2
 7 Great Expectations_24    pocket     53      2
 8 Great Expectations_15    joe        50      2
 9 Great Expectations_18    joe        50      2
10 The War of the Worlds_13 brother    50      3
# i 105,191 more rows
</pre></div>
</div>
</div>
</div>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "r"
        },
        kernelOptions: {
            name: "ir",
            path: "./src/TextMining/With_R"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'ir'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#acknowledgements">Acknowledgements</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#packages-to-install">Packages to Install</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#sentiment-analysis">Sentiment Analysis</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-sentiments-dataset">The <code class="docutils literal notranslate"><span class="pre">sentiments</span></code> Dataset</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sentiment-analysis-with-inner-join">Sentiment Analysis with Inner Join</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#comparing-sentiment-dictionaries">Comparing Sentiment Dictionaries</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#most-common-positive-and-negative-words">Most Common Positive and Negative Words</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#wordclouds">Wordclouds</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#looking-beyond-just-words">Looking Beyond just Words</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#analyzing-word-and-document-frequency">Analyzing Word and Document Frequency</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#term-frequency-in-jane-austen-s-novels">Term Frequency in Jane Austen’s Novels</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#zipfs-law">Zipf’s Law</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-bind-tf-idf-function">The <code class="docutils literal notranslate"><span class="pre">bind_tf_idf()</span></code> Function</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#a-corpus-of-physics-texts">A Corpus of Physics Texts</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#relationships-between-words">Relationships Between Words</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tokenizing-by-n-gram">Tokenizing by n-gram</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#counting-and-filtering-n-grams">Counting and Filtering n-grams</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#analyzing-bigrams">Analyzing bigrams</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#using-bigrams-to-provide-context">Using Bigrams to Provide Context</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#visualizing-a-network-of-bigrams-with-ggraph">Visualizing a Network of Bigrams with ggraph</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#visualizing-bigrams-in-other-texts">Visualizing Bigrams in Other Texts</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#counting-and-correlating-pairs-of-words-with-the-widyr-package">Counting and Correlating Pairs of Words with the widyr Package</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#counting-and-correlating-among-sections">Counting and Correlating Among Sections</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#pairwise-correlation">Pairwise Correlation</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#converting-to-and-from-non-tidy-formats">Converting To and From Non-tidy Formats</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tidying-a-document-term-matrix">Tidying a Document-term Matrix</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#tidying-documenttermmatrix-objects">Tidying DocumentTermMatrix Objects</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#tidying-dfm-objects">Tidying dfm Objects</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#casting-tidy-text-data-into-a-matrix">Casting Tidy Text Data Into a Matrix</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tidying-corpus-objects-with-metadata">Tidying Corpus Objects with Metadata</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#topic-modeling">Topic Modeling</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#latent-dirichlet-allocation">Latent Dirichlet Allocation</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#word-topic-probabilities">Word-topic Probabilities</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#document-topic-probabilities">Document-topic Probabilities</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-the-great-library-heist">Example: The Great Library Heist</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#lda-on-chapters">LDA on Chapters</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#per-document-classification">Per-document classification</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#by-word-assignments-augment">By Word Assignments: <code class="docutils literal notranslate"><span class="pre">augment</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#alternative-lda-implementations">Alternative LDA Implementations</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By University of South Carolina Libraries
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2025.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>