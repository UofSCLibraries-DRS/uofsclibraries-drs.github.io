
<!DOCTYPE html>


<html lang="en" data-content_root="../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Tokenizers &#8212; USC Libraries - DRS Resources for Data Literacy</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'src/TextMining/tokenizers';</script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
        
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../_static/USC_Libraries_logo_horizontal_RGB_2C.png" class="logo__image only-light" alt="USC Libraries - DRS Resources for Data Literacy - Home"/>
    <script>document.write(`<img src="../../_static/USC_Libraries_logo_horizontal_RGB_2C.png" class="logo__image only-dark" alt="USC Libraries - DRS Resources for Data Literacy - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../intro.html">
                    Introduction
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Data Wrangling with Python</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../Python/BasicPython.html">Basic Python Tutorials</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Python/IntermediatePython.html">Intermediate Python Tutorials</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Python/PandasPython.html">Pandas Python Tutorials</a></li>
<li class="toctree-l1"><a class="reference internal" href="../DataProcessingBasics/DataCleaning_Basics.html">Data Cleaning Basics</a></li>


</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Data Wrangling with R</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../overview/DataWranglingR.html">Brief Overview</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Data Extraction</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../API/API.html">Using APIs</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Text Mining</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="IntroductionTextAnalysis.html">Introduction to Text Analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="AdvancedTextAnalysis.html">Advanced Text Analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="MultiLingualNer.html">Named Entity Recognition (NER)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../overview/Tidy_Text.html">Tidy Text</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Data Visualization</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../overview/Charts.html">Bar, Line, Scatter Plot</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Wolfe-notebooks/Maps.html">Creating Maps with Python</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Machine Learning</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../overview/MachineLearning.html">Brief Overview</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/UofSCLibraries-DRS/uofsclibraries-drs.github.io" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/UofSCLibraries-DRS/uofsclibraries-drs.github.io/issues/new?title=Issue%20on%20page%20%2Fsrc/TextMining/tokenizers.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../_sources/src/TextMining/tokenizers.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Tokenizers</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-a-word">What is a word?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tokenization">Tokenization</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#constellate-datasets">Constellate Datasets</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#creating-your-own-basic-tokenizer">Creating your own basic tokenizer</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#nltk">NLTK</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#word-punctuation">Word Punctuation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#penn-treebank">Penn Treebank</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tweet">Tweet</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multi-word-expression">Multi-Word Expression</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#spacy">spaCy</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <p><img align="left" src="https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/CC_BY.png"><br /></p>
<p>Created by <a class="reference external" href="http://nkelber.com">Nathan Kelber</a> for <a class="reference external" href="https://labs.jstor.org/">JSTOR Labs</a> under <a class="reference external" href="https://creativecommons.org/licenses/by/4.0/">Creative Commons CC BY License</a><br />
For questions/comments/improvements, email <a class="reference external" href="mailto:nathan&#46;kelber&#37;&#52;&#48;ithaka&#46;org">nathan<span>&#46;</span>kelber<span>&#64;</span>ithaka<span>&#46;</span>org</a>.<br /></p>
<hr class="docutils" />
<section class="tex2jax_ignore mathjax_ignore" id="tokenizers">
<h1>Tokenizers<a class="headerlink" href="#tokenizers" title="Link to this heading">#</a></h1>
<p><strong>Description:</strong>
This notebook focuses on the basic concepts surrounding tokenization. It includes material on the following concepts:</p>
<ul class="simple">
<li><p>Word segmentation</p></li>
<li><p>n-grams</p></li>
<li><p>Stemming</p></li>
<li><p>Lemmatization</p></li>
<li><p>Tokenizers</p></li>
</ul>
<p><strong>Use Case:</strong> For Learners (Detailed explanation, not ideal for researchers)</p>
<p><strong>Difficulty:</strong> Intermediate</p>
<p><strong>Completion time:</strong> 90 minutes</p>
<p><strong>Knowledge Required:</strong></p>
<ul class="simple">
<li><p>Python Basics (<a class="reference internal" href="../Python/basic/python-basics-1.html"><span class="std std-doc">Start Python Basics 1</span></a>)</p></li>
</ul>
<p><strong>Knowledge Recommended:</strong></p>
<ul class="simple">
<li><p><a class="reference internal" href="../Python/intermediate/python-intermediate-2.html"><span class="std std-doc">Python Intermediate 2</span></a></p></li>
</ul>
<p><strong>Data Format:</strong> None</p>
<p><strong>Libraries Used:</strong></p>
<ul class="simple">
<li><p>urllib.request</p></li>
<li><p>NLTK</p></li>
<li><p>spaCy</p></li>
</ul>
<p><strong>Research Pipeline:</strong></p>
<ol class="arabic simple">
<li><p>Scan documents</p></li>
<li><p>OCR files</p></li>
<li><p>Clean up texts</p></li>
<li><p><strong>Tokenize text files</strong> (this notebook)</p></li>
</ol>
<hr class="docutils" />
<section id="what-is-a-word">
<h2>What is a word?<a class="headerlink" href="#what-is-a-word" title="Link to this heading">#</a></h2>
<p>The concept of a word makes intuitive sense in everyday language, but it starts to break down significantly when we begin trying to formalize it for analysis with computer programs. Linguists have spent decades creating formal rules for breaking down texts into smaller parts for analysis, dealing in great detail with the normally unspoken rules of grammar. In this lesson, we consider what a word is and consider how we could write a program for collecting the words within a text.</p>
<p>Let’s take a look at an example sentence:</p>
<blockquote>
<div><p>Now that summer’s here, we’re going to visit the beach at Lake Michigan and eat ice cream.</p>
</div></blockquote>
<p>How many words are in this sentence? We could start by simply looking at words that are separated by spaces.</p>
<blockquote>
<div><p>Now, that, summer’s, here, we’re, going, to, visit, the, beach, at, Lake, Michigan, and, eat, ice, cream.</p>
</div></blockquote>
<p>That would give us 17 words. But we could ask a few questions about this count. For example, is ‘Lake Michigan’ one word or two words? Certainly, lake and Michigan have their own individual meanings, but Lake Michigan certainly has a different meaning from either of those words individually. Similarly, what about ‘ice cream’?</p>
<p>What about contractions? Is ‘we’re’ a single word or two words: ‘we’ and ‘are’? If our goal is to count how many times a given word occurs in the sentence, does ‘we’ occur in the sentence? Does the word ‘summer’ occur in our sentence?</p>
<p>Verb conjugations pose yet another problem. Should the word ‘going’ be counted separately from ‘go’. What about ‘went’? From a computational linguistics perspective, we could ‘stem’ words, simply lopping off the ‘ing’ from ‘going’ to get ‘go’. But that would poses some serious programming challenges for words like ‘running’ where the base form is ‘run’ instead of ‘runn’. And we might run into issues with words ‘sing’ or ‘singing’ that should not have ‘ing’ removed in the former case but once in the later case. How could we distinguish between words that are conjugated, like’sings’, and words that are plural like ‘wings’. Sometimes an -s ending is plural (fens) and other times it is not (lens).</p>
</section>
<section id="tokenization">
<h2>Tokenization<a class="headerlink" href="#tokenization" title="Link to this heading">#</a></h2>
<p>Tokenization, or segmenting a text into word chunks, is the first part of a Natural Language Processing pipeline. Tokens can be sentences, words, or sub-word chunks. The tokenization process involves many practical decisions, and this has led to many different methods that are reflected by a variety of available tokenizers. A tokenizer takes a text as input and generated tokens as output automatically.</p>
<p>In the case of tokenizing words, this is traditionally done by splitting on whitespace and punctuation. (There are more advanced tokenization methods for language models such as BERT and GPT. These include Byte-Pair Encoding, WordPiece, and SentencePiece.) We will look at a few examples of traditional tokenizers with a goal of gathering tokens into one-, two-, and three-word constructions. The general name for these is n-grams.</p>
<p>An n-gram is a sequence of n items from a given sample of text or speech. Most often, this refers to a sequence of words, but it can also be used to analyze text at the level of syllables, letters, or phonemes. N-grams are often described by their length. For example, word n-grams might include:</p>
<ul class="simple">
<li><p>stock (a 1-gram, or unigram)</p></li>
<li><p>vegetable stock (a 2-gram, or bigram)</p></li>
<li><p>homemade vegetable stock (a 3-gram, or trigram)</p></li>
</ul>
<p>A text analysis approach that looks only at unigrams would not be able to differentiate between the “stock” in “stock market” and “vegetable stock.” By including bigrams and trigrams in our analysis, we are able to look at concepts that extend across multiple words. One of the most popular examples of text analysis with n-grams is the <a class="reference external" href="https://books.google.com/ngrams">Google N-Gram Viewer</a>.</p>
</section>
<section id="constellate-datasets">
<h2>Constellate Datasets<a class="headerlink" href="#constellate-datasets" title="Link to this heading">#</a></h2>
<p>The Constellate <a class="reference external" href="https://constellate.org/builder">dataset builder</a> has a historical term frequency viewer that is similar to the Google N-Gram Viewer. For example, we could create a dataset of medical journals and see how common particular terms are over time.</p>
<p><img alt="The Constellate Term Frequency Viewer showing diseases represented in medical journals in the 20th century" src="https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/frequency-viewer.png" /></p>
<p>The Constellate term frequency viewer will graph frequencies for bigrams and trigrams as well.</p>
<p><img alt="The Constellate Term Frequency Viewer showing the frequency of different kinds of fevers whose names are bigrams" src="https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/frequency-viewer-2.png" /></p>
<p>Building a dataset triggers a process that gathers up all the unigrams, bigrams, and trigrams for the documents you’ve selected. We are able to supply these n-gram lists with their accompanying metadata for any source, even if the materials are under copyright. This is the essence of a “non-consumptive” dataset. The researcher can access the n-grams but not the underlying full-text. In cases where there are no copyright restrictions, we also supply the full-text of the material.</p>
<p>The materials are available for download and analysis in <a class="reference external" href="https://constellate.org/docs/what-format-are-jstor-portico-datasets">several dataset types</a>. The most complete type is a JSON-Lines file which contains all of the data we can legally provide. Many of the notebooks we offer rely on this <a class="reference external" href="https://constellate.org/docs/what-format-are-jstor-portico-datasets">data format</a> and make it easy to accomplish common text analysis tasks such as counting word frequencies, creating word clouds, significant terms weighting, and topic modeling.</p>
<p>We can create our own Constellate-compatible datasets from any texts by extracting the unigrams, bigrams, trigrams, and full text. We would then simply need to put them into the appropriate form matching the Constellate data schema. Then we could run the analyses mentioned above on our own texts. This notebook focuses on the tokenization processes to gather the unigrams, bigrams, and trigrams.</p>
</section>
<section id="creating-your-own-basic-tokenizer">
<h2>Creating your own basic tokenizer<a class="headerlink" href="#creating-your-own-basic-tokenizer" title="Link to this heading">#</a></h2>
<p>It is possible to create your own basic tokenizer by using Python string methods. The following example uses the <code class="docutils literal notranslate"><span class="pre">.split()</span></code> method to gather unigrams.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Download Shakespeare&#39;s Othello from Project Gutenberg</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">urllib.request</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">pathlib</span><span class="w"> </span><span class="kn">import</span> <span class="n">Path</span>

<span class="c1"># Check if a data folder exists. If not, create it.</span>
<span class="n">data_folder</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="s1">&#39;../data/&#39;</span><span class="p">)</span>
<span class="n">data_folder</span><span class="o">.</span><span class="n">mkdir</span><span class="p">(</span><span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">text_address</span> <span class="o">=</span> <span class="s1">&#39;https://www.gutenberg.org/cache/epub/1531/pg1531.txt&#39;</span>
<span class="n">text_name</span> <span class="o">=</span> <span class="s1">&#39;../data/&#39;</span> <span class="o">+</span> <span class="n">text_address</span><span class="o">.</span><span class="n">rsplit</span><span class="p">(</span><span class="s1">&#39;/&#39;</span><span class="p">,</span> <span class="mi">1</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="n">urllib</span><span class="o">.</span><span class="n">request</span><span class="o">.</span><span class="n">urlretrieve</span><span class="p">(</span><span class="n">text_address</span><span class="p">,</span> <span class="n">text_name</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Opening a file in read mode</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">text_name</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">text</span> <span class="o">=</span> <span class="n">f</span><span class="o">.</span><span class="n">read</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># See the raw string version of our text</span>
<span class="n">text</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Splitting the text string into a list of strings</span>
<span class="n">tokenized_list</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>
<span class="nb">list</span><span class="p">(</span><span class="n">tokenized_list</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Cleaning up the tokens</span>
<span class="n">unigrams</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">tokenized_list</span><span class="p">:</span>
    <span class="n">token</span> <span class="o">=</span> <span class="n">token</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="c1"># lowercase tokens</span>
    <span class="n">token</span> <span class="o">=</span> <span class="n">token</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s1">&#39;.&#39;</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">)</span> <span class="c1"># remove periods</span>
    <span class="n">token</span> <span class="o">=</span> <span class="n">token</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s1">&#39;!&#39;</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">)</span> <span class="c1"># remove exclamation points</span>
    <span class="n">token</span> <span class="o">=</span> <span class="n">token</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s1">&#39;?&#39;</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">)</span> <span class="c1"># remove question marks</span>
    <span class="n">unigrams</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">token</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Preview the unigrams</span>
<span class="nb">list</span><span class="p">(</span><span class="n">unigrams</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Count up the tokens using a Counter() object</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">collections</span><span class="w"> </span><span class="kn">import</span> <span class="n">Counter</span>
<span class="n">word_counts</span> <span class="o">=</span> <span class="n">Counter</span><span class="p">(</span><span class="n">unigrams</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">word_counts</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="nltk">
<h2>NLTK<a class="headerlink" href="#nltk" title="Link to this heading">#</a></h2>
<p>While writing your own tokenizer may allow you to create highly customized results, it is easier and more often more effective to use existing tokenizers offered in packages such as the Natural Language Toolkit (NLTK) and spaCy. Ultimately, whatever tokenizer you use, it is helpful to understand Python string manipulations and regular expressions in case you wish to adapt a particular tokenizer to your texts.</p>
<p>The NLTK library has multiple tokenizers available.</p>
<section id="word-punctuation">
<h3><a class="reference external" href="https://www.nltk.org/_modules/nltk/tokenize/punkt.html">Word Punctuation</a><a class="headerlink" href="#word-punctuation" title="Link to this heading">#</a></h3>
<p>The word punctuation tokenizer splits on white spaces and splits out punctuation into separate tokens.</p>
</section>
<section id="penn-treebank">
<h3><a class="reference external" href="https://www.nltk.org/_modules/nltk/tokenize/treebank.html">Penn Treebank</a><a class="headerlink" href="#penn-treebank" title="Link to this heading">#</a></h3>
<p>The Tree Bank tokenizer is the default tokenizer for NLTK. It features a variety of regular expressions for addressing punctuation such as contractions, quotes, parentheses, brackets, and dashes.</p>
</section>
<section id="tweet">
<h3><a class="reference external" href="https://www.nltk.org/_modules/nltk/tokenize/casual.html#TweetTokenizer">Tweet</a><a class="headerlink" href="#tweet" title="Link to this heading">#</a></h3>
<p>The Twitter tokenizer is designed to work with Twitter and social media text. It uses regular expressions for addressing emoticons, phone numbers, URLs, Twitter usernames, and email addresses.</p>
</section>
<section id="multi-word-expression">
<h3><a class="reference external" href="https://www.nltk.org/_modules/nltk/tokenize/mwe.html">Multi-Word Expression</a><a class="headerlink" href="#multi-word-expression" title="Link to this heading">#</a></h3>
<p>The MWETokenizer takes a “string which has already been divided into tokens and retokenizes it, merging multi-word expressions into single tokens, using a lexicon of MWEs.” The lexicon of Multi-Word Entities is constructed by the user. It can be constructed ad-hoc depended on the user’s research interest or discovered through the use of techniques like part of speech tagging, collocation, and named entity recognition.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import a variety of tokenizers</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">nltk</span>
<span class="n">nltk</span><span class="o">.</span><span class="n">download</span><span class="p">(</span><span class="s1">&#39;punkt&#39;</span><span class="p">,</span> <span class="n">download_dir</span><span class="o">=</span><span class="s1">&#39;../data/nltk_data&#39;</span><span class="p">)</span>
<span class="n">nltk</span><span class="o">.</span><span class="n">download</span><span class="p">(</span><span class="s1">&#39;averaged_perceptron_tagger&#39;</span><span class="p">,</span> <span class="n">download_dir</span><span class="o">=</span><span class="s1">&#39;../data/nltk_data&#39;</span><span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">nltk.tokenize</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span><span class="n">TreebankWordTokenizer</span><span class="p">,</span>
                          <span class="n">word_tokenize</span><span class="p">,</span>
                          <span class="n">wordpunct_tokenize</span><span class="p">,</span>
                          <span class="n">TweetTokenizer</span><span class="p">,</span>
                          <span class="n">MWETokenizer</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">string</span> <span class="o">=</span> <span class="s2">&quot;Nathan Kelber is helping us tokenize with the Constellate platform. http://constellate.org #NLP&quot;</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Python .split() tokenization</span>
<span class="n">split_tokens</span> <span class="o">=</span> <span class="n">string</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Python .split()&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">split_tokens</span><span class="p">,</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>

<span class="c1"># Punctuation-based tokenization</span>
<span class="n">punct_tokens</span> <span class="o">=</span> <span class="n">wordpunct_tokenize</span><span class="p">(</span><span class="n">string</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Wordpunct tokenizer&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">punct_tokens</span><span class="p">,</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>

<span class="c1"># Treebank Tokenizer</span>
<span class="n">treebank_tokens</span> <span class="o">=</span> <span class="n">TreebankWordTokenizer</span><span class="p">()</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">string</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Treebank Tokenizer&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">treebank_tokens</span><span class="p">,</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>

<span class="c1"># TweetTokenizer</span>
<span class="n">tweet_tokens</span> <span class="o">=</span> <span class="n">TweetTokenizer</span><span class="p">()</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">string</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Tweet Tokenizer&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tweet_tokens</span><span class="p">,</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>

<span class="c1"># Multi-Word Expression Tokenizer</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">MWETokenizer</span><span class="p">([(</span><span class="s1">&#39;Nathan&#39;</span><span class="p">,</span> <span class="s1">&#39;Kelber&#39;</span><span class="p">)])</span>
<span class="n">MWE_tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">word_tokenize</span><span class="p">(</span><span class="n">string</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;MWE Tokenizer&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">MWE_tokens</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>The tokenizer will generate a list of unigrams, but we still need to generate our bigrams and trigrams. We can simply pass the tokens into NLTK’s bigrams and trigrams methods then store the results in a list.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Creating our bigrams and trigrams</span>
<span class="n">bigrams</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">nltk</span><span class="o">.</span><span class="n">bigrams</span><span class="p">(</span><span class="n">treebank_tokens</span><span class="p">))</span>
<span class="n">trigrams</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">nltk</span><span class="o">.</span><span class="n">trigrams</span><span class="p">(</span><span class="n">treebank_tokens</span><span class="p">))</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Bigrams: </span><span class="se">\n</span><span class="s1"> &#39;</span><span class="p">,</span> <span class="n">bigrams</span><span class="p">,</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
    
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Trigrams: </span><span class="se">\n</span><span class="s1">,&#39;</span><span class="p">,</span> <span class="n">trigrams</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>The NLTK bigrams and trigrams method creates a list of bigrams that are tuples. If we want them to be strings, then we would need to access each index of the tuple and create a string out of it.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Function definitions for Converting NLTK tuples into strings</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">collections</span><span class="w"> </span><span class="kn">import</span> <span class="n">Counter</span>

<span class="k">def</span><span class="w"> </span><span class="nf">convert_tuple_bigrams</span><span class="p">(</span><span class="n">tuples_to_convert</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Converts NLTK tuples into bigram strings&quot;&quot;&quot;</span>
    <span class="n">string_grams</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">tuple_grams</span> <span class="ow">in</span> <span class="n">tuples_to_convert</span><span class="p">:</span>
        <span class="n">first_word</span> <span class="o">=</span> <span class="n">tuple_grams</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">second_word</span> <span class="o">=</span> <span class="n">tuple_grams</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">gram_string</span> <span class="o">=</span> <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">first_word</span><span class="si">}</span><span class="s1"> </span><span class="si">{</span><span class="n">second_word</span><span class="si">}</span><span class="s1">&#39;</span>
        <span class="n">string_grams</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">gram_string</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">string_grams</span>

<span class="k">def</span><span class="w"> </span><span class="nf">convert_tuple_trigrams</span><span class="p">(</span><span class="n">tuples_to_convert</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Converts NLTK tuples into trigram strings&quot;&quot;&quot;</span>
    <span class="n">string_grams</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">tuple_grams</span> <span class="ow">in</span> <span class="n">tuples_to_convert</span><span class="p">:</span>
        <span class="n">first_word</span> <span class="o">=</span> <span class="n">tuple_grams</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">second_word</span> <span class="o">=</span> <span class="n">tuple_grams</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">third_word</span> <span class="o">=</span> <span class="n">tuple_grams</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
        <span class="n">gram_string</span> <span class="o">=</span> <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">first_word</span><span class="si">}</span><span class="s1"> </span><span class="si">{</span><span class="n">second_word</span><span class="si">}</span><span class="s1"> </span><span class="si">{</span><span class="n">third_word</span><span class="si">}</span><span class="s1">&#39;</span>
        <span class="n">string_grams</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">gram_string</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">string_grams</span>

<span class="k">def</span><span class="w"> </span><span class="nf">convert_strings_to_counts</span><span class="p">(</span><span class="n">string_grams</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Converts a Counter of n-grams into a dictionary&quot;&quot;&quot;</span>
    <span class="n">counter_of_grams</span> <span class="o">=</span> <span class="n">Counter</span><span class="p">(</span><span class="n">string_grams</span><span class="p">)</span>
    <span class="n">dict_of_grams</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">counter_of_grams</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">dict_of_grams</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Converting the tuples</span>
<span class="n">string_bigrams</span> <span class="o">=</span> <span class="n">convert_tuple_bigrams</span><span class="p">(</span><span class="n">bigrams</span><span class="p">)</span>
<span class="n">bigramCount</span> <span class="o">=</span> <span class="n">convert_strings_to_counts</span><span class="p">(</span><span class="n">string_bigrams</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Bigrams as a dictionary of counts&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">bigramCount</span><span class="p">,</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>

<span class="n">string_trigrams</span> <span class="o">=</span> <span class="n">convert_tuple_trigrams</span><span class="p">(</span><span class="n">trigrams</span><span class="p">)</span>
<span class="n">trigramCount</span> <span class="o">=</span> <span class="n">convert_strings_to_counts</span><span class="p">(</span><span class="n">string_trigrams</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Trigrams as a dictionary of counts&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">trigramCount</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Depending on the analysis we are doing, we may want to group similar words together. For example, we may want to group plural words together and verb tenses.</p>
<ul class="simple">
<li><p>ducks -&gt; duck</p></li>
<li><p>flown -&gt; fly</p></li>
</ul>
<p>To accomplish this, we could use a stemmer, such as the Snowball stemmer. A stemmer removes the last part of particular words to get a base form. It is a quick method which is useful for very large datasets and/or working with limited computing power.</p>
<p>In an ideal world, a lemmatizer will do a better job. It does not simply strip off letters but looks up verb tenses and takes into account the part of speech of each word.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Snowball stemmer</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">nltk.stem.snowball</span><span class="w"> </span><span class="kn">import</span> <span class="n">SnowballStemmer</span>
<span class="n">stemmer</span> <span class="o">=</span> <span class="n">SnowballStemmer</span><span class="p">(</span><span class="s1">&#39;english&#39;</span><span class="p">)</span>
<span class="n">unstemmed_token</span> <span class="o">=</span> <span class="s1">&#39;running&#39;</span>
<span class="c1">#unstemmed_token = &#39;flown&#39;</span>

<span class="n">stemmed_token</span> <span class="o">=</span> <span class="n">stemmer</span><span class="o">.</span><span class="n">stem</span><span class="p">(</span><span class="n">unstemmed_token</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">stemmed_token</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Part of Speech tagging allows us to see the parts of speech of various tokens.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Part of Speech Tagging</span>
<span class="n">pos_list</span> <span class="o">=</span> <span class="n">nltk</span><span class="o">.</span><span class="n">pos_tag</span><span class="p">(</span><span class="n">nltk</span><span class="o">.</span><span class="n">word_tokenize</span><span class="p">(</span><span class="n">string</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">pos_list</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="spacy">
<h2>spaCy<a class="headerlink" href="#spacy" title="Link to this heading">#</a></h2>
<p>spaCy takes a different approach from NLTK, creating a document model of a text. It is more sophisticated, but uses a different syntax for NLP tasks.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Install the spaCy Program</span>
<span class="o">!</span>pip<span class="w"> </span>install<span class="w"> </span>spacy
<span class="o">!</span>pip<span class="w"> </span>install<span class="w"> </span>-U<span class="w"> </span>pip<span class="w"> </span>setuptools<span class="w"> </span>wheel
<span class="o">!</span>pin<span class="w"> </span>install<span class="w"> </span>-U<span class="w"> </span>spacy
<span class="o">!</span>python<span class="w"> </span>-m<span class="w"> </span>spacy<span class="w"> </span>download<span class="w"> </span>en_core_web_sm
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">spacy.lang.en</span><span class="w"> </span><span class="kn">import</span> <span class="n">English</span>

<span class="n">nlp</span> <span class="o">=</span> <span class="n">English</span><span class="p">()</span>

<span class="n">string</span> <span class="o">=</span> <span class="s2">&quot;Nathan Kelber is helping us tokenize with the Constellate platform. http://constellate.org #NLP&quot;</span>

<span class="n">my_doc</span> <span class="o">=</span> <span class="n">nlp</span><span class="p">(</span><span class="n">string</span><span class="p">)</span>

<span class="n">tokens</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">my_doc</span><span class="p">:</span>
    <span class="n">tokens</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">token</span><span class="o">.</span><span class="n">text</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>In order to change tokenization with spaCy, you can <a class="reference external" href="https://machinelearningknowledge.ai/complete-guide-to-spacy-tokenizer-with-examples/">add rules</a>. spaCy also supports Parts of Speech tagging and lemmatization.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">spacy</span>
<span class="n">nlp</span> <span class="o">=</span> <span class="n">spacy</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;en_core_web_sm&#39;</span><span class="p">)</span>
<span class="n">my_doc</span> <span class="o">=</span> <span class="n">nlp</span><span class="p">(</span><span class="n">string</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Parts of Speech&#39;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">my_doc</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">token</span><span class="p">,</span> <span class="n">token</span><span class="o">.</span><span class="n">pos_</span><span class="p">,)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">Lemmatizations&#39;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">my_doc</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">token</span><span class="p">,</span> <span class="n">token</span><span class="o">.</span><span class="n">lemma_</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We can gather our n-grams by defining a function that accepts our tokens and an argument <code class="docutils literal notranslate"><span class="pre">n</span></code> for the “n” in “n-gram.” So, a bigram would be n = 2.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># A function for gathering n-grams with spaCy</span>
<span class="k">def</span><span class="w"> </span><span class="nf">n_grams</span><span class="p">(</span><span class="n">tokens</span><span class="p">,</span> <span class="n">n</span><span class="p">):</span>
    <span class="n">n_grams</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span><span class="o">-</span><span class="n">n</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>
        <span class="n">n_grams</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tokens</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="n">n</span><span class="p">])</span>
    <span class="k">return</span><span class="p">(</span><span class="n">n_grams</span><span class="p">)</span>
    <span class="c1"># return[tokens[i:i+n] for i in range(len(tokens)-n+1)] # Written as a list comprehension</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">bigrams</span> <span class="o">=</span> <span class="n">n_grams</span><span class="p">(</span><span class="n">tokens</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">trigrams</span> <span class="o">=</span> <span class="n">n_grams</span><span class="p">(</span><span class="n">tokens</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">bigrams</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">trigrams</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>While NLTK and spaCy tokenizers are the most prominent, there are also tokenizers available for packages such as:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://radimrehurek.com/gensim/">Gensim</a></p></li>
<li><p><a class="reference external" href="https://keras.io/">Keras</a></p></li>
<li><p><a class="reference external" href="https://nlp.stanford.edu/software/tokenizer.shtml">Stanford NLP</a></p></li>
</ul>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./src/TextMining"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-a-word">What is a word?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tokenization">Tokenization</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#constellate-datasets">Constellate Datasets</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#creating-your-own-basic-tokenizer">Creating your own basic tokenizer</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#nltk">NLTK</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#word-punctuation">Word Punctuation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#penn-treebank">Penn Treebank</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tweet">Tweet</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multi-word-expression">Multi-Word Expression</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#spacy">spaCy</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By University of South Carolina Libraries
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2025.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>