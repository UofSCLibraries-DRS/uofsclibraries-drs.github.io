{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82c5126e",
   "metadata": {},
   "source": [
    "# Performing Analysis on Tidy Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31475fc",
   "metadata": {},
   "source": [
    "## Packages to Install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bfe2e561",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The downloaded binary packages are in\n",
      "\t/var/folders/2h/84wxzls579b1yv00g4jj02fh0000gn/T//RtmpkIayK9/downloaded_packages\n",
      "\n",
      "The downloaded binary packages are in\n",
      "\t/var/folders/2h/84wxzls579b1yv00g4jj02fh0000gn/T//RtmpkIayK9/downloaded_packages\n",
      "\n",
      "The downloaded binary packages are in\n",
      "\t/var/folders/2h/84wxzls579b1yv00g4jj02fh0000gn/T//RtmpkIayK9/downloaded_packages\n",
      "\n",
      "The downloaded binary packages are in\n",
      "\t/var/folders/2h/84wxzls579b1yv00g4jj02fh0000gn/T//RtmpkIayK9/downloaded_packages\n",
      "\n",
      "The downloaded binary packages are in\n",
      "\t/var/folders/2h/84wxzls579b1yv00g4jj02fh0000gn/T//RtmpkIayK9/downloaded_packages\n"
     ]
    }
   ],
   "source": [
    "install.packages(\"tidytext\")\n",
    "install.packages(\"textdata\")\n",
    "install.packages(\"dplyr\")\n",
    "install.packages(\"stringr\")\n",
    "install.packages(\"janeaustenr\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be36b25",
   "metadata": {},
   "source": [
    "## Sentiment Analysis\n",
    "In the previous chapter, we explored the tidy text format and how it helps analyze word frequency and compare documents. Now, we shift focus to opinion mining or sentiment analysis, where we assess the emotional tone of a text. Just as human readers infer emotions like positivity, negativity, surprise, or disgust from words, we can use text mining tools to programmatically analyze a text's emotional content. A common approach to sentiment analysis is to break the text into individual words, assign sentiment scores to those words, and sum them to understand the overall sentiment of the text. While this isn’t the only method for sentiment analysis, it is widely used and fits naturally within the tidy data framework and toolset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb901314",
   "metadata": {},
   "source": [
    "### The `sentiments` Dataset \n",
    "As discussed earlier, there are several methods and dictionaries available for evaluating opinion or emotion in text. The **tidytext** package provides access to three commonly used general-purpose sentiment lexicons: **AFINN**, developed by [Finn Årup Nielsen](https://www2.imm.dtu.dk/pubdb/pubs/6010-full.html); **bing**, from [Bing Liu and collaborators](https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html); and **nrc**, created by [Saif Mohammad and Peter Turney](https://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm). All three lexicons are based on unigrams, meaning they assign sentiment scores to individual words. The *nrc* lexicon categorizes words as positive, negative, or into specific emotions like joy, anger, sadness, and trust, using a simple \"yes\"/\"no\" system. The *bing* lexicon also applies a binary positive or negative label to each word, while the *AFINN* lexicon assigns numerical sentiment scores ranging from -5 (most negative) to 5 (most positive). These lexicons are distributed under different licenses, so it’s important to review the terms and ensure they align with your project requirements before use.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a88efaa",
   "metadata": {},
   "source": [
    "The `get_sentiments()` function allows us to easily access specific sentiment lexicons along with their corresponding sentiment measures. By specifying the lexicon name (such as `\"afinn\"`, `\"bing\"`, or `\"nrc\"`), we can retrieve the appropriate set of words and their sentiment scores or categories, making it straightforward to incorporate sentiment analysis into a tidy text workflow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c9eee0f3",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[90m# A tibble: 2,477 x 2\u001b[39m\n",
      "   word       value\n",
      "   \u001b[3m\u001b[90m<chr>\u001b[39m\u001b[23m      \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m\n",
      "\u001b[90m 1\u001b[39m abandon       -\u001b[31m2\u001b[39m\n",
      "\u001b[90m 2\u001b[39m abandoned     -\u001b[31m2\u001b[39m\n",
      "\u001b[90m 3\u001b[39m abandons      -\u001b[31m2\u001b[39m\n",
      "\u001b[90m 4\u001b[39m abducted      -\u001b[31m2\u001b[39m\n",
      "\u001b[90m 5\u001b[39m abduction     -\u001b[31m2\u001b[39m\n",
      "\u001b[90m 6\u001b[39m abductions    -\u001b[31m2\u001b[39m\n",
      "\u001b[90m 7\u001b[39m abhor         -\u001b[31m3\u001b[39m\n",
      "\u001b[90m 8\u001b[39m abhorred      -\u001b[31m3\u001b[39m\n",
      "\u001b[90m 9\u001b[39m abhorrent     -\u001b[31m3\u001b[39m\n",
      "\u001b[90m10\u001b[39m abhors        -\u001b[31m3\u001b[39m\n",
      "\u001b[90m# i 2,467 more rows\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "library(tidytext)\n",
    "\n",
    "print(get_sentiments(\"afinn\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1ee68f48",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[90m# A tibble: 6,786 x 2\u001b[39m\n",
      "   word        sentiment\n",
      "   \u001b[3m\u001b[90m<chr>\u001b[39m\u001b[23m       \u001b[3m\u001b[90m<chr>\u001b[39m\u001b[23m    \n",
      "\u001b[90m 1\u001b[39m 2-faces     negative \n",
      "\u001b[90m 2\u001b[39m abnormal    negative \n",
      "\u001b[90m 3\u001b[39m abolish     negative \n",
      "\u001b[90m 4\u001b[39m abominable  negative \n",
      "\u001b[90m 5\u001b[39m abominably  negative \n",
      "\u001b[90m 6\u001b[39m abominate   negative \n",
      "\u001b[90m 7\u001b[39m abomination negative \n",
      "\u001b[90m 8\u001b[39m abort       negative \n",
      "\u001b[90m 9\u001b[39m aborted     negative \n",
      "\u001b[90m10\u001b[39m aborts      negative \n",
      "\u001b[90m# i 6,776 more rows\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "print(get_sentiments(\"bing\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e709bf33",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[90m# A tibble: 13,872 x 2\u001b[39m\n",
      "   word        sentiment\n",
      "   \u001b[3m\u001b[90m<chr>\u001b[39m\u001b[23m       \u001b[3m\u001b[90m<chr>\u001b[39m\u001b[23m    \n",
      "\u001b[90m 1\u001b[39m abacus      trust    \n",
      "\u001b[90m 2\u001b[39m abandon     fear     \n",
      "\u001b[90m 3\u001b[39m abandon     negative \n",
      "\u001b[90m 4\u001b[39m abandon     sadness  \n",
      "\u001b[90m 5\u001b[39m abandoned   anger    \n",
      "\u001b[90m 6\u001b[39m abandoned   fear     \n",
      "\u001b[90m 7\u001b[39m abandoned   negative \n",
      "\u001b[90m 8\u001b[39m abandoned   sadness  \n",
      "\u001b[90m 9\u001b[39m abandonment anger    \n",
      "\u001b[90m10\u001b[39m abandonment fear     \n",
      "\u001b[90m# i 13,862 more rows\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "print(get_sentiments(\"nrc\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40fa742",
   "metadata": {},
   "source": [
    "The sentiment lexicons used in text mining were developed through either crowdsourcing platforms like Amazon Mechanical Turk or the manual effort of individual researchers, and they were validated using datasets such as crowdsourced opinions, movie or restaurant reviews, or Twitter posts. Because of this, applying these lexicons to texts that differ greatly in style or era—like 200-year-old narrative fiction—may produce less precise results, although shared vocabulary still allows meaningful sentiment analysis. In addition to these general-purpose lexicons, domain-specific options exist, such as those designed for financial texts. These dictionary-based methods work by summing the sentiment scores of individual words but have limitations, as they don't account for context, qualifiers, or negation (e.g., \"not good\"). For many types of narrative text where sarcasm or constant negation is minimal, this limitation is less significant, though later chapters explore strategies for handling negation. It's also important to choose an appropriate text chunk size for analysis—large text sections can have sentiment scores that cancel each other out, whereas sentence- or paragraph-level analysis often yields clearer results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b922a44",
   "metadata": {},
   "source": [
    "### Sentiment Analysis with Inner Join"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4b5b83",
   "metadata": {},
   "source": [
    "When text data is in a tidy format, performing sentiment analysis becomes as simple as using an inner join. This highlights one of the key advantages of approaching text mining as a tidy data task—the tools and workflows remain consistent and intuitive. Just as removing stop words involves an `anti_join`, sentiment analysis works by applying an `inner_join` to combine the tokenized text with a sentiment lexicon, matching words in the text to their corresponding sentiment values. This tidy approach simplifies the process and integrates seamlessly with other tidyverse functions.\n",
    "\n",
    "To find the most common joy-related words in *Emma* using the NRC lexicon, we first need to convert the novel's text into a tidy format with one word per row, using `unnest_tokens()`, just like we did earlier. To preserve important context, we can also create additional columns that track which line and chapter each word appears in. This is done using `group_by()` to organize the text by book and `mutate()` to assign line numbers and detect chapters, often using a regular expression. Once the text is structured this way, we can easily filter for words associated with joy in the NRC lexicon and analyze their frequency within the novel.\n",
    "\n",
    "Notice that we named the output column from `unnest_tokens()` as **word**, which is a practical and consistent choice. The sentiment lexicons and stop word datasets provided by **tidytext** also use **word** as the column name for individual tokens. By keeping this naming consistent, performing operations like `inner_join()` for sentiment analysis or `anti_join()` for removing stop words becomes straightforward and seamless, avoiding unnecessary renaming or data wrangling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "89b610c1",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "library(janeaustenr)\n",
    "suppressPackageStartupMessages(library(dplyr))\n",
    "library(stringr)\n",
    "\n",
    "tidy_books <- austen_books() %>%\n",
    "  group_by(book) %>%\n",
    "  mutate(\n",
    "    linenumber = row_number(),\n",
    "    chapter = cumsum(str_detect(text, \n",
    "                                regex(\"^chapter [\\\\divxlc]\", \n",
    "                                      ignore_case = TRUE)))) %>%\n",
    "  ungroup() %>%\n",
    "  unnest_tokens(word, text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321c0ccc",
   "metadata": {},
   "source": [
    "Now that the text is in a tidy format with one word per row, we can easily perform sentiment analysis. First, we use the NRC lexicon and apply `filter()` to select only the words associated with the emotion *joy*. Next, we filter the text data to include only words from *Emma* and use `inner_join()` to connect the text with the joy words from the lexicon. Finally, we use `count()` from **dplyr** to find the most common joy-related words in the novel. The results show mostly positive, happy words like *hope*, *friendship*, and *love*, which align with the theme of joy. However, some words like *found* or *present* may not always carry joyful meaning in Austen’s writing, a nuance that will be explored further in Section 2.4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ee3037ec",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[22mJoining with `by = join_by(word)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[90m# A tibble: 301 x 2\u001b[39m\n",
      "   word          n\n",
      "   \u001b[3m\u001b[90m<chr>\u001b[39m\u001b[23m     \u001b[3m\u001b[90m<int>\u001b[39m\u001b[23m\n",
      "\u001b[90m 1\u001b[39m good        359\n",
      "\u001b[90m 2\u001b[39m friend      166\n",
      "\u001b[90m 3\u001b[39m hope        143\n",
      "\u001b[90m 4\u001b[39m happy       125\n",
      "\u001b[90m 5\u001b[39m love        117\n",
      "\u001b[90m 6\u001b[39m deal         92\n",
      "\u001b[90m 7\u001b[39m found        92\n",
      "\u001b[90m 8\u001b[39m present      89\n",
      "\u001b[90m 9\u001b[39m kind         82\n",
      "\u001b[90m10\u001b[39m happiness    76\n",
      "\u001b[90m# i 291 more rows\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "nrc_joy <- get_sentiments(\"nrc\") %>% \n",
    "  filter(sentiment == \"joy\")\n",
    "\n",
    "tidy_books %>%\n",
    "  filter(book == \"Emma\") %>%\n",
    "  inner_join(nrc_joy) %>%\n",
    "  count(word, sort = TRUE) %>%\n",
    "  print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ebfcf6c",
   "metadata": {},
   "source": [
    "We can also explore how sentiment shifts across the course of each novel using just a few lines of code, mostly relying on **dplyr** functions. First, we apply `inner_join()` with the Bing lexicon to assign a sentiment score (positive or negative) to each word in the tidy text. Then, we count the number of positive and negative words within specific sections of each book to track changes in sentiment over time. To divide the text into sections, we create an **index** column that marks every 80 lines of text using integer division. The `%/%` operator performs integer division, meaning `x %/% y` returns the largest whole number less than or equal to `x/y`. This gives us consistent, sequential sections of 80 lines, allowing us to visualize how positive and negative sentiment patterns vary throughout the narrative.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
